{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a35b920c-2333-4274-b841-1a7f9ddd1b97",
   "metadata": {},
   "source": [
    "# Database<br>\n",
    "The following part has the goal to clean and manipulate the necessary data. The goal of this work is to predict the damages in the US done to property caused by natural hazards. The data was retrived from the following sources: \n",
    "- Natural Hazards: https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/. The data was taken from the National Oceanic and Atmospheric Administration\n",
    "- Population: https://www.census.gov/data/datasets.html. The data was taken from the official Census dataset of the US\n",
    "- GDP per State and County: https://www.bea.gov/data/gdp/gdp-county-metro-and-other-areas. The GDP per state and county was taken from the Bureau of Economic Analysis of the US\n",
    "- County Size: https://www.census.gov/data/datasets.html. The data was retrived from the official Census dataset of the US\n",
    "\n",
    "All the datasets were uploaded to a Google Drive account to make it easier for everyone to have access to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92adc0f6-16bf-4ce4-ad5f-62e873521f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all necessary packages\n",
    "import gdown\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import gdown\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349c1ed4-d981-4dd4-99d6-1063072d494e",
   "metadata": {},
   "source": [
    "## Download of the Natural Hazard datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec19e53-1df8-4f7c-bd03-fc2238e39274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1fqXYPTptNnH_0USoQoP_MHamTmpyh5UD\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2024.csv\n",
      "100%|██████████| 51.2M/51.2M [00:04<00:00, 10.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2024\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1fqXYPTptNnH_0USoQoP_MHamTmpyh5UD/view?usp=sharing\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2024.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "643224f1-1d6d-4bde-bcbc-b55926550364",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2024 = pd.read_csv(\"Events_2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3027a61-6442-4154-b48f-fea2f9a7ff83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1whLHKGUmc0cc5rxiY3pNALXLjYRq-ACJ\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2023.csv\n",
      "100%|██████████| 72.4M/72.4M [00:06<00:00, 11.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2023\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1whLHKGUmc0cc5rxiY3pNALXLjYRq-ACJ/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2023.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a6100f5-5355-4473-af1d-cbb04e103e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023 = pd.read_csv(\"Events_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eccb684e-a9d6-4c7e-8905-6ed2c6a079c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Os9fdmnDRqxNsMr4m6WfrS3yOH8gT91v\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2022.csv\n",
      "100%|██████████| 66.5M/66.5M [00:05<00:00, 11.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2022\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1Os9fdmnDRqxNsMr4m6WfrS3yOH8gT91v/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2022.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f606c4b-6d90-4618-bd10-81a92dbe44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022 = pd.read_csv(\"Events_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82c3a8e4-5062-4b04-ba25-aa636b5c8ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ZPBwl6PwMl77zjH1NnSCZMllLEur7uGy\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2021.csv\n",
      "100%|██████████| 60.5M/60.5M [00:05<00:00, 10.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2021\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1ZPBwl6PwMl77zjH1NnSCZMllLEur7uGy/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2021.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "277c92a3-3cca-41a8-9300-507b9ebda668",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2021 = pd.read_csv(\"Events_2021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec1fd669-9e85-4fba-8b47-d0b5b93c286c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1YGz2ewDyeSrbLpYL1GG6rL7Y83QbGTp_\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2020.csv\n",
      "100%|██████████| 59.6M/59.6M [00:05<00:00, 11.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2020\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1YGz2ewDyeSrbLpYL1GG6rL7Y83QbGTp_/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2020.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a70788d4-9932-484c-b323-3c2a591bdd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020 = pd.read_csv(\"Events_2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1504ad60-3a66-4548-b9ca-681102d6274b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=15nX61Tzvw0c95Ve8g79JJ0rOCnIEi9-W\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2019.csv\n",
      "100%|██████████| 63.1M/63.1M [00:06<00:00, 9.65MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2019\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/15nX61Tzvw0c95Ve8g79JJ0rOCnIEi9-W/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2019.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78274c3b-442c-4e29-9907-91872a32d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019 = pd.read_csv(\"Events_2019.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75f8dd0c-36bc-422c-aca3-4363813570c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1vx2tq1S479G1KIVuUO6_r3hlnoK8XI8V\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2018.csv\n",
      "100%|██████████| 57.5M/57.5M [00:06<00:00, 9.35MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2018\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1vx2tq1S479G1KIVuUO6_r3hlnoK8XI8V/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2018.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82fbfd84-759d-423b-9a68-1fbf64a2dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = pd.read_csv(\"Events_2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78373780-1c17-4224-ab7f-5971a184111d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1w6NItR79UvMovTcbLOT68iPXfp7moK91\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2017.csv\n",
      "100%|██████████| 51.9M/51.9M [00:04<00:00, 10.7MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2017\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1w6NItR79UvMovTcbLOT68iPXfp7moK91/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2017.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d08815fa-506c-425c-97bc-5a0547eca25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = pd.read_csv(\"Events_2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3896a0d-bdd8-4ba5-a192-7ec6c894f221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1CbDvOshWCdRsq4DL7HTdxdrmA9QiRj0N\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2016.csv\n",
      "100%|██████████| 51.0M/51.0M [00:05<00:00, 9.91MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2016\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1CbDvOshWCdRsq4DL7HTdxdrmA9QiRj0N/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2016.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c392b1d-018a-4327-b796-d06df40a3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016 = pd.read_csv(\"Events_2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a190753-ec25-42e4-8e84-028465aab756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1lmRvb1t0KywXtbrf1yAPZtjsA6m3mtC5\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2015.csv\n",
      "100%|██████████| 54.2M/54.2M [00:06<00:00, 7.87MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2015\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1lmRvb1t0KywXtbrf1yAPZtjsA6m3mtC5/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2015.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ad0c66c-80d3-4cb7-8b86-1aea3e197b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_2015 = pd.read_csv(\"Events_2015.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f39651b-1687-44ed-a693-4417556bff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Lm-9Xr6G9x-547hVw_SME6dxv7ozd9ZX\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2014.csv\n",
      "100%|██████████| 58.8M/58.8M [00:06<00:00, 9.55MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2014\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1Lm-9Xr6G9x-547hVw_SME6dxv7ozd9ZX/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2014.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c99e7cab-5749-4585-b0ca-038b277461a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2014 = pd.read_csv(\"Events_2014.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30b858e5-b7e6-4db9-a7a5-ebeca5932e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1OboT4yG9StT95KR_ek8YA_zQj9NBAQsa\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2013.csv\n",
      "100%|██████████| 61.8M/61.8M [00:06<00:00, 9.82MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2013\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1OboT4yG9StT95KR_ek8YA_zQj9NBAQsa/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2013.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "675d3ad9-5ae3-4cb9-b541-0c41edb95b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2013 = pd.read_csv(\"Events_2013.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d4c66e8-4f7e-40d4-a267-a7f94360aae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1KAGVSTn_2RDdSm7925rKtjXHu_VpHjGt\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2012.csv\n",
      "100%|██████████| 64.9M/64.9M [00:12<00:00, 5.16MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2012\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1KAGVSTn_2RDdSm7925rKtjXHu_VpHjGt/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2012.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62ea8ea2-409a-4217-ab97-ca49acb873ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2012 = pd.read_csv(\"Events_2012.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4160b09d-e96f-41eb-b921-a29cf91f0aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1n1ZTk15RkO2TLD3Y6uh0qcwj3nOzpXLx\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2011.csv\n",
      "100%|██████████| 83.9M/83.9M [00:06<00:00, 12.2MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2011\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1n1ZTk15RkO2TLD3Y6uh0qcwj3nOzpXLx/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2011.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b053a87-a31c-4ec8-b23d-6f1f347bf6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2011 = pd.read_csv(\"Events_2011.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "29bbf487-0589-4769-b383-386bef6b0562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1EIDGYVSyzWk6If8XDNZYFDK0K2g_twK1\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2010.csv\n",
      "100%|██████████| 62.2M/62.2M [00:05<00:00, 10.6MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2010\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1EIDGYVSyzWk6If8XDNZYFDK0K2g_twK1/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2010.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d31d827c-5839-4e44-8637-3c590b577516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_2010 = pd.read_csv(\"Events_2010.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1332034-b479-4e09-bcbd-29bb224a8c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ye7aTZDkHIAOPlDEzbk44QIrDltwI3Lm\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2009.csv\n",
      "100%|██████████| 55.0M/55.0M [00:04<00:00, 11.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2009\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1ye7aTZDkHIAOPlDEzbk44QIrDltwI3Lm/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2009.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff125826-ea80-4917-a298-618b248a6304",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2009 = pd.read_csv(\"Events_2009.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00031edc-0fa4-4044-ac10-d43c50d466dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1d8VZXOHy2ylXvjal-68g5QpsXubuttq-\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2008.csv\n",
      "100%|██████████| 65.4M/65.4M [00:05<00:00, 11.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2008\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1d8VZXOHy2ylXvjal-68g5QpsXubuttq-/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2008.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c87c7273-3fcf-4747-9233-bcb053511c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2008 = pd.read_csv(\"Events_2008.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bc3aa3c-eb97-4d05-9cfb-bc09fe6dc549",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1TuJWoZWsM3TkwCfDZ37Hl9iw1aZ1voOs\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2007.csv\n",
      "100%|██████████| 54.0M/54.0M [00:06<00:00, 7.84MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2007\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1TuJWoZWsM3TkwCfDZ37Hl9iw1aZ1voOs/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2007.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f873e2d6-c76c-4b41-90f0-14df8b6b2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2007 = pd.read_csv(\"Events_2007.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5673b52-9dba-4726-81bc-2496f512114c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=18C-S_hZEMa3D7qT2DNCOjPszEVrJOt-q\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2006.csv\n",
      "100%|██████████| 35.3M/35.3M [00:04<00:00, 7.70MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2006\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/18C-S_hZEMa3D7qT2DNCOjPszEVrJOt-q/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2006.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7c73995c-de25-4ffd-b50d-8fdbf2c6c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2006 = pd.read_csv(\"Events_2006.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b93e7aea-26d8-457b-ab89-1831f455ecbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ZkWUmAh1hh6d2pKMWFM9YPLZbQQA1tL9\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2005.csv\n",
      "100%|██████████| 41.1M/41.1M [00:04<00:00, 9.58MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2005\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1ZkWUmAh1hh6d2pKMWFM9YPLZbQQA1tL9/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2005.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5cfd242b-c78b-4116-a360-25377fab0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2005 = pd.read_csv(\"Events_2005.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f243578-c0ca-478a-bee8-7a7382ed171e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1VyY_p7iym-z8VwlOLfF_QHTHMsptUQIJ\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2004.csv\n",
      "100%|██████████| 40.5M/40.5M [00:05<00:00, 6.78MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2004\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1VyY_p7iym-z8VwlOLfF_QHTHMsptUQIJ/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2004.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0553d83e-0d10-4fa2-821b-ef54df560d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2004 = pd.read_csv(\"Events_2004.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2113765e-f42c-4d26-88e3-1e486cc78e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1UXmC9me-lqlEUdgfDU7s3T5thD3Bq3fN\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2003.csv\n",
      "100%|██████████| 34.4M/34.4M [00:03<00:00, 8.87MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2003\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1UXmC9me-lqlEUdgfDU7s3T5thD3Bq3fN/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2003.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "633d232b-4ef7-4d5c-b83b-a7e8e7bf361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2003 = pd.read_csv(\"Events_2003.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "561edb23-1031-4ad2-bb1c-48c6cdcddaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1EjLtlPWextlkfFKLL1xv_5-nWof3d_Tx\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Events_2002.csv\n",
      "100%|██████████| 36.6M/36.6M [00:03<00:00, 9.97MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Damages of 2002\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1EjLtlPWextlkfFKLL1xv_5-nWof3d_Tx/view?usp=drive_link\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Events_2002.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "977bb38e-723e-478f-8ddf-27daa2b18fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2002 = pd.read_csv(\"Events_2002.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07eadfb-ec2f-4f7a-b054-49c1ce0c3a2c",
   "metadata": {},
   "source": [
    "To be sure that there is no mistake in the links or code, we want to check that all datasets are different from each other. This is done by comparing the value of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00371a87-437d-4e63-bf02-e6e3a04d739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets are different.\n"
     ]
    }
   ],
   "source": [
    "# Dynamically access the datasets by their names\n",
    "years = list(range(2002, 2025))  # Years from 2002 to 2024\n",
    "datasets = {year: globals()[f'df_{year}'] for year in years}\n",
    "\n",
    "# Check pairwise equality\n",
    "all_unique = True\n",
    "for i, year1 in enumerate(years):\n",
    "    for year2 in years[i + 1:]:\n",
    "        if datasets[year1].equals(datasets[year2]):\n",
    "            all_unique = False\n",
    "            print(f\"Dataset df_{year1} and Dataset df_{year2} are identical.\")\n",
    "if all_unique:\n",
    "    print(\"All datasets are different.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2086cd3-6b86-4fd9-9c0e-f6e32786833c",
   "metadata": {},
   "source": [
    "We also want to know if each dataset has the same quantity of variables (and thus the same variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0a288b1-2dbe-46d2-947c-bab173f1232f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset df_2002: 50936 rows, 51 columns.\n",
      "Dataset df_2003: 52955 rows, 51 columns.\n",
      "Dataset df_2004: 52409 rows, 51 columns.\n",
      "Dataset df_2005: 53976 rows, 51 columns.\n",
      "Dataset df_2006: 56400 rows, 51 columns.\n",
      "Dataset df_2007: 59011 rows, 51 columns.\n",
      "Dataset df_2008: 71190 rows, 51 columns.\n",
      "Dataset df_2009: 57398 rows, 51 columns.\n",
      "Dataset df_2010: 62807 rows, 51 columns.\n",
      "Dataset df_2011: 79091 rows, 51 columns.\n",
      "Dataset df_2012: 64503 rows, 51 columns.\n",
      "Dataset df_2013: 59986 rows, 51 columns.\n",
      "Dataset df_2014: 59475 rows, 51 columns.\n",
      "Dataset df_2015: 57907 rows, 51 columns.\n",
      "Dataset df_2016: 56005 rows, 51 columns.\n",
      "Dataset df_2017: 57029 rows, 51 columns.\n",
      "Dataset df_2018: 62697 rows, 51 columns.\n",
      "Dataset df_2019: 67861 rows, 51 columns.\n",
      "Dataset df_2020: 61279 rows, 51 columns.\n",
      "Dataset df_2021: 61389 rows, 51 columns.\n",
      "Dataset df_2022: 69886 rows, 51 columns.\n",
      "Dataset df_2023: 75243 rows, 51 columns.\n",
      "Dataset df_2024: 50623 rows, 51 columns.\n"
     ]
    }
   ],
   "source": [
    "# Get row and column counts for each dataset\n",
    "dataset_info = {year: (len(datasets[year]), datasets[year].shape[1]) if datasets[year] is not None else (0, 0) for year in years}\n",
    "\n",
    "# Display the row and column counts\n",
    "for year, (row_count, col_count) in dataset_info.items():\n",
    "    print(f\"Dataset df_{year}: {row_count} rows, {col_count} columns.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb8f43-ae9c-4506-a95c-52a783c6bb84",
   "metadata": {},
   "source": [
    "Now we can merge all these datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8b708f2-6b4f-4ad7-aa6e-e78fd79833a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [globals()[f\"df_{year}\"] for year in range(2002, 2025)]\n",
    "\n",
    "# Combine them into one DataFrame\n",
    "df = pd.concat(dfs, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985fb9b4-7e70-4325-8aba-e74863b37845",
   "metadata": {},
   "source": [
    "## GDP per county download and manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "022d0e49-0e93-470c-aaea-22e200102ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1FU-ycF-Z9CX55xDsJcxwpFr7ubHl0TTs\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\GDP_county.csv\n",
      "100%|██████████| 25.0M/25.0M [00:02<00:00, 10.2MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# GDP per county\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1FU-ycF-Z9CX55xDsJcxwpFr7ubHl0TTs/view?usp=sharing\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"GDP_county.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89854329-368a-4975-a34b-644389fa89ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicar\\AppData\\Local\\Temp\\ipykernel_5224\\1499514357.py:1: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  GDP_county = pd.read_csv(\"GDP_county.csv\")\n"
     ]
    }
   ],
   "source": [
    "GDP_county = pd.read_csv(\"GDP_county.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee5a970b-97d2-4653-b1ec-00556f93fae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have some columns with mixed values\n",
    "GDP = pd.read_csv(\"GDP_county.csv\", dtype={2: str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "371a3f28-549e-4ee0-ab44-60ac481b2a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GeoFIPS</th>\n",
       "      <th>GeoName</th>\n",
       "      <th>Region</th>\n",
       "      <th>TableName</th>\n",
       "      <th>LineCode</th>\n",
       "      <th>IndustryClassification</th>\n",
       "      <th>Description</th>\n",
       "      <th>Unit</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>...</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"00000\"</td>\n",
       "      <td>United States *</td>\n",
       "      <td></td>\n",
       "      <td>CAGDP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>All industry total</td>\n",
       "      <td>Thousands of dollars</td>\n",
       "      <td>10581929000</td>\n",
       "      <td>10929108000</td>\n",
       "      <td>...</td>\n",
       "      <td>16880683000</td>\n",
       "      <td>17608138000</td>\n",
       "      <td>18295019000</td>\n",
       "      <td>18804913000</td>\n",
       "      <td>19612102000</td>\n",
       "      <td>20656516000</td>\n",
       "      <td>21521395000</td>\n",
       "      <td>21322950000</td>\n",
       "      <td>23594031000</td>\n",
       "      <td>25744108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"00000\"</td>\n",
       "      <td>United States *</td>\n",
       "      <td></td>\n",
       "      <td>CAGDP2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Private industries</td>\n",
       "      <td>Thousands of dollars</td>\n",
       "      <td>9188994000</td>\n",
       "      <td>9454709000</td>\n",
       "      <td>...</td>\n",
       "      <td>14665532000</td>\n",
       "      <td>15332504000</td>\n",
       "      <td>15951002000</td>\n",
       "      <td>16413054000</td>\n",
       "      <td>17156255000</td>\n",
       "      <td>18097765000</td>\n",
       "      <td>18889076000</td>\n",
       "      <td>18612228000</td>\n",
       "      <td>20784824000</td>\n",
       "      <td>22807496000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"00000\"</td>\n",
       "      <td>United States *</td>\n",
       "      <td></td>\n",
       "      <td>CAGDP2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>Agriculture, forestry, fishing and hunting</td>\n",
       "      <td>Thousands of dollars</td>\n",
       "      <td>100765000</td>\n",
       "      <td>96667000</td>\n",
       "      <td>...</td>\n",
       "      <td>215847000</td>\n",
       "      <td>200581000</td>\n",
       "      <td>182147000</td>\n",
       "      <td>167492000</td>\n",
       "      <td>176840000</td>\n",
       "      <td>177117000</td>\n",
       "      <td>162043000</td>\n",
       "      <td>160778000</td>\n",
       "      <td>225670000</td>\n",
       "      <td>270820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"00000\"</td>\n",
       "      <td>United States *</td>\n",
       "      <td></td>\n",
       "      <td>CAGDP2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>21</td>\n",
       "      <td>Mining, quarrying, and oil and gas extraction</td>\n",
       "      <td>Thousands of dollars</td>\n",
       "      <td>123885000</td>\n",
       "      <td>112384000</td>\n",
       "      <td>...</td>\n",
       "      <td>388169000</td>\n",
       "      <td>418110000</td>\n",
       "      <td>262252000</td>\n",
       "      <td>211804000</td>\n",
       "      <td>267302000</td>\n",
       "      <td>313494000</td>\n",
       "      <td>293865000</td>\n",
       "      <td>201564000</td>\n",
       "      <td>332009000</td>\n",
       "      <td>457438000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"00000\"</td>\n",
       "      <td>United States *</td>\n",
       "      <td></td>\n",
       "      <td>CAGDP2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>22</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>Thousands of dollars</td>\n",
       "      <td>181276000</td>\n",
       "      <td>177568000</td>\n",
       "      <td>...</td>\n",
       "      <td>287632000</td>\n",
       "      <td>299279000</td>\n",
       "      <td>300480000</td>\n",
       "      <td>303424000</td>\n",
       "      <td>313711000</td>\n",
       "      <td>320369000</td>\n",
       "      <td>331218000</td>\n",
       "      <td>344769000</td>\n",
       "      <td>386726000</td>\n",
       "      <td>438206000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    GeoFIPS          GeoName Region TableName  LineCode  \\\n",
       "0   \"00000\"  United States *           CAGDP2       1.0   \n",
       "1   \"00000\"  United States *           CAGDP2       2.0   \n",
       "2   \"00000\"  United States *           CAGDP2       3.0   \n",
       "3   \"00000\"  United States *           CAGDP2       6.0   \n",
       "4   \"00000\"  United States *           CAGDP2      10.0   \n",
       "\n",
       "  IndustryClassification                                       Description  \\\n",
       "0                    ...                               All industry total    \n",
       "1                    ...                               Private industries    \n",
       "2                     11       Agriculture, forestry, fishing and hunting    \n",
       "3                     21    Mining, quarrying, and oil and gas extraction    \n",
       "4                     22                                        Utilities    \n",
       "\n",
       "                   Unit         2001         2002  ...         2013  \\\n",
       "0  Thousands of dollars  10581929000  10929108000  ...  16880683000   \n",
       "1  Thousands of dollars   9188994000   9454709000  ...  14665532000   \n",
       "2  Thousands of dollars    100765000     96667000  ...    215847000   \n",
       "3  Thousands of dollars    123885000    112384000  ...    388169000   \n",
       "4  Thousands of dollars    181276000    177568000  ...    287632000   \n",
       "\n",
       "          2014         2015         2016         2017         2018  \\\n",
       "0  17608138000  18295019000  18804913000  19612102000  20656516000   \n",
       "1  15332504000  15951002000  16413054000  17156255000  18097765000   \n",
       "2    200581000    182147000    167492000    176840000    177117000   \n",
       "3    418110000    262252000    211804000    267302000    313494000   \n",
       "4    299279000    300480000    303424000    313711000    320369000   \n",
       "\n",
       "          2019         2020         2021         2022  \n",
       "0  21521395000  21322950000  23594031000  25744108000  \n",
       "1  18889076000  18612228000  20784824000  22807496000  \n",
       "2    162043000    160778000    225670000    270820000  \n",
       "3    293865000    201564000    332009000    457438000  \n",
       "4    331218000    344769000    386726000    438206000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GDP.head() # as we see we have the data for the counties and also for the states. We want to create 2 separate datasets based on this (will be easier afterwards if we do this)\n",
    "# We also have data for each industry. We only care for the option \"All industry total\", which is the total GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "03aec10a-9162-4ba7-89a2-d5114153e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDP = GDP[GDP['Description'] == \"All industry total \"]\n",
    "GDP['GeoFIPS'] = GDP['GeoFIPS'].str.replace('\"', '', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "191f3936-863a-4fc0-a92d-8804d1acf097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the 2 datasets based on the data of States and for counties\n",
    "# GDP per state\n",
    "state_names = [\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"]\n",
    "GDP_state = GDP[GDP['GeoName'].isin(state_names)]\n",
    "\n",
    "# GDP per county\n",
    "state_others = [\"United States *\", \"New England\", \"Mideast\", \"Great Lakes\",\"Plains\", \"Southeast\", \"Southwest\",\"Rocky Mountain\", \"Far West\", \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"]\n",
    "GDP_county = GDP[~GDP['GeoName'].isin(state_others)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f50e1ef-3df8-4ed6-843a-723ec5bec552",
   "metadata": {},
   "source": [
    "We now procede with the manipulation for the GDP for each County "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c8e7d69d-6c5b-469e-82aa-da04e093fadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3119 entries, 68 to 107746\n",
      "Data columns (total 30 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   GeoFIPS                 3119 non-null   object \n",
      " 1   GeoName                 3119 non-null   object \n",
      " 2   Region                  3119 non-null   object \n",
      " 3   TableName               3119 non-null   object \n",
      " 4   LineCode                3119 non-null   float64\n",
      " 5   IndustryClassification  3119 non-null   object \n",
      " 6   Description             3119 non-null   object \n",
      " 7   Unit                    3119 non-null   object \n",
      " 8   2001                    3119 non-null   object \n",
      " 9   2002                    3119 non-null   object \n",
      " 10  2003                    3119 non-null   object \n",
      " 11  2004                    3119 non-null   object \n",
      " 12  2005                    3119 non-null   object \n",
      " 13  2006                    3119 non-null   object \n",
      " 14  2007                    3119 non-null   object \n",
      " 15  2008                    3119 non-null   object \n",
      " 16  2009                    3119 non-null   object \n",
      " 17  2010                    3119 non-null   object \n",
      " 18  2011                    3119 non-null   object \n",
      " 19  2012                    3119 non-null   object \n",
      " 20  2013                    3119 non-null   object \n",
      " 21  2014                    3119 non-null   object \n",
      " 22  2015                    3119 non-null   object \n",
      " 23  2016                    3119 non-null   object \n",
      " 24  2017                    3119 non-null   object \n",
      " 25  2018                    3119 non-null   object \n",
      " 26  2019                    3119 non-null   object \n",
      " 27  2020                    3119 non-null   object \n",
      " 28  2021                    3119 non-null   object \n",
      " 29  2022                    3119 non-null   object \n",
      "dtypes: float64(1), object(29)\n",
      "memory usage: 755.4+ KB\n"
     ]
    }
   ],
   "source": [
    "GDP_county.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2c7bba57-fa73-4781-89ec-f7cc1fdf0a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicar\\AppData\\Local\\Temp\\ipykernel_5224\\622782924.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  GDP_county.drop(columns=['Region', 'TableName',  'LineCode',  'IndustryClassification' , 'Description', 'Unit'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "GDP_county.drop(columns=['Region', 'TableName',  'LineCode',  'IndustryClassification' , 'Description', 'Unit'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "192e6af3-81d1-4aaa-8256-5e5b37897e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicar\\AppData\\Local\\Temp\\ipykernel_5224\\4072028309.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  GDP_county['County'] = GDP_county['GeoName'].str.split(',').str[0] # we notice that the \"GeoName\" has a structure of \"coutny name , state abreviation\"-> we only want the county name so we separate it\n",
      "C:\\Users\\nicar\\AppData\\Local\\Temp\\ipykernel_5224\\4072028309.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  GDP_county.drop(columns=['GeoName'], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GeoFIPS</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>...</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>County</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>01001</td>\n",
       "      <td>760973</td>\n",
       "      <td>800045</td>\n",
       "      <td>834208</td>\n",
       "      <td>984461</td>\n",
       "      <td>1015320</td>\n",
       "      <td>1124089</td>\n",
       "      <td>1182234</td>\n",
       "      <td>1097703</td>\n",
       "      <td>1175769</td>\n",
       "      <td>...</td>\n",
       "      <td>1569120</td>\n",
       "      <td>1729098</td>\n",
       "      <td>1806246</td>\n",
       "      <td>1762558</td>\n",
       "      <td>1826642</td>\n",
       "      <td>1804013</td>\n",
       "      <td>1813553</td>\n",
       "      <td>1947622</td>\n",
       "      <td>2364891</td>\n",
       "      <td>Autauga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>01003</td>\n",
       "      <td>3128482</td>\n",
       "      <td>3369358</td>\n",
       "      <td>3642003</td>\n",
       "      <td>4117129</td>\n",
       "      <td>4717641</td>\n",
       "      <td>5031537</td>\n",
       "      <td>5330102</td>\n",
       "      <td>5238424</td>\n",
       "      <td>5103539</td>\n",
       "      <td>...</td>\n",
       "      <td>6034727</td>\n",
       "      <td>6492574</td>\n",
       "      <td>6983037</td>\n",
       "      <td>7382558</td>\n",
       "      <td>7935575</td>\n",
       "      <td>8563693</td>\n",
       "      <td>8762106</td>\n",
       "      <td>9879022</td>\n",
       "      <td>10811119</td>\n",
       "      <td>Baldwin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>01005</td>\n",
       "      <td>642028</td>\n",
       "      <td>637765</td>\n",
       "      <td>664978</td>\n",
       "      <td>741585</td>\n",
       "      <td>748820</td>\n",
       "      <td>744004</td>\n",
       "      <td>739799</td>\n",
       "      <td>713602</td>\n",
       "      <td>721550</td>\n",
       "      <td>...</td>\n",
       "      <td>779299</td>\n",
       "      <td>765000</td>\n",
       "      <td>757473</td>\n",
       "      <td>761761</td>\n",
       "      <td>789103</td>\n",
       "      <td>793908</td>\n",
       "      <td>786529</td>\n",
       "      <td>862519</td>\n",
       "      <td>930064</td>\n",
       "      <td>Barbour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>01007</td>\n",
       "      <td>217109</td>\n",
       "      <td>224813</td>\n",
       "      <td>242515</td>\n",
       "      <td>265078</td>\n",
       "      <td>274898</td>\n",
       "      <td>282976</td>\n",
       "      <td>310906</td>\n",
       "      <td>317444</td>\n",
       "      <td>313213</td>\n",
       "      <td>...</td>\n",
       "      <td>381354</td>\n",
       "      <td>377535</td>\n",
       "      <td>392287</td>\n",
       "      <td>406741</td>\n",
       "      <td>408791</td>\n",
       "      <td>466293</td>\n",
       "      <td>501320</td>\n",
       "      <td>528278</td>\n",
       "      <td>558848</td>\n",
       "      <td>Bibb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>01009</td>\n",
       "      <td>622473</td>\n",
       "      <td>637704</td>\n",
       "      <td>677995</td>\n",
       "      <td>721248</td>\n",
       "      <td>746559</td>\n",
       "      <td>741471</td>\n",
       "      <td>759827</td>\n",
       "      <td>788183</td>\n",
       "      <td>791532</td>\n",
       "      <td>...</td>\n",
       "      <td>928552</td>\n",
       "      <td>987510</td>\n",
       "      <td>925988</td>\n",
       "      <td>987967</td>\n",
       "      <td>1064218</td>\n",
       "      <td>1058218</td>\n",
       "      <td>973414</td>\n",
       "      <td>1153245</td>\n",
       "      <td>1282006</td>\n",
       "      <td>Blount</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    GeoFIPS     2001     2002     2003     2004     2005     2006     2007  \\\n",
       "68    01001   760973   800045   834208   984461  1015320  1124089  1182234   \n",
       "102   01003  3128482  3369358  3642003  4117129  4717641  5031537  5330102   \n",
       "136   01005   642028   637765   664978   741585   748820   744004   739799   \n",
       "170   01007   217109   224813   242515   265078   274898   282976   310906   \n",
       "204   01009   622473   637704   677995   721248   746559   741471   759827   \n",
       "\n",
       "        2008     2009  ...     2014     2015     2016     2017     2018  \\\n",
       "68   1097703  1175769  ...  1569120  1729098  1806246  1762558  1826642   \n",
       "102  5238424  5103539  ...  6034727  6492574  6983037  7382558  7935575   \n",
       "136   713602   721550  ...   779299   765000   757473   761761   789103   \n",
       "170   317444   313213  ...   381354   377535   392287   406741   408791   \n",
       "204   788183   791532  ...   928552   987510   925988   987967  1064218   \n",
       "\n",
       "        2019     2020     2021      2022   County  \n",
       "68   1804013  1813553  1947622   2364891  Autauga  \n",
       "102  8563693  8762106  9879022  10811119  Baldwin  \n",
       "136   793908   786529   862519    930064  Barbour  \n",
       "170   466293   501320   528278    558848     Bibb  \n",
       "204  1058218   973414  1153245   1282006   Blount  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GDP_county['County'] = GDP_county['GeoName'].str.split(',').str[0] # we notice that the \"GeoName\" has a structure of \"coutny name , state abreviation\"-> we only want the county name so we separate it\n",
    "GDP_county.drop(columns=['GeoName'], inplace=True)\n",
    "GDP_county.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fbfd5be2-3447-4869-a735-9f3a7b719226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique counties: 1838\n"
     ]
    }
   ],
   "source": [
    "unique_count = GDP_county['County'].nunique()\n",
    "\n",
    "print(f\"Number of unique counties: {unique_count}\") # we see that many places have the same name (as the quantity of unique values has another number)\n",
    "# we can use the FIPS to differentiate them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b7f883fd-56c9-4a2f-8398-a8bd29cde2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GeoFIPS</th>\n",
       "      <th>GeoName</th>\n",
       "      <th>Region</th>\n",
       "      <th>TableName</th>\n",
       "      <th>LineCode</th>\n",
       "      <th>IndustryClassification</th>\n",
       "      <th>Description</th>\n",
       "      <th>Unit</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>...</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>01000</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>5</td>\n",
       "      <td>CAGDP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>All industry total</td>\n",
       "      <td>Thousands of dollars</td>\n",
       "      <td>123534517</td>\n",
       "      <td>128380896</td>\n",
       "      <td>...</td>\n",
       "      <td>193981544</td>\n",
       "      <td>197064403</td>\n",
       "      <td>203113340</td>\n",
       "      <td>208824280</td>\n",
       "      <td>216615470</td>\n",
       "      <td>226263784</td>\n",
       "      <td>234526408</td>\n",
       "      <td>235118280</td>\n",
       "      <td>257986516</td>\n",
       "      <td>281569005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>02000</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>8</td>\n",
       "      <td>CAGDP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>All industry total</td>\n",
       "      <td>Thousands of dollars</td>\n",
       "      <td>28660864</td>\n",
       "      <td>29884530</td>\n",
       "      <td>...</td>\n",
       "      <td>57532958</td>\n",
       "      <td>56587381</td>\n",
       "      <td>51574180</td>\n",
       "      <td>51121833</td>\n",
       "      <td>53550915</td>\n",
       "      <td>54761954</td>\n",
       "      <td>54469922</td>\n",
       "      <td>51261531</td>\n",
       "      <td>58646002</td>\n",
       "      <td>65698817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3536</th>\n",
       "      <td>04000</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>6</td>\n",
       "      <td>CAGDP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>All industry total</td>\n",
       "      <td>Thousands of dollars</td>\n",
       "      <td>172899646</td>\n",
       "      <td>180844454</td>\n",
       "      <td>...</td>\n",
       "      <td>278679501</td>\n",
       "      <td>286675965</td>\n",
       "      <td>298942157</td>\n",
       "      <td>313787090</td>\n",
       "      <td>333098997</td>\n",
       "      <td>353670978</td>\n",
       "      <td>375544988</td>\n",
       "      <td>386443549</td>\n",
       "      <td>432279816</td>\n",
       "      <td>475653697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4080</th>\n",
       "      <td>05000</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>5</td>\n",
       "      <td>CAGDP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>All industry total</td>\n",
       "      <td>Thousands of dollars</td>\n",
       "      <td>71099645</td>\n",
       "      <td>74453133</td>\n",
       "      <td>...</td>\n",
       "      <td>114021074</td>\n",
       "      <td>117388715</td>\n",
       "      <td>118573554</td>\n",
       "      <td>120987446</td>\n",
       "      <td>123882586</td>\n",
       "      <td>129213792</td>\n",
       "      <td>132637152</td>\n",
       "      <td>135884518</td>\n",
       "      <td>151931913</td>\n",
       "      <td>165989269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6664</th>\n",
       "      <td>06000</td>\n",
       "      <td>California</td>\n",
       "      <td>8</td>\n",
       "      <td>CAGDP2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>All industry total</td>\n",
       "      <td>Thousands of dollars</td>\n",
       "      <td>1373270815</td>\n",
       "      <td>1419321823</td>\n",
       "      <td>...</td>\n",
       "      <td>2223300903</td>\n",
       "      <td>2342218803</td>\n",
       "      <td>2487156418</td>\n",
       "      <td>2586485176</td>\n",
       "      <td>2740550256</td>\n",
       "      <td>2899530864</td>\n",
       "      <td>3062158945</td>\n",
       "      <td>3068809395</td>\n",
       "      <td>3416939441</td>\n",
       "      <td>3641643425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GeoFIPS     GeoName Region TableName  LineCode IndustryClassification  \\\n",
       "34     01000     Alabama      5    CAGDP2       1.0                    ...   \n",
       "2346   02000      Alaska      8    CAGDP2       1.0                    ...   \n",
       "3536   04000     Arizona      6    CAGDP2       1.0                    ...   \n",
       "4080   05000    Arkansas      5    CAGDP2       1.0                    ...   \n",
       "6664   06000  California      8    CAGDP2       1.0                    ...   \n",
       "\n",
       "              Description                  Unit        2001        2002  ...  \\\n",
       "34    All industry total   Thousands of dollars   123534517   128380896  ...   \n",
       "2346  All industry total   Thousands of dollars    28660864    29884530  ...   \n",
       "3536  All industry total   Thousands of dollars   172899646   180844454  ...   \n",
       "4080  All industry total   Thousands of dollars    71099645    74453133  ...   \n",
       "6664  All industry total   Thousands of dollars  1373270815  1419321823  ...   \n",
       "\n",
       "            2013        2014        2015        2016        2017        2018  \\\n",
       "34     193981544   197064403   203113340   208824280   216615470   226263784   \n",
       "2346    57532958    56587381    51574180    51121833    53550915    54761954   \n",
       "3536   278679501   286675965   298942157   313787090   333098997   353670978   \n",
       "4080   114021074   117388715   118573554   120987446   123882586   129213792   \n",
       "6664  2223300903  2342218803  2487156418  2586485176  2740550256  2899530864   \n",
       "\n",
       "            2019        2020        2021        2022  \n",
       "34     234526408   235118280   257986516   281569005  \n",
       "2346    54469922    51261531    58646002    65698817  \n",
       "3536   375544988   386443549   432279816   475653697  \n",
       "4080   132637152   135884518   151931913   165989269  \n",
       "6664  3062158945  3068809395  3416939441  3641643425  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GDP_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "663262c0-7050-42a2-84d2-88ec4c9f49b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicar\\AppData\\Local\\Temp\\ipykernel_5224\\2736625036.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  GDP_state.drop(columns=['Region', 'TableName',  'LineCode',  'IndustryClassification' , 'Description', 'Unit'], inplace=True)\n",
      "C:\\Users\\nicar\\AppData\\Local\\Temp\\ipykernel_5224\\2736625036.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  GDP_state.rename(columns={'GeoName': 'State'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# we do now the same thing for the state GDP\n",
    "GDP_state.drop(columns=['Region', 'TableName',  'LineCode',  'IndustryClassification' , 'Description', 'Unit'], inplace=True)\n",
    "GDP_state.rename(columns={'GeoName': 'State'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd205dd1-c950-4754-981c-01db1b4cbeab",
   "metadata": {},
   "source": [
    "## Population per county and per State download and maipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "674ebc90-f467-4c22-9c47-fb9ee2b71410",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1zUGdtsDXcrBBoyTwuP9eWOOuQInFYOhe\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Pop_2000_2010.csv\n",
      "100%|██████████| 371k/371k [00:00<00:00, 4.51MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we download the data of the population\n",
    "# We start with the 2000-2010 population dataset\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1zUGdtsDXcrBBoyTwuP9eWOOuQInFYOhe/view?usp=sharing\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Pop_2000_2010.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7203f510-b6d8-44b4-9a29-f5c7fc139519",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop_2000_2010 = pd.read_csv(\"Pop_2000_2010.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d8439d96-580c-4f00-a76b-2b2eb30391e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1mBvNlAN0mfSXrstvCONp2ApTEdsycdva\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Pop_2010_2020.csv\n",
      "100%|██████████| 3.87M/3.87M [00:00<00:00, 10.2MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Population 2010-2020 dataset\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1mBvNlAN0mfSXrstvCONp2ApTEdsycdva/view?usp=sharing\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Pop_2010_2020.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b800c7c4-9b97-4079-a140-8e45d3adc61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop_2010_2020 = pd.read_csv(\"Pop_2010_2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1bd57747-1d99-49a6-a531-04af51d39146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1wNNgknDem9h_SHdpNlDbJqjavQ7Lxfbp\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\Pop_2020_2023.csv\n",
      "100%|██████████| 1.35M/1.35M [00:00<00:00, 7.42MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Population 2020-2023 dataset\n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1wNNgknDem9h_SHdpNlDbJqjavQ7Lxfbp/view?usp=sharing\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"Pop_2020_2023.csv\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9547b415-a2a2-4338-b700-fcbdf4d743e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pop_2020_2023 = pd.read_csv(\"Pop_2020_2023.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "041eb57f-4627-4ac6-b380-834cda8df5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3194 entries, 0 to 3193\n",
      "Data columns (total 20 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   SUMLEV             3194 non-null   int64 \n",
      " 1   REGION             3194 non-null   int64 \n",
      " 2   DIVISION           3194 non-null   int64 \n",
      " 3   STATE              3194 non-null   int64 \n",
      " 4   COUNTY             3194 non-null   int64 \n",
      " 5   STNAME             3194 non-null   object\n",
      " 6   CTYNAME            3194 non-null   object\n",
      " 7   ESTIMATESBASE2000  3194 non-null   int64 \n",
      " 8   POPESTIMATE2000    3194 non-null   int64 \n",
      " 9   POPESTIMATE2001    3194 non-null   int64 \n",
      " 10  POPESTIMATE2002    3194 non-null   int64 \n",
      " 11  POPESTIMATE2003    3194 non-null   int64 \n",
      " 12  POPESTIMATE2004    3194 non-null   int64 \n",
      " 13  POPESTIMATE2005    3194 non-null   int64 \n",
      " 14  POPESTIMATE2006    3194 non-null   int64 \n",
      " 15  POPESTIMATE2007    3194 non-null   int64 \n",
      " 16  POPESTIMATE2008    3194 non-null   int64 \n",
      " 17  POPESTIMATE2009    3194 non-null   int64 \n",
      " 18  CENSUS2010POP      3194 non-null   int64 \n",
      " 19  POPESTIMATE2010    3194 non-null   int64 \n",
      "dtypes: int64(18), object(2)\n",
      "memory usage: 499.2+ KB\n",
      "None\n",
      "Index(['STATE', 'COUNTY', 'CTYNAME', 'POPESTIMATE2000', 'POPESTIMATE2001',\n",
      "       'POPESTIMATE2002', 'POPESTIMATE2003', 'POPESTIMATE2004',\n",
      "       'POPESTIMATE2005', 'POPESTIMATE2006', 'POPESTIMATE2007',\n",
      "       'POPESTIMATE2008', 'POPESTIMATE2009', 'CENSUS2010POP'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Now we want to keep only the interesting things in each dataset, we start by looking at the 2000-2010 dataset\n",
    "print(Pop_2000_2010.info())\n",
    "# we are only interested in the following columns: State, County, POPESTIMATE2000, POPESTIMATE2001, POPESTIMATE2002, POPESTIMATE2003, POPESTIMATE208, POPESTIMATE20004, POPESTIMATE2005, POPESTIMATE2006, POPESTIMATE2007, POPESTIMATE2008, POPESTIMATE2009, CENSUS2010POP\n",
    "# thus now we delete the rest of the columns\n",
    "Pop_2000_2010 = Pop_2000_2010[[\"STATE\", \"COUNTY\", \"CTYNAME\", \"POPESTIMATE2000\", \"POPESTIMATE2001\", \"POPESTIMATE2002\",\"POPESTIMATE2003\", \"POPESTIMATE2004\", \"POPESTIMATE2005\", \"POPESTIMATE2006\", \"POPESTIMATE2007\", \"POPESTIMATE2008\", \"POPESTIMATE2009\", \"CENSUS2010POP\"]]\n",
    "print(Pop_2000_2010.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "71a6c812-9178-449b-8cff-41fd67cbe870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>CTYNAME</th>\n",
       "      <th>POPESTIMATE2000</th>\n",
       "      <th>POPESTIMATE2001</th>\n",
       "      <th>POPESTIMATE2002</th>\n",
       "      <th>POPESTIMATE2003</th>\n",
       "      <th>POPESTIMATE2004</th>\n",
       "      <th>POPESTIMATE2005</th>\n",
       "      <th>POPESTIMATE2006</th>\n",
       "      <th>POPESTIMATE2007</th>\n",
       "      <th>POPESTIMATE2008</th>\n",
       "      <th>POPESTIMATE2009</th>\n",
       "      <th>POPESTIMATE2010</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>4452173</td>\n",
       "      <td>4467634</td>\n",
       "      <td>4480089</td>\n",
       "      <td>4503491</td>\n",
       "      <td>4530729</td>\n",
       "      <td>4569805</td>\n",
       "      <td>4628981</td>\n",
       "      <td>4672840</td>\n",
       "      <td>4718206</td>\n",
       "      <td>4757938</td>\n",
       "      <td>4779736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>44021</td>\n",
       "      <td>44889</td>\n",
       "      <td>45909</td>\n",
       "      <td>46800</td>\n",
       "      <td>48366</td>\n",
       "      <td>49676</td>\n",
       "      <td>51328</td>\n",
       "      <td>52405</td>\n",
       "      <td>53277</td>\n",
       "      <td>54135</td>\n",
       "      <td>54571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>141342</td>\n",
       "      <td>144875</td>\n",
       "      <td>147957</td>\n",
       "      <td>151509</td>\n",
       "      <td>156266</td>\n",
       "      <td>162183</td>\n",
       "      <td>168121</td>\n",
       "      <td>172404</td>\n",
       "      <td>175827</td>\n",
       "      <td>179406</td>\n",
       "      <td>182265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>29015</td>\n",
       "      <td>28863</td>\n",
       "      <td>28653</td>\n",
       "      <td>28594</td>\n",
       "      <td>28287</td>\n",
       "      <td>28027</td>\n",
       "      <td>27861</td>\n",
       "      <td>27757</td>\n",
       "      <td>27808</td>\n",
       "      <td>27657</td>\n",
       "      <td>27457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>19913</td>\n",
       "      <td>21028</td>\n",
       "      <td>21199</td>\n",
       "      <td>21399</td>\n",
       "      <td>21721</td>\n",
       "      <td>22042</td>\n",
       "      <td>22099</td>\n",
       "      <td>22438</td>\n",
       "      <td>22705</td>\n",
       "      <td>22941</td>\n",
       "      <td>22915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE  COUNTY         CTYNAME  POPESTIMATE2000  POPESTIMATE2001  \\\n",
       "0      1       0         Alabama          4452173          4467634   \n",
       "1      1       1  Autauga County            44021            44889   \n",
       "2      1       3  Baldwin County           141342           144875   \n",
       "3      1       5  Barbour County            29015            28863   \n",
       "4      1       7     Bibb County            19913            21028   \n",
       "\n",
       "   POPESTIMATE2002  POPESTIMATE2003  POPESTIMATE2004  POPESTIMATE2005  \\\n",
       "0          4480089          4503491          4530729          4569805   \n",
       "1            45909            46800            48366            49676   \n",
       "2           147957           151509           156266           162183   \n",
       "3            28653            28594            28287            28027   \n",
       "4            21199            21399            21721            22042   \n",
       "\n",
       "   POPESTIMATE2006  POPESTIMATE2007  POPESTIMATE2008  POPESTIMATE2009  \\\n",
       "0          4628981          4672840          4718206          4757938   \n",
       "1            51328            52405            53277            54135   \n",
       "2           168121           172404           175827           179406   \n",
       "3            27861            27757            27808            27657   \n",
       "4            22099            22438            22705            22941   \n",
       "\n",
       "   POPESTIMATE2010  \n",
       "0          4779736  \n",
       "1            54571  \n",
       "2           182265  \n",
       "3            27457  \n",
       "4            22915  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pop_2000_2010.rename(columns={'CENSUS2010POP': 'POPESTIMATE2010'}, inplace=True) # we use the actual census for 2010 but we rename the column so that aftwerards we can automize some process\n",
    "Pop_2000_2010.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d6120e92-3cc9-4285-bbba-073fdf34c0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3194 entries, 0 to 3193\n",
      "Columns: 180 entries, SUMLEV to RNETMIG2020\n",
      "dtypes: float64(60), int64(117), object(3)\n",
      "memory usage: 4.4+ MB\n",
      "None\n",
      "Index(['STATE', 'COUNTY', 'CTYNAME', 'POPESTIMATE2011', 'POPESTIMATE2012',\n",
      "       'POPESTIMATE2013', 'POPESTIMATE2014', 'POPESTIMATE2015',\n",
      "       'POPESTIMATE2016', 'POPESTIMATE2017', 'POPESTIMATE2018',\n",
      "       'POPESTIMATE2019', 'POPESTIMATE2020'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# We do the same for the rest of the population datasets\n",
    "# Population 2010-2020\n",
    "print(Pop_2010_2020.info())\n",
    "# we are only interested in the following columns: State, County, POPESTIMATE2011, POPESTIMATE2012, POPESTIMATE2013, POPESTIMATE2014, POPESTIMATE2015, POPESTIMATE2016, POPESTIMATE2017, POPESTIMATE2018, POPESTIMATE2019,POPESTIMATE2020\n",
    "# thus now we delete the rest of the columns\n",
    "Pop_2010_2020 = Pop_2010_2020[[\"STATE\", \"COUNTY\", \"CTYNAME\", \"POPESTIMATE2011\", \"POPESTIMATE2012\", \"POPESTIMATE2013\",\"POPESTIMATE2014\", \"POPESTIMATE2015\", \"POPESTIMATE2016\", \"POPESTIMATE2017\", \"POPESTIMATE2018\", \"POPESTIMATE2019\", \"POPESTIMATE2020\"]]\n",
    "print(Pop_2010_2020.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "80a3521f-2526-4498-9a97-65accf815ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3195 entries, 0 to 3194\n",
      "Data columns (total 67 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   SUMLEV                 3195 non-null   int64  \n",
      " 1   REGION                 3195 non-null   int64  \n",
      " 2   DIVISION               3195 non-null   int64  \n",
      " 3   STATE                  3195 non-null   int64  \n",
      " 4   COUNTY                 3195 non-null   int64  \n",
      " 5   STNAME                 3195 non-null   object \n",
      " 6   CTYNAME                3195 non-null   object \n",
      " 7   ESTIMATESBASE2020      3195 non-null   int64  \n",
      " 8   POPESTIMATE2020        3195 non-null   int64  \n",
      " 9   POPESTIMATE2021        3195 non-null   int64  \n",
      " 10  POPESTIMATE2022        3195 non-null   int64  \n",
      " 11  POPESTIMATE2023        3195 non-null   int64  \n",
      " 12  NPOPCHG2020            3195 non-null   int64  \n",
      " 13  NPOPCHG2021            3195 non-null   int64  \n",
      " 14  NPOPCHG2022            3195 non-null   int64  \n",
      " 15  NPOPCHG2023            3195 non-null   int64  \n",
      " 16  BIRTHS2020             3195 non-null   int64  \n",
      " 17  BIRTHS2021             3195 non-null   int64  \n",
      " 18  BIRTHS2022             3195 non-null   int64  \n",
      " 19  BIRTHS2023             3195 non-null   int64  \n",
      " 20  DEATHS2020             3195 non-null   int64  \n",
      " 21  DEATHS2021             3195 non-null   int64  \n",
      " 22  DEATHS2022             3195 non-null   int64  \n",
      " 23  DEATHS2023             3195 non-null   int64  \n",
      " 24  NATURALCHG2020         3195 non-null   int64  \n",
      " 25  NATURALCHG2021         3195 non-null   int64  \n",
      " 26  NATURALCHG2022         3195 non-null   int64  \n",
      " 27  NATURALCHG2023         3195 non-null   int64  \n",
      " 28  INTERNATIONALMIG2020   3195 non-null   int64  \n",
      " 29  INTERNATIONALMIG2021   3195 non-null   int64  \n",
      " 30  INTERNATIONALMIG2022   3195 non-null   int64  \n",
      " 31  INTERNATIONALMIG2023   3195 non-null   int64  \n",
      " 32  DOMESTICMIG2020        3195 non-null   int64  \n",
      " 33  DOMESTICMIG2021        3195 non-null   int64  \n",
      " 34  DOMESTICMIG2022        3195 non-null   int64  \n",
      " 35  DOMESTICMIG2023        3195 non-null   int64  \n",
      " 36  NETMIG2020             3195 non-null   int64  \n",
      " 37  NETMIG2021             3195 non-null   int64  \n",
      " 38  NETMIG2022             3195 non-null   int64  \n",
      " 39  NETMIG2023             3195 non-null   int64  \n",
      " 40  RESIDUAL2020           3195 non-null   int64  \n",
      " 41  RESIDUAL2021           3195 non-null   int64  \n",
      " 42  RESIDUAL2022           3195 non-null   int64  \n",
      " 43  RESIDUAL2023           3195 non-null   int64  \n",
      " 44  GQESTIMATESBASE2020    3195 non-null   int64  \n",
      " 45  GQESTIMATES2020        3195 non-null   int64  \n",
      " 46  GQESTIMATES2021        3195 non-null   int64  \n",
      " 47  GQESTIMATES2022        3195 non-null   int64  \n",
      " 48  GQESTIMATES2023        3195 non-null   int64  \n",
      " 49  RBIRTH2021             3195 non-null   float64\n",
      " 50  RBIRTH2022             3195 non-null   float64\n",
      " 51  RBIRTH2023             3195 non-null   float64\n",
      " 52  RDEATH2021             3195 non-null   float64\n",
      " 53  RDEATH2022             3195 non-null   float64\n",
      " 54  RDEATH2023             3195 non-null   float64\n",
      " 55  RNATURALCHG2021        3195 non-null   float64\n",
      " 56  RNATURALCHG2022        3195 non-null   float64\n",
      " 57  RNATURALCHG2023        3195 non-null   float64\n",
      " 58  RINTERNATIONALMIG2021  3195 non-null   float64\n",
      " 59  RINTERNATIONALMIG2022  3195 non-null   float64\n",
      " 60  RINTERNATIONALMIG2023  3195 non-null   float64\n",
      " 61  RDOMESTICMIG2021       3195 non-null   float64\n",
      " 62  RDOMESTICMIG2022       3195 non-null   float64\n",
      " 63  RDOMESTICMIG2023       3195 non-null   float64\n",
      " 64  RNETMIG2021            3195 non-null   float64\n",
      " 65  RNETMIG2022            3195 non-null   float64\n",
      " 66  RNETMIG2023            3195 non-null   float64\n",
      "dtypes: float64(18), int64(47), object(2)\n",
      "memory usage: 1.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Population 2020-2023\n",
    "print(Pop_2020_2023.info())\n",
    "# we are only interested in the following columns: State, County, POPESTIMATE2011, POPESTIMATE2012, POPESTIMATE2013, POPESTIMATE2014, POPESTIMATE2015, POPESTIMATE2016, POPESTIMATE2017, POPESTIMATE2018, POPESTIMATE2019,POPESTIMATE2020\n",
    "# thus now we delete the rest of the columns\n",
    "Pop_2020_2023 = Pop_2020_2023[[\"STATE\", \"COUNTY\", \"CTYNAME\", \"POPESTIMATE2021\", \"POPESTIMATE2022\", \"POPESTIMATE2023\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c1571d52-59d9-4e97-ba1e-c5aa64280d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3195 entries, 0 to 3194\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   STATE            3195 non-null   int64 \n",
      " 1   COUNTY           3195 non-null   int64 \n",
      " 2   CTYNAME          3195 non-null   object\n",
      " 3   POPESTIMATE2021  3195 non-null   int64 \n",
      " 4   POPESTIMATE2022  3195 non-null   int64 \n",
      " 5   POPESTIMATE2023  3195 non-null   int64 \n",
      "dtypes: int64(5), object(1)\n",
      "memory usage: 149.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(Pop_2020_2023.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2f13c1f3-3395-46fd-b728-c039a3b8fdc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>CTYNAME</th>\n",
       "      <th>POPESTIMATE2000</th>\n",
       "      <th>POPESTIMATE2001</th>\n",
       "      <th>POPESTIMATE2002</th>\n",
       "      <th>POPESTIMATE2003</th>\n",
       "      <th>POPESTIMATE2004</th>\n",
       "      <th>POPESTIMATE2005</th>\n",
       "      <th>POPESTIMATE2006</th>\n",
       "      <th>...</th>\n",
       "      <th>POPESTIMATE2011</th>\n",
       "      <th>POPESTIMATE2012</th>\n",
       "      <th>POPESTIMATE2013</th>\n",
       "      <th>POPESTIMATE2014</th>\n",
       "      <th>POPESTIMATE2015</th>\n",
       "      <th>POPESTIMATE2016</th>\n",
       "      <th>POPESTIMATE2017</th>\n",
       "      <th>POPESTIMATE2018</th>\n",
       "      <th>POPESTIMATE2019</th>\n",
       "      <th>POPESTIMATE2020</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>4452173</td>\n",
       "      <td>4467634</td>\n",
       "      <td>4480089</td>\n",
       "      <td>4503491</td>\n",
       "      <td>4530729</td>\n",
       "      <td>4569805</td>\n",
       "      <td>4628981</td>\n",
       "      <td>...</td>\n",
       "      <td>4799642</td>\n",
       "      <td>4816632</td>\n",
       "      <td>4831586</td>\n",
       "      <td>4843737</td>\n",
       "      <td>4854803</td>\n",
       "      <td>4866824</td>\n",
       "      <td>4877989</td>\n",
       "      <td>4891628</td>\n",
       "      <td>4907965</td>\n",
       "      <td>4921532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>44021</td>\n",
       "      <td>44889</td>\n",
       "      <td>45909</td>\n",
       "      <td>46800</td>\n",
       "      <td>48366</td>\n",
       "      <td>49676</td>\n",
       "      <td>51328</td>\n",
       "      <td>...</td>\n",
       "      <td>55229</td>\n",
       "      <td>54970</td>\n",
       "      <td>54747</td>\n",
       "      <td>54922</td>\n",
       "      <td>54903</td>\n",
       "      <td>55302</td>\n",
       "      <td>55448</td>\n",
       "      <td>55533</td>\n",
       "      <td>55769</td>\n",
       "      <td>56145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>141342</td>\n",
       "      <td>144875</td>\n",
       "      <td>147957</td>\n",
       "      <td>151509</td>\n",
       "      <td>156266</td>\n",
       "      <td>162183</td>\n",
       "      <td>168121</td>\n",
       "      <td>...</td>\n",
       "      <td>186579</td>\n",
       "      <td>190203</td>\n",
       "      <td>194978</td>\n",
       "      <td>199306</td>\n",
       "      <td>203101</td>\n",
       "      <td>207787</td>\n",
       "      <td>212737</td>\n",
       "      <td>218071</td>\n",
       "      <td>223565</td>\n",
       "      <td>229287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>29015</td>\n",
       "      <td>28863</td>\n",
       "      <td>28653</td>\n",
       "      <td>28594</td>\n",
       "      <td>28287</td>\n",
       "      <td>28027</td>\n",
       "      <td>27861</td>\n",
       "      <td>...</td>\n",
       "      <td>27344</td>\n",
       "      <td>27172</td>\n",
       "      <td>26946</td>\n",
       "      <td>26768</td>\n",
       "      <td>26300</td>\n",
       "      <td>25828</td>\n",
       "      <td>25169</td>\n",
       "      <td>24887</td>\n",
       "      <td>24657</td>\n",
       "      <td>24589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>19913</td>\n",
       "      <td>21028</td>\n",
       "      <td>21199</td>\n",
       "      <td>21399</td>\n",
       "      <td>21721</td>\n",
       "      <td>22042</td>\n",
       "      <td>22099</td>\n",
       "      <td>...</td>\n",
       "      <td>22736</td>\n",
       "      <td>22657</td>\n",
       "      <td>22510</td>\n",
       "      <td>22541</td>\n",
       "      <td>22553</td>\n",
       "      <td>22590</td>\n",
       "      <td>22532</td>\n",
       "      <td>22300</td>\n",
       "      <td>22313</td>\n",
       "      <td>22136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE  COUNTY         CTYNAME  POPESTIMATE2000  POPESTIMATE2001  \\\n",
       "0      1       0         Alabama          4452173          4467634   \n",
       "1      1       1  Autauga County            44021            44889   \n",
       "2      1       3  Baldwin County           141342           144875   \n",
       "3      1       5  Barbour County            29015            28863   \n",
       "4      1       7     Bibb County            19913            21028   \n",
       "\n",
       "   POPESTIMATE2002  POPESTIMATE2003  POPESTIMATE2004  POPESTIMATE2005  \\\n",
       "0          4480089          4503491          4530729          4569805   \n",
       "1            45909            46800            48366            49676   \n",
       "2           147957           151509           156266           162183   \n",
       "3            28653            28594            28287            28027   \n",
       "4            21199            21399            21721            22042   \n",
       "\n",
       "   POPESTIMATE2006  ...  POPESTIMATE2011  POPESTIMATE2012  POPESTIMATE2013  \\\n",
       "0          4628981  ...          4799642          4816632          4831586   \n",
       "1            51328  ...            55229            54970            54747   \n",
       "2           168121  ...           186579           190203           194978   \n",
       "3            27861  ...            27344            27172            26946   \n",
       "4            22099  ...            22736            22657            22510   \n",
       "\n",
       "   POPESTIMATE2014  POPESTIMATE2015  POPESTIMATE2016  POPESTIMATE2017  \\\n",
       "0          4843737          4854803          4866824          4877989   \n",
       "1            54922            54903            55302            55448   \n",
       "2           199306           203101           207787           212737   \n",
       "3            26768            26300            25828            25169   \n",
       "4            22541            22553            22590            22532   \n",
       "\n",
       "   POPESTIMATE2018  POPESTIMATE2019  POPESTIMATE2020  \n",
       "0          4891628          4907965          4921532  \n",
       "1            55533            55769            56145  \n",
       "2           218071           223565           229287  \n",
       "3            24887            24657            24589  \n",
       "4            22300            22313            22136  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we merge all the population datasets\n",
    "Population = pd.merge(Pop_2000_2010, Pop_2010_2020, on=[\"STATE\", \"COUNTY\", \"CTYNAME\"], how='inner')\n",
    "Population.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fd077b9e-ca73-4449-9fea-a8d3b562f7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>State</th>\n",
       "      <th>POPESTIMATE2000</th>\n",
       "      <th>POPESTIMATE2001</th>\n",
       "      <th>POPESTIMATE2002</th>\n",
       "      <th>POPESTIMATE2003</th>\n",
       "      <th>POPESTIMATE2004</th>\n",
       "      <th>POPESTIMATE2005</th>\n",
       "      <th>POPESTIMATE2006</th>\n",
       "      <th>...</th>\n",
       "      <th>POPESTIMATE2014</th>\n",
       "      <th>POPESTIMATE2015</th>\n",
       "      <th>POPESTIMATE2016</th>\n",
       "      <th>POPESTIMATE2017</th>\n",
       "      <th>POPESTIMATE2018</th>\n",
       "      <th>POPESTIMATE2019</th>\n",
       "      <th>POPESTIMATE2020</th>\n",
       "      <th>POPESTIMATE2021</th>\n",
       "      <th>POPESTIMATE2022</th>\n",
       "      <th>POPESTIMATE2023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>4452173</td>\n",
       "      <td>4467634</td>\n",
       "      <td>4480089</td>\n",
       "      <td>4503491</td>\n",
       "      <td>4530729</td>\n",
       "      <td>4569805</td>\n",
       "      <td>4628981</td>\n",
       "      <td>...</td>\n",
       "      <td>4843737</td>\n",
       "      <td>4854803</td>\n",
       "      <td>4866824</td>\n",
       "      <td>4877989</td>\n",
       "      <td>4891628</td>\n",
       "      <td>4907965</td>\n",
       "      <td>4921532</td>\n",
       "      <td>5050380</td>\n",
       "      <td>5073903</td>\n",
       "      <td>5108468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>44021</td>\n",
       "      <td>44889</td>\n",
       "      <td>45909</td>\n",
       "      <td>46800</td>\n",
       "      <td>48366</td>\n",
       "      <td>49676</td>\n",
       "      <td>51328</td>\n",
       "      <td>...</td>\n",
       "      <td>54922</td>\n",
       "      <td>54903</td>\n",
       "      <td>55302</td>\n",
       "      <td>55448</td>\n",
       "      <td>55533</td>\n",
       "      <td>55769</td>\n",
       "      <td>56145</td>\n",
       "      <td>59203</td>\n",
       "      <td>59726</td>\n",
       "      <td>60342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>141342</td>\n",
       "      <td>144875</td>\n",
       "      <td>147957</td>\n",
       "      <td>151509</td>\n",
       "      <td>156266</td>\n",
       "      <td>162183</td>\n",
       "      <td>168121</td>\n",
       "      <td>...</td>\n",
       "      <td>199306</td>\n",
       "      <td>203101</td>\n",
       "      <td>207787</td>\n",
       "      <td>212737</td>\n",
       "      <td>218071</td>\n",
       "      <td>223565</td>\n",
       "      <td>229287</td>\n",
       "      <td>239439</td>\n",
       "      <td>246531</td>\n",
       "      <td>253507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>29015</td>\n",
       "      <td>28863</td>\n",
       "      <td>28653</td>\n",
       "      <td>28594</td>\n",
       "      <td>28287</td>\n",
       "      <td>28027</td>\n",
       "      <td>27861</td>\n",
       "      <td>...</td>\n",
       "      <td>26768</td>\n",
       "      <td>26300</td>\n",
       "      <td>25828</td>\n",
       "      <td>25169</td>\n",
       "      <td>24887</td>\n",
       "      <td>24657</td>\n",
       "      <td>24589</td>\n",
       "      <td>24533</td>\n",
       "      <td>24700</td>\n",
       "      <td>24585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>19913</td>\n",
       "      <td>21028</td>\n",
       "      <td>21199</td>\n",
       "      <td>21399</td>\n",
       "      <td>21721</td>\n",
       "      <td>22042</td>\n",
       "      <td>22099</td>\n",
       "      <td>...</td>\n",
       "      <td>22541</td>\n",
       "      <td>22553</td>\n",
       "      <td>22590</td>\n",
       "      <td>22532</td>\n",
       "      <td>22300</td>\n",
       "      <td>22313</td>\n",
       "      <td>22136</td>\n",
       "      <td>22359</td>\n",
       "      <td>21986</td>\n",
       "      <td>21868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE  COUNTY           State  POPESTIMATE2000  POPESTIMATE2001  \\\n",
       "0      1       0         Alabama          4452173          4467634   \n",
       "1      1       1  Autauga County            44021            44889   \n",
       "2      1       3  Baldwin County           141342           144875   \n",
       "3      1       5  Barbour County            29015            28863   \n",
       "4      1       7     Bibb County            19913            21028   \n",
       "\n",
       "   POPESTIMATE2002  POPESTIMATE2003  POPESTIMATE2004  POPESTIMATE2005  \\\n",
       "0          4480089          4503491          4530729          4569805   \n",
       "1            45909            46800            48366            49676   \n",
       "2           147957           151509           156266           162183   \n",
       "3            28653            28594            28287            28027   \n",
       "4            21199            21399            21721            22042   \n",
       "\n",
       "   POPESTIMATE2006  ...  POPESTIMATE2014  POPESTIMATE2015  POPESTIMATE2016  \\\n",
       "0          4628981  ...          4843737          4854803          4866824   \n",
       "1            51328  ...            54922            54903            55302   \n",
       "2           168121  ...           199306           203101           207787   \n",
       "3            27861  ...            26768            26300            25828   \n",
       "4            22099  ...            22541            22553            22590   \n",
       "\n",
       "   POPESTIMATE2017  POPESTIMATE2018  POPESTIMATE2019  POPESTIMATE2020  \\\n",
       "0          4877989          4891628          4907965          4921532   \n",
       "1            55448            55533            55769            56145   \n",
       "2           212737           218071           223565           229287   \n",
       "3            25169            24887            24657            24589   \n",
       "4            22532            22300            22313            22136   \n",
       "\n",
       "   POPESTIMATE2021  POPESTIMATE2022  POPESTIMATE2023  \n",
       "0          5050380          5073903          5108468  \n",
       "1            59203            59726            60342  \n",
       "2           239439           246531           253507  \n",
       "3            24533            24700            24585  \n",
       "4            22359            21986            21868  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Population = pd.merge(Population, Pop_2020_2023, on=[\"STATE\", \"COUNTY\", \"CTYNAME\"], how='inner')\n",
    "Population = Population.rename(columns={'CTYNAME': 'State'})\n",
    "Population.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "81c52e62-42a4-454e-9180-3ff618c8b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we did for the GDP data we want to divide it based on the state or on the county \n",
    "# County level \n",
    "Population_county = Population[Population[\"COUNTY\"] != 0] # as we saw from the observation of the dataset, the states have the value = 0 when the county is = 0\n",
    "\n",
    "# State level\n",
    "Population_state = Population[Population[\"COUNTY\"] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020cb1dd-40dc-4832-a5be-b793ab6c8bca",
   "metadata": {},
   "source": [
    "## Calculation of GDP per capita per state and per county "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b6ace0-b990-4910-af87-697cb3565009",
   "metadata": {},
   "source": [
    "Since there is no data on the GDP per capita per state and county, we will need to calculate it ourselves. The calculation is very easy: we divide the GDP (of the respective county or state) with the Population. The year, obviously, has to be respected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e86e36d3-7ea4-4bfd-9625-0cd3ae21d5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicar\\AppData\\Local\\Temp\\ipykernel_5224\\4213616799.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Population_state.drop(columns = ['POPESTIMATE2000', 'POPESTIMATE2023'], inplace = True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>State</th>\n",
       "      <th>POPESTIMATE2001</th>\n",
       "      <th>POPESTIMATE2002</th>\n",
       "      <th>POPESTIMATE2003</th>\n",
       "      <th>POPESTIMATE2004</th>\n",
       "      <th>POPESTIMATE2005</th>\n",
       "      <th>POPESTIMATE2006</th>\n",
       "      <th>POPESTIMATE2007</th>\n",
       "      <th>...</th>\n",
       "      <th>POPESTIMATE2013</th>\n",
       "      <th>POPESTIMATE2014</th>\n",
       "      <th>POPESTIMATE2015</th>\n",
       "      <th>POPESTIMATE2016</th>\n",
       "      <th>POPESTIMATE2017</th>\n",
       "      <th>POPESTIMATE2018</th>\n",
       "      <th>POPESTIMATE2019</th>\n",
       "      <th>POPESTIMATE2020</th>\n",
       "      <th>POPESTIMATE2021</th>\n",
       "      <th>POPESTIMATE2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>4467634</td>\n",
       "      <td>4480089</td>\n",
       "      <td>4503491</td>\n",
       "      <td>4530729</td>\n",
       "      <td>4569805</td>\n",
       "      <td>4628981</td>\n",
       "      <td>4672840</td>\n",
       "      <td>...</td>\n",
       "      <td>4831586</td>\n",
       "      <td>4843737</td>\n",
       "      <td>4854803</td>\n",
       "      <td>4866824</td>\n",
       "      <td>4877989</td>\n",
       "      <td>4891628</td>\n",
       "      <td>4907965</td>\n",
       "      <td>4921532</td>\n",
       "      <td>5050380</td>\n",
       "      <td>5073903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>633714</td>\n",
       "      <td>642337</td>\n",
       "      <td>648414</td>\n",
       "      <td>659286</td>\n",
       "      <td>666946</td>\n",
       "      <td>675302</td>\n",
       "      <td>680300</td>\n",
       "      <td>...</td>\n",
       "      <td>737626</td>\n",
       "      <td>737075</td>\n",
       "      <td>738430</td>\n",
       "      <td>742575</td>\n",
       "      <td>740983</td>\n",
       "      <td>736624</td>\n",
       "      <td>733603</td>\n",
       "      <td>731158</td>\n",
       "      <td>734923</td>\n",
       "      <td>733276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>5273477</td>\n",
       "      <td>5396255</td>\n",
       "      <td>5510364</td>\n",
       "      <td>5652404</td>\n",
       "      <td>5839077</td>\n",
       "      <td>6029141</td>\n",
       "      <td>6167681</td>\n",
       "      <td>...</td>\n",
       "      <td>6634690</td>\n",
       "      <td>6732873</td>\n",
       "      <td>6832810</td>\n",
       "      <td>6944767</td>\n",
       "      <td>7048088</td>\n",
       "      <td>7164228</td>\n",
       "      <td>7291843</td>\n",
       "      <td>7421401</td>\n",
       "      <td>7272487</td>\n",
       "      <td>7365684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2691571</td>\n",
       "      <td>2705927</td>\n",
       "      <td>2724816</td>\n",
       "      <td>2749686</td>\n",
       "      <td>2781097</td>\n",
       "      <td>2821761</td>\n",
       "      <td>2848650</td>\n",
       "      <td>...</td>\n",
       "      <td>2960459</td>\n",
       "      <td>2968759</td>\n",
       "      <td>2979732</td>\n",
       "      <td>2991815</td>\n",
       "      <td>3003855</td>\n",
       "      <td>3012161</td>\n",
       "      <td>3020985</td>\n",
       "      <td>3030522</td>\n",
       "      <td>3028443</td>\n",
       "      <td>3046404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>California</td>\n",
       "      <td>34479458</td>\n",
       "      <td>34871843</td>\n",
       "      <td>35253159</td>\n",
       "      <td>35574576</td>\n",
       "      <td>35827943</td>\n",
       "      <td>36021202</td>\n",
       "      <td>36250311</td>\n",
       "      <td>...</td>\n",
       "      <td>38253768</td>\n",
       "      <td>38586706</td>\n",
       "      <td>38904296</td>\n",
       "      <td>39149186</td>\n",
       "      <td>39337785</td>\n",
       "      <td>39437463</td>\n",
       "      <td>39437610</td>\n",
       "      <td>39368078</td>\n",
       "      <td>39145060</td>\n",
       "      <td>39040616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     STATE  COUNTY       State  POPESTIMATE2001  POPESTIMATE2002  \\\n",
       "0        1       0     Alabama          4467634          4480089   \n",
       "68       2       0      Alaska           633714           642337   \n",
       "95       4       0     Arizona          5273477          5396255   \n",
       "111      5       0    Arkansas          2691571          2705927   \n",
       "187      6       0  California         34479458         34871843   \n",
       "\n",
       "     POPESTIMATE2003  POPESTIMATE2004  POPESTIMATE2005  POPESTIMATE2006  \\\n",
       "0            4503491          4530729          4569805          4628981   \n",
       "68            648414           659286           666946           675302   \n",
       "95           5510364          5652404          5839077          6029141   \n",
       "111          2724816          2749686          2781097          2821761   \n",
       "187         35253159         35574576         35827943         36021202   \n",
       "\n",
       "     POPESTIMATE2007  ...  POPESTIMATE2013  POPESTIMATE2014  POPESTIMATE2015  \\\n",
       "0            4672840  ...          4831586          4843737          4854803   \n",
       "68            680300  ...           737626           737075           738430   \n",
       "95           6167681  ...          6634690          6732873          6832810   \n",
       "111          2848650  ...          2960459          2968759          2979732   \n",
       "187         36250311  ...         38253768         38586706         38904296   \n",
       "\n",
       "     POPESTIMATE2016  POPESTIMATE2017  POPESTIMATE2018  POPESTIMATE2019  \\\n",
       "0            4866824          4877989          4891628          4907965   \n",
       "68            742575           740983           736624           733603   \n",
       "95           6944767          7048088          7164228          7291843   \n",
       "111          2991815          3003855          3012161          3020985   \n",
       "187         39149186         39337785         39437463         39437610   \n",
       "\n",
       "     POPESTIMATE2020  POPESTIMATE2021  POPESTIMATE2022  \n",
       "0            4921532          5050380          5073903  \n",
       "68            731158           734923           733276  \n",
       "95           7421401          7272487          7365684  \n",
       "111          3030522          3028443          3046404  \n",
       "187         39368078         39145060         39040616  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first of all we need to unify the years: we have data from 2000 until 2023 for the population and for the GDP we only have from 2001 to 2022\n",
    "# thus we need to shorten the population dataset\n",
    "Population_state.drop(columns = ['POPESTIMATE2000', 'POPESTIMATE2023'], inplace = True)\n",
    "Population_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e4114aea-889c-4927-93ea-9b5915441749",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDP_capita_state = pd.merge(Population_state, GDP_state, on=\"State\", how=\"inner\")\n",
    "\n",
    "for year in range(2001, 2023): \n",
    "\n",
    "    GDP_capita_state[str(year)] = pd.to_numeric(GDP_capita_state[str(year)], errors='coerce')\n",
    "    GDP_capita_state[f\"POPESTIMATE{year}\"] = pd.to_numeric(GDP_capita_state[f\"POPESTIMATE{year}\"], errors='coerce')\n",
    "    \n",
    "    # Calculate GDP per capita\n",
    "    GDP_capita_state[f\"GDP_capita{year}\"] = (\n",
    "        (GDP_capita_state[str(year)] * 1000) / GDP_capita_state[f\"POPESTIMATE{year}\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5a4f36ed-23b7-4758-9b4f-609472456946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>State</th>\n",
       "      <th>POPESTIMATE2001</th>\n",
       "      <th>POPESTIMATE2002</th>\n",
       "      <th>POPESTIMATE2003</th>\n",
       "      <th>POPESTIMATE2004</th>\n",
       "      <th>POPESTIMATE2005</th>\n",
       "      <th>POPESTIMATE2006</th>\n",
       "      <th>POPESTIMATE2007</th>\n",
       "      <th>...</th>\n",
       "      <th>GDP_capita2013</th>\n",
       "      <th>GDP_capita2014</th>\n",
       "      <th>GDP_capita2015</th>\n",
       "      <th>GDP_capita2016</th>\n",
       "      <th>GDP_capita2017</th>\n",
       "      <th>GDP_capita2018</th>\n",
       "      <th>GDP_capita2019</th>\n",
       "      <th>GDP_capita2020</th>\n",
       "      <th>GDP_capita2021</th>\n",
       "      <th>GDP_capita2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>4467634</td>\n",
       "      <td>4480089</td>\n",
       "      <td>4503491</td>\n",
       "      <td>4530729</td>\n",
       "      <td>4569805</td>\n",
       "      <td>4628981</td>\n",
       "      <td>4672840</td>\n",
       "      <td>...</td>\n",
       "      <td>40148.626973</td>\n",
       "      <td>40684.373037</td>\n",
       "      <td>41837.607005</td>\n",
       "      <td>42907.711477</td>\n",
       "      <td>44406.715554</td>\n",
       "      <td>46255.312955</td>\n",
       "      <td>47784.857471</td>\n",
       "      <td>47773.392513</td>\n",
       "      <td>51082.594973</td>\n",
       "      <td>55493.572699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>633714</td>\n",
       "      <td>642337</td>\n",
       "      <td>648414</td>\n",
       "      <td>659286</td>\n",
       "      <td>666946</td>\n",
       "      <td>675302</td>\n",
       "      <td>680300</td>\n",
       "      <td>...</td>\n",
       "      <td>77997.464840</td>\n",
       "      <td>76772.894210</td>\n",
       "      <td>69843.018296</td>\n",
       "      <td>68843.999596</td>\n",
       "      <td>72270.099314</td>\n",
       "      <td>74341.799887</td>\n",
       "      <td>74249.862664</td>\n",
       "      <td>70110.059659</td>\n",
       "      <td>79798.838790</td>\n",
       "      <td>89596.300711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>5273477</td>\n",
       "      <td>5396255</td>\n",
       "      <td>5510364</td>\n",
       "      <td>5652404</td>\n",
       "      <td>5839077</td>\n",
       "      <td>6029141</td>\n",
       "      <td>6167681</td>\n",
       "      <td>...</td>\n",
       "      <td>42003.394431</td>\n",
       "      <td>42578.549306</td>\n",
       "      <td>43750.983417</td>\n",
       "      <td>45183.242289</td>\n",
       "      <td>47260.902106</td>\n",
       "      <td>49366.237088</td>\n",
       "      <td>51502.067173</td>\n",
       "      <td>52071.509005</td>\n",
       "      <td>59440.438463</td>\n",
       "      <td>64576.989320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2691571</td>\n",
       "      <td>2705927</td>\n",
       "      <td>2724816</td>\n",
       "      <td>2749686</td>\n",
       "      <td>2781097</td>\n",
       "      <td>2821761</td>\n",
       "      <td>2848650</td>\n",
       "      <td>...</td>\n",
       "      <td>38514.660733</td>\n",
       "      <td>39541.342022</td>\n",
       "      <td>39793.361953</td>\n",
       "      <td>40439.481051</td>\n",
       "      <td>41241.200391</td>\n",
       "      <td>42897.372352</td>\n",
       "      <td>43905.266660</td>\n",
       "      <td>44838.650899</td>\n",
       "      <td>50168.325110</td>\n",
       "      <td>54486.952157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>California</td>\n",
       "      <td>34479458</td>\n",
       "      <td>34871843</td>\n",
       "      <td>35253159</td>\n",
       "      <td>35574576</td>\n",
       "      <td>35827943</td>\n",
       "      <td>36021202</td>\n",
       "      <td>36250311</td>\n",
       "      <td>...</td>\n",
       "      <td>58119.788435</td>\n",
       "      <td>60700.148984</td>\n",
       "      <td>63930.122730</td>\n",
       "      <td>66067.406255</td>\n",
       "      <td>69667.121725</td>\n",
       "      <td>73522.246195</td>\n",
       "      <td>77645.652082</td>\n",
       "      <td>77951.720046</td>\n",
       "      <td>87289.160906</td>\n",
       "      <td>93278.329036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE  COUNTY       State  POPESTIMATE2001  POPESTIMATE2002  \\\n",
       "0      1       0     Alabama          4467634          4480089   \n",
       "1      2       0      Alaska           633714           642337   \n",
       "2      4       0     Arizona          5273477          5396255   \n",
       "3      5       0    Arkansas          2691571          2705927   \n",
       "4      6       0  California         34479458         34871843   \n",
       "\n",
       "   POPESTIMATE2003  POPESTIMATE2004  POPESTIMATE2005  POPESTIMATE2006  \\\n",
       "0          4503491          4530729          4569805          4628981   \n",
       "1           648414           659286           666946           675302   \n",
       "2          5510364          5652404          5839077          6029141   \n",
       "3          2724816          2749686          2781097          2821761   \n",
       "4         35253159         35574576         35827943         36021202   \n",
       "\n",
       "   POPESTIMATE2007  ...  GDP_capita2013  GDP_capita2014  GDP_capita2015  \\\n",
       "0          4672840  ...    40148.626973    40684.373037    41837.607005   \n",
       "1           680300  ...    77997.464840    76772.894210    69843.018296   \n",
       "2          6167681  ...    42003.394431    42578.549306    43750.983417   \n",
       "3          2848650  ...    38514.660733    39541.342022    39793.361953   \n",
       "4         36250311  ...    58119.788435    60700.148984    63930.122730   \n",
       "\n",
       "   GDP_capita2016  GDP_capita2017  GDP_capita2018  GDP_capita2019  \\\n",
       "0    42907.711477    44406.715554    46255.312955    47784.857471   \n",
       "1    68843.999596    72270.099314    74341.799887    74249.862664   \n",
       "2    45183.242289    47260.902106    49366.237088    51502.067173   \n",
       "3    40439.481051    41241.200391    42897.372352    43905.266660   \n",
       "4    66067.406255    69667.121725    73522.246195    77645.652082   \n",
       "\n",
       "   GDP_capita2020  GDP_capita2021  GDP_capita2022  \n",
       "0    47773.392513    51082.594973    55493.572699  \n",
       "1    70110.059659    79798.838790    89596.300711  \n",
       "2    52071.509005    59440.438463    64576.989320  \n",
       "3    44838.650899    50168.325110    54486.952157  \n",
       "4    77951.720046    87289.160906    93278.329036  \n",
       "\n",
       "[5 rows x 70 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GDP_capita_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9c0448d2-77c4-453e-ab60-b19f4c421e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now drop the columns we do not need\n",
    "columns_to_keep = [\n",
    "    'STATE', 'COUNTY', 'State','GDP_capita2001', 'GDP_capita2002', 'GDP_capita2003', 'GDP_capita2004',\n",
    "    'GDP_capita2005', 'GDP_capita2006', 'GDP_capita2007', 'GDP_capita2008',\n",
    "    'GDP_capita2009', 'GDP_capita2010', 'GDP_capita2011', 'GDP_capita2012',\n",
    "    'GDP_capita2013', 'GDP_capita2014', 'GDP_capita2015', 'GDP_capita2016',\n",
    "    'GDP_capita2017', 'GDP_capita2018', 'GDP_capita2019', 'GDP_capita2020',\n",
    "    'GDP_capita2021', 'GDP_capita2022'\n",
    "]\n",
    "\n",
    "GDP_capita_state = GDP_capita_state[columns_to_keep]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc60bb6-4feb-4e33-aaa5-461ba32b93a7",
   "metadata": {},
   "source": [
    "Now we manipulate the data for the counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "105b5393-b6c1-4072-aaef-801e082d65dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicar\\AppData\\Local\\Temp\\ipykernel_5224\\3113787343.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Population_county.drop(columns = ['POPESTIMATE2000', 'POPESTIMATE2023'], inplace = True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>State</th>\n",
       "      <th>POPESTIMATE2001</th>\n",
       "      <th>POPESTIMATE2002</th>\n",
       "      <th>POPESTIMATE2003</th>\n",
       "      <th>POPESTIMATE2004</th>\n",
       "      <th>POPESTIMATE2005</th>\n",
       "      <th>POPESTIMATE2006</th>\n",
       "      <th>POPESTIMATE2007</th>\n",
       "      <th>...</th>\n",
       "      <th>POPESTIMATE2013</th>\n",
       "      <th>POPESTIMATE2014</th>\n",
       "      <th>POPESTIMATE2015</th>\n",
       "      <th>POPESTIMATE2016</th>\n",
       "      <th>POPESTIMATE2017</th>\n",
       "      <th>POPESTIMATE2018</th>\n",
       "      <th>POPESTIMATE2019</th>\n",
       "      <th>POPESTIMATE2020</th>\n",
       "      <th>POPESTIMATE2021</th>\n",
       "      <th>POPESTIMATE2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>44889</td>\n",
       "      <td>45909</td>\n",
       "      <td>46800</td>\n",
       "      <td>48366</td>\n",
       "      <td>49676</td>\n",
       "      <td>51328</td>\n",
       "      <td>52405</td>\n",
       "      <td>...</td>\n",
       "      <td>54747</td>\n",
       "      <td>54922</td>\n",
       "      <td>54903</td>\n",
       "      <td>55302</td>\n",
       "      <td>55448</td>\n",
       "      <td>55533</td>\n",
       "      <td>55769</td>\n",
       "      <td>56145</td>\n",
       "      <td>59203</td>\n",
       "      <td>59726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>144875</td>\n",
       "      <td>147957</td>\n",
       "      <td>151509</td>\n",
       "      <td>156266</td>\n",
       "      <td>162183</td>\n",
       "      <td>168121</td>\n",
       "      <td>172404</td>\n",
       "      <td>...</td>\n",
       "      <td>194978</td>\n",
       "      <td>199306</td>\n",
       "      <td>203101</td>\n",
       "      <td>207787</td>\n",
       "      <td>212737</td>\n",
       "      <td>218071</td>\n",
       "      <td>223565</td>\n",
       "      <td>229287</td>\n",
       "      <td>239439</td>\n",
       "      <td>246531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>28863</td>\n",
       "      <td>28653</td>\n",
       "      <td>28594</td>\n",
       "      <td>28287</td>\n",
       "      <td>28027</td>\n",
       "      <td>27861</td>\n",
       "      <td>27757</td>\n",
       "      <td>...</td>\n",
       "      <td>26946</td>\n",
       "      <td>26768</td>\n",
       "      <td>26300</td>\n",
       "      <td>25828</td>\n",
       "      <td>25169</td>\n",
       "      <td>24887</td>\n",
       "      <td>24657</td>\n",
       "      <td>24589</td>\n",
       "      <td>24533</td>\n",
       "      <td>24700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>21028</td>\n",
       "      <td>21199</td>\n",
       "      <td>21399</td>\n",
       "      <td>21721</td>\n",
       "      <td>22042</td>\n",
       "      <td>22099</td>\n",
       "      <td>22438</td>\n",
       "      <td>...</td>\n",
       "      <td>22510</td>\n",
       "      <td>22541</td>\n",
       "      <td>22553</td>\n",
       "      <td>22590</td>\n",
       "      <td>22532</td>\n",
       "      <td>22300</td>\n",
       "      <td>22313</td>\n",
       "      <td>22136</td>\n",
       "      <td>22359</td>\n",
       "      <td>21986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>51845</td>\n",
       "      <td>52551</td>\n",
       "      <td>53457</td>\n",
       "      <td>54124</td>\n",
       "      <td>54624</td>\n",
       "      <td>55485</td>\n",
       "      <td>56240</td>\n",
       "      <td>...</td>\n",
       "      <td>57630</td>\n",
       "      <td>57536</td>\n",
       "      <td>57535</td>\n",
       "      <td>57487</td>\n",
       "      <td>57801</td>\n",
       "      <td>57770</td>\n",
       "      <td>57840</td>\n",
       "      <td>57879</td>\n",
       "      <td>59079</td>\n",
       "      <td>59516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE  COUNTY           State  POPESTIMATE2001  POPESTIMATE2002  \\\n",
       "1      1       1  Autauga County            44889            45909   \n",
       "2      1       3  Baldwin County           144875           147957   \n",
       "3      1       5  Barbour County            28863            28653   \n",
       "4      1       7     Bibb County            21028            21199   \n",
       "5      1       9   Blount County            51845            52551   \n",
       "\n",
       "   POPESTIMATE2003  POPESTIMATE2004  POPESTIMATE2005  POPESTIMATE2006  \\\n",
       "1            46800            48366            49676            51328   \n",
       "2           151509           156266           162183           168121   \n",
       "3            28594            28287            28027            27861   \n",
       "4            21399            21721            22042            22099   \n",
       "5            53457            54124            54624            55485   \n",
       "\n",
       "   POPESTIMATE2007  ...  POPESTIMATE2013  POPESTIMATE2014  POPESTIMATE2015  \\\n",
       "1            52405  ...            54747            54922            54903   \n",
       "2           172404  ...           194978           199306           203101   \n",
       "3            27757  ...            26946            26768            26300   \n",
       "4            22438  ...            22510            22541            22553   \n",
       "5            56240  ...            57630            57536            57535   \n",
       "\n",
       "   POPESTIMATE2016  POPESTIMATE2017  POPESTIMATE2018  POPESTIMATE2019  \\\n",
       "1            55302            55448            55533            55769   \n",
       "2           207787           212737           218071           223565   \n",
       "3            25828            25169            24887            24657   \n",
       "4            22590            22532            22300            22313   \n",
       "5            57487            57801            57770            57840   \n",
       "\n",
       "   POPESTIMATE2020  POPESTIMATE2021  POPESTIMATE2022  \n",
       "1            56145            59203            59726  \n",
       "2           229287           239439           246531  \n",
       "3            24589            24533            24700  \n",
       "4            22136            22359            21986  \n",
       "5            57879            59079            59516  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Population_county.drop(columns = ['POPESTIMATE2000', 'POPESTIMATE2023'], inplace = True)\n",
    "Population_county.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "37f2ce7a-0aca-45db-9c56-3180aad30d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   STATE  COUNTY  GEOID\n",
      "1      1       1  01001\n",
      "2      1       3  01003\n",
      "3      1       5  01005\n",
      "4      1       7  01007\n",
      "5      1       9  01009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicar\\AppData\\Local\\Temp\\ipykernel_5224\\443939415.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Population_county['GEOID'] = (\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>State</th>\n",
       "      <th>POPESTIMATE2001</th>\n",
       "      <th>POPESTIMATE2002</th>\n",
       "      <th>POPESTIMATE2003</th>\n",
       "      <th>POPESTIMATE2004</th>\n",
       "      <th>POPESTIMATE2005</th>\n",
       "      <th>POPESTIMATE2006</th>\n",
       "      <th>POPESTIMATE2007</th>\n",
       "      <th>...</th>\n",
       "      <th>POPESTIMATE2014</th>\n",
       "      <th>POPESTIMATE2015</th>\n",
       "      <th>POPESTIMATE2016</th>\n",
       "      <th>POPESTIMATE2017</th>\n",
       "      <th>POPESTIMATE2018</th>\n",
       "      <th>POPESTIMATE2019</th>\n",
       "      <th>POPESTIMATE2020</th>\n",
       "      <th>POPESTIMATE2021</th>\n",
       "      <th>POPESTIMATE2022</th>\n",
       "      <th>GEOID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3175</th>\n",
       "      <td>56</td>\n",
       "      <td>37</td>\n",
       "      <td>Sweetwater County</td>\n",
       "      <td>36899</td>\n",
       "      <td>37428</td>\n",
       "      <td>37450</td>\n",
       "      <td>38026</td>\n",
       "      <td>38739</td>\n",
       "      <td>39749</td>\n",
       "      <td>41470</td>\n",
       "      <td>...</td>\n",
       "      <td>44996</td>\n",
       "      <td>44780</td>\n",
       "      <td>44319</td>\n",
       "      <td>43663</td>\n",
       "      <td>43188</td>\n",
       "      <td>42917</td>\n",
       "      <td>42673</td>\n",
       "      <td>41626</td>\n",
       "      <td>41374</td>\n",
       "      <td>56037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>56</td>\n",
       "      <td>39</td>\n",
       "      <td>Teton County</td>\n",
       "      <td>18653</td>\n",
       "      <td>18837</td>\n",
       "      <td>19066</td>\n",
       "      <td>19467</td>\n",
       "      <td>19632</td>\n",
       "      <td>20014</td>\n",
       "      <td>20472</td>\n",
       "      <td>...</td>\n",
       "      <td>22801</td>\n",
       "      <td>23083</td>\n",
       "      <td>23255</td>\n",
       "      <td>23383</td>\n",
       "      <td>23261</td>\n",
       "      <td>23385</td>\n",
       "      <td>23497</td>\n",
       "      <td>23605</td>\n",
       "      <td>23297</td>\n",
       "      <td>56039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3177</th>\n",
       "      <td>56</td>\n",
       "      <td>41</td>\n",
       "      <td>Uinta County</td>\n",
       "      <td>19413</td>\n",
       "      <td>19587</td>\n",
       "      <td>19480</td>\n",
       "      <td>19470</td>\n",
       "      <td>19494</td>\n",
       "      <td>19709</td>\n",
       "      <td>20171</td>\n",
       "      <td>...</td>\n",
       "      <td>20835</td>\n",
       "      <td>20777</td>\n",
       "      <td>20711</td>\n",
       "      <td>20449</td>\n",
       "      <td>20299</td>\n",
       "      <td>20196</td>\n",
       "      <td>20215</td>\n",
       "      <td>20681</td>\n",
       "      <td>20727</td>\n",
       "      <td>56041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3178</th>\n",
       "      <td>56</td>\n",
       "      <td>43</td>\n",
       "      <td>Washakie County</td>\n",
       "      <td>8068</td>\n",
       "      <td>7988</td>\n",
       "      <td>7976</td>\n",
       "      <td>7960</td>\n",
       "      <td>8022</td>\n",
       "      <td>7979</td>\n",
       "      <td>8169</td>\n",
       "      <td>...</td>\n",
       "      <td>8277</td>\n",
       "      <td>8282</td>\n",
       "      <td>8180</td>\n",
       "      <td>8013</td>\n",
       "      <td>7886</td>\n",
       "      <td>7824</td>\n",
       "      <td>7760</td>\n",
       "      <td>7719</td>\n",
       "      <td>7724</td>\n",
       "      <td>56043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3179</th>\n",
       "      <td>56</td>\n",
       "      <td>45</td>\n",
       "      <td>Weston County</td>\n",
       "      <td>6487</td>\n",
       "      <td>6578</td>\n",
       "      <td>6610</td>\n",
       "      <td>6646</td>\n",
       "      <td>6594</td>\n",
       "      <td>6717</td>\n",
       "      <td>7033</td>\n",
       "      <td>...</td>\n",
       "      <td>7134</td>\n",
       "      <td>7202</td>\n",
       "      <td>7228</td>\n",
       "      <td>6962</td>\n",
       "      <td>6895</td>\n",
       "      <td>6880</td>\n",
       "      <td>6743</td>\n",
       "      <td>6746</td>\n",
       "      <td>6858</td>\n",
       "      <td>56045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      STATE  COUNTY              State  POPESTIMATE2001  POPESTIMATE2002  \\\n",
       "3175     56      37  Sweetwater County            36899            37428   \n",
       "3176     56      39       Teton County            18653            18837   \n",
       "3177     56      41       Uinta County            19413            19587   \n",
       "3178     56      43    Washakie County             8068             7988   \n",
       "3179     56      45      Weston County             6487             6578   \n",
       "\n",
       "      POPESTIMATE2003  POPESTIMATE2004  POPESTIMATE2005  POPESTIMATE2006  \\\n",
       "3175            37450            38026            38739            39749   \n",
       "3176            19066            19467            19632            20014   \n",
       "3177            19480            19470            19494            19709   \n",
       "3178             7976             7960             8022             7979   \n",
       "3179             6610             6646             6594             6717   \n",
       "\n",
       "      POPESTIMATE2007  ...  POPESTIMATE2014  POPESTIMATE2015  POPESTIMATE2016  \\\n",
       "3175            41470  ...            44996            44780            44319   \n",
       "3176            20472  ...            22801            23083            23255   \n",
       "3177            20171  ...            20835            20777            20711   \n",
       "3178             8169  ...             8277             8282             8180   \n",
       "3179             7033  ...             7134             7202             7228   \n",
       "\n",
       "      POPESTIMATE2017  POPESTIMATE2018  POPESTIMATE2019  POPESTIMATE2020  \\\n",
       "3175            43663            43188            42917            42673   \n",
       "3176            23383            23261            23385            23497   \n",
       "3177            20449            20299            20196            20215   \n",
       "3178             8013             7886             7824             7760   \n",
       "3179             6962             6895             6880             6743   \n",
       "\n",
       "      POPESTIMATE2021  POPESTIMATE2022  GEOID  \n",
       "3175            41626            41374  56037  \n",
       "3176            23605            23297  56039  \n",
       "3177            20681            20727  56041  \n",
       "3178             7719             7724  56043  \n",
       "3179             6746             6858  56045  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to recognize the counties we need to have the GEOID. This can be easily retrived from the data we already have. It is composed from to numbers that represents the state and three numbers that represent the county. \n",
    "# This needs to be done because some counties have the same name\n",
    "\n",
    "Population_county['GEOID'] = (\n",
    "    Population_county['STATE'].astype(str).str.zfill(2) +  \n",
    "    Population_county['COUNTY'].astype(str).str.zfill(3)  \n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(Population_county[['STATE', 'COUNTY', 'GEOID']].head())\n",
    "Population_county.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "136751e9-ef98-4c25-98ae-4dc82609e56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicar\\AppData\\Local\\Temp\\ipykernel_5224\\1145949407.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Population_county['GEOID'] = Population_county['GEOID'].astype('Int64')\n"
     ]
    }
   ],
   "source": [
    "print(Population_county['GEOID'].dtype) # we want to transorm it to a int64\n",
    "Population_county['GEOID'] = Population_county['GEOID'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e8da1efc-b3fc-44e1-85bd-0d84e31e56e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicar\\AppData\\Local\\Temp\\ipykernel_5224\\688756234.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  GDP_county.rename(columns={'GeoFIPS': 'GEOID'}, inplace=True)\n",
      "C:\\Users\\nicar\\AppData\\Local\\Temp\\ipykernel_5224\\688756234.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  GDP_county['GEOID'] = GDP_county['GEOID'].astype('Int64')\n"
     ]
    }
   ],
   "source": [
    "GDP_county.rename(columns={'GeoFIPS': 'GEOID'}, inplace=True)\n",
    "GDP_county['GEOID'] = GDP_county['GEOID'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "aec7e6be-7cf7-487d-8902-c3bb2b798dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can do the same process as before and merge based on GeoFIPS\n",
    "GDP_capita_county = pd.merge(Population_county, GDP_county, on=\"GEOID\", how=\"inner\")\n",
    "\n",
    "for year in range(2001, 2023): \n",
    "\n",
    "    GDP_capita_county[str(year)] = pd.to_numeric(GDP_capita_county[str(year)], errors='coerce')\n",
    "    GDP_capita_county[f\"POPESTIMATE{year}\"] = pd.to_numeric(GDP_capita_county[f\"POPESTIMATE{year}\"], errors='coerce')\n",
    "    \n",
    "    # Calculate GDP per capita\n",
    "    GDP_capita_county[f\"GDP_capita{year}\"] = (\n",
    "        (GDP_capita_county[str(year)] * 1000) / GDP_capita_county[f\"POPESTIMATE{year}\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c7483688-594a-4c7a-bbdd-620cc8cf2e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3076 entries, 0 to 3075\n",
      "Data columns (total 71 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   STATE            3076 non-null   int64  \n",
      " 1   COUNTY           3076 non-null   int64  \n",
      " 2   State            3076 non-null   object \n",
      " 3   POPESTIMATE2001  3076 non-null   int64  \n",
      " 4   POPESTIMATE2002  3076 non-null   int64  \n",
      " 5   POPESTIMATE2003  3076 non-null   int64  \n",
      " 6   POPESTIMATE2004  3076 non-null   int64  \n",
      " 7   POPESTIMATE2005  3076 non-null   int64  \n",
      " 8   POPESTIMATE2006  3076 non-null   int64  \n",
      " 9   POPESTIMATE2007  3076 non-null   int64  \n",
      " 10  POPESTIMATE2008  3076 non-null   int64  \n",
      " 11  POPESTIMATE2009  3076 non-null   int64  \n",
      " 12  POPESTIMATE2010  3076 non-null   int64  \n",
      " 13  POPESTIMATE2011  3076 non-null   int64  \n",
      " 14  POPESTIMATE2012  3076 non-null   int64  \n",
      " 15  POPESTIMATE2013  3076 non-null   int64  \n",
      " 16  POPESTIMATE2014  3076 non-null   int64  \n",
      " 17  POPESTIMATE2015  3076 non-null   int64  \n",
      " 18  POPESTIMATE2016  3076 non-null   int64  \n",
      " 19  POPESTIMATE2017  3076 non-null   int64  \n",
      " 20  POPESTIMATE2018  3076 non-null   int64  \n",
      " 21  POPESTIMATE2019  3076 non-null   int64  \n",
      " 22  POPESTIMATE2020  3076 non-null   int64  \n",
      " 23  POPESTIMATE2021  3076 non-null   int64  \n",
      " 24  POPESTIMATE2022  3076 non-null   int64  \n",
      " 25  GEOID            3076 non-null   Int64  \n",
      " 26  2001             3071 non-null   float64\n",
      " 27  2002             3072 non-null   float64\n",
      " 28  2003             3072 non-null   float64\n",
      " 29  2004             3072 non-null   float64\n",
      " 30  2005             3072 non-null   float64\n",
      " 31  2006             3072 non-null   float64\n",
      " 32  2007             3072 non-null   float64\n",
      " 33  2008             3074 non-null   float64\n",
      " 34  2009             3076 non-null   int64  \n",
      " 35  2010             3076 non-null   int64  \n",
      " 36  2011             3076 non-null   int64  \n",
      " 37  2012             3076 non-null   int64  \n",
      " 38  2013             3076 non-null   int64  \n",
      " 39  2014             3076 non-null   int64  \n",
      " 40  2015             3076 non-null   int64  \n",
      " 41  2016             3076 non-null   int64  \n",
      " 42  2017             3076 non-null   int64  \n",
      " 43  2018             3076 non-null   int64  \n",
      " 44  2019             3076 non-null   int64  \n",
      " 45  2020             3076 non-null   int64  \n",
      " 46  2021             3076 non-null   int64  \n",
      " 47  2022             3076 non-null   int64  \n",
      " 48  County           3076 non-null   object \n",
      " 49  GDP_capita2001   3071 non-null   float64\n",
      " 50  GDP_capita2002   3072 non-null   float64\n",
      " 51  GDP_capita2003   3072 non-null   float64\n",
      " 52  GDP_capita2004   3072 non-null   float64\n",
      " 53  GDP_capita2005   3072 non-null   float64\n",
      " 54  GDP_capita2006   3072 non-null   float64\n",
      " 55  GDP_capita2007   3072 non-null   float64\n",
      " 56  GDP_capita2008   3074 non-null   float64\n",
      " 57  GDP_capita2009   3076 non-null   float64\n",
      " 58  GDP_capita2010   3076 non-null   float64\n",
      " 59  GDP_capita2011   3076 non-null   float64\n",
      " 60  GDP_capita2012   3076 non-null   float64\n",
      " 61  GDP_capita2013   3076 non-null   float64\n",
      " 62  GDP_capita2014   3076 non-null   float64\n",
      " 63  GDP_capita2015   3076 non-null   float64\n",
      " 64  GDP_capita2016   3076 non-null   float64\n",
      " 65  GDP_capita2017   3076 non-null   float64\n",
      " 66  GDP_capita2018   3076 non-null   float64\n",
      " 67  GDP_capita2019   3076 non-null   float64\n",
      " 68  GDP_capita2020   3076 non-null   float64\n",
      " 69  GDP_capita2021   3076 non-null   float64\n",
      " 70  GDP_capita2022   3076 non-null   float64\n",
      "dtypes: Int64(1), float64(30), int64(38), object(2)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "GDP_capita_county.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7bfd8839-222c-4ecc-849c-7105c3fe66d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we keep only the columns we need \n",
    "GDP_capita_county = GDP_capita_county[[\"STATE\", \"COUNTY\", \"State\", \"GEOID\", \"GDP_capita2002\",\"GDP_capita2003\",\"GDP_capita2004\",\"GDP_capita2005\",\"GDP_capita2006\",\"GDP_capita2007\",\"GDP_capita2008\",\"GDP_capita2009\",\"GDP_capita2010\",\"GDP_capita2011\",\"GDP_capita2012\",\"GDP_capita2013\",\"GDP_capita2014\",\"GDP_capita2015\",\"GDP_capita2016\",\"GDP_capita2017\",\"GDP_capita2018\",\"GDP_capita2019\",\"GDP_capita2020\",\"GDP_capita2021\",\"GDP_capita2022\"]]\n",
    "# we change also the name of the column called State to County\n",
    "GDP_capita_county = GDP_capita_county.rename(columns={'State': 'County'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9a915deb-f9ed-4a97-8c1a-0b3cc2ab54b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>County</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>GDP_capita2002</th>\n",
       "      <th>GDP_capita2003</th>\n",
       "      <th>GDP_capita2004</th>\n",
       "      <th>GDP_capita2005</th>\n",
       "      <th>GDP_capita2006</th>\n",
       "      <th>GDP_capita2007</th>\n",
       "      <th>...</th>\n",
       "      <th>GDP_capita2013</th>\n",
       "      <th>GDP_capita2014</th>\n",
       "      <th>GDP_capita2015</th>\n",
       "      <th>GDP_capita2016</th>\n",
       "      <th>GDP_capita2017</th>\n",
       "      <th>GDP_capita2018</th>\n",
       "      <th>GDP_capita2019</th>\n",
       "      <th>GDP_capita2020</th>\n",
       "      <th>GDP_capita2021</th>\n",
       "      <th>GDP_capita2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>1001</td>\n",
       "      <td>17426.757281</td>\n",
       "      <td>17824.957265</td>\n",
       "      <td>20354.401853</td>\n",
       "      <td>20438.843707</td>\n",
       "      <td>21900.112999</td>\n",
       "      <td>22559.564927</td>\n",
       "      <td>...</td>\n",
       "      <td>27590.059729</td>\n",
       "      <td>28569.971960</td>\n",
       "      <td>31493.688869</td>\n",
       "      <td>32661.495063</td>\n",
       "      <td>31787.584764</td>\n",
       "      <td>32892.910522</td>\n",
       "      <td>32347.953164</td>\n",
       "      <td>32301.237866</td>\n",
       "      <td>32897.353175</td>\n",
       "      <td>39595.670227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>1003</td>\n",
       "      <td>22772.548781</td>\n",
       "      <td>24038.195751</td>\n",
       "      <td>26346.927675</td>\n",
       "      <td>29088.381643</td>\n",
       "      <td>29928.069664</td>\n",
       "      <td>30916.347649</td>\n",
       "      <td>...</td>\n",
       "      <td>29628.173435</td>\n",
       "      <td>30278.702096</td>\n",
       "      <td>31967.218281</td>\n",
       "      <td>33606.707831</td>\n",
       "      <td>34702.745644</td>\n",
       "      <td>36389.868437</td>\n",
       "      <td>38305.159573</td>\n",
       "      <td>38214.578236</td>\n",
       "      <td>41259.034660</td>\n",
       "      <td>43852.979950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>1005</td>\n",
       "      <td>22258.227760</td>\n",
       "      <td>23255.857872</td>\n",
       "      <td>26216.459858</td>\n",
       "      <td>26717.807828</td>\n",
       "      <td>26704.138401</td>\n",
       "      <td>26652.700220</td>\n",
       "      <td>...</td>\n",
       "      <td>30142.581459</td>\n",
       "      <td>29113.082785</td>\n",
       "      <td>29087.452471</td>\n",
       "      <td>29327.590212</td>\n",
       "      <td>30265.842902</td>\n",
       "      <td>31707.437618</td>\n",
       "      <td>32198.077625</td>\n",
       "      <td>31987.026719</td>\n",
       "      <td>35157.502140</td>\n",
       "      <td>37654.412955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>1007</td>\n",
       "      <td>10604.887023</td>\n",
       "      <td>11333.006215</td>\n",
       "      <td>12203.765941</td>\n",
       "      <td>12471.554305</td>\n",
       "      <td>12804.923300</td>\n",
       "      <td>13856.226045</td>\n",
       "      <td>...</td>\n",
       "      <td>16588.271879</td>\n",
       "      <td>16918.237878</td>\n",
       "      <td>16739.901565</td>\n",
       "      <td>17365.515715</td>\n",
       "      <td>18051.704243</td>\n",
       "      <td>18331.434978</td>\n",
       "      <td>20897.817416</td>\n",
       "      <td>22647.271413</td>\n",
       "      <td>23627.085290</td>\n",
       "      <td>25418.357136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>1009</td>\n",
       "      <td>12134.954616</td>\n",
       "      <td>12682.997549</td>\n",
       "      <td>13325.844357</td>\n",
       "      <td>13667.234183</td>\n",
       "      <td>13363.449581</td>\n",
       "      <td>13510.437411</td>\n",
       "      <td>...</td>\n",
       "      <td>15340.430331</td>\n",
       "      <td>16138.626251</td>\n",
       "      <td>17163.639524</td>\n",
       "      <td>16107.780890</td>\n",
       "      <td>17092.558952</td>\n",
       "      <td>18421.637528</td>\n",
       "      <td>18295.608575</td>\n",
       "      <td>16818.086007</td>\n",
       "      <td>19520.387955</td>\n",
       "      <td>21540.526917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE  COUNTY          County  GEOID  GDP_capita2002  GDP_capita2003  \\\n",
       "0      1       1  Autauga County   1001    17426.757281    17824.957265   \n",
       "1      1       3  Baldwin County   1003    22772.548781    24038.195751   \n",
       "2      1       5  Barbour County   1005    22258.227760    23255.857872   \n",
       "3      1       7     Bibb County   1007    10604.887023    11333.006215   \n",
       "4      1       9   Blount County   1009    12134.954616    12682.997549   \n",
       "\n",
       "   GDP_capita2004  GDP_capita2005  GDP_capita2006  GDP_capita2007  ...  \\\n",
       "0    20354.401853    20438.843707    21900.112999    22559.564927  ...   \n",
       "1    26346.927675    29088.381643    29928.069664    30916.347649  ...   \n",
       "2    26216.459858    26717.807828    26704.138401    26652.700220  ...   \n",
       "3    12203.765941    12471.554305    12804.923300    13856.226045  ...   \n",
       "4    13325.844357    13667.234183    13363.449581    13510.437411  ...   \n",
       "\n",
       "   GDP_capita2013  GDP_capita2014  GDP_capita2015  GDP_capita2016  \\\n",
       "0    27590.059729    28569.971960    31493.688869    32661.495063   \n",
       "1    29628.173435    30278.702096    31967.218281    33606.707831   \n",
       "2    30142.581459    29113.082785    29087.452471    29327.590212   \n",
       "3    16588.271879    16918.237878    16739.901565    17365.515715   \n",
       "4    15340.430331    16138.626251    17163.639524    16107.780890   \n",
       "\n",
       "   GDP_capita2017  GDP_capita2018  GDP_capita2019  GDP_capita2020  \\\n",
       "0    31787.584764    32892.910522    32347.953164    32301.237866   \n",
       "1    34702.745644    36389.868437    38305.159573    38214.578236   \n",
       "2    30265.842902    31707.437618    32198.077625    31987.026719   \n",
       "3    18051.704243    18331.434978    20897.817416    22647.271413   \n",
       "4    17092.558952    18421.637528    18295.608575    16818.086007   \n",
       "\n",
       "   GDP_capita2021  GDP_capita2022  \n",
       "0    32897.353175    39595.670227  \n",
       "1    41259.034660    43852.979950  \n",
       "2    35157.502140    37654.412955  \n",
       "3    23627.085290    25418.357136  \n",
       "4    19520.387955    21540.526917  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GDP_capita_county.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f8d4f985-d2c3-4841-98f4-a147410a6531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we pivot the GDP county and state datasets\n",
    "gdp_columns = [col for col in GDP_capita_state.columns if col.startswith(\"GDP_capita\")]\n",
    "\n",
    "# Include 'CTYNAME' and 'GeoFIPS' in id_vars to retain them\n",
    "GDP_capita_state = GDP_capita_state.melt(\n",
    "    id_vars=[\"STATE\", \"State\"],  # Include additional columns to retain\n",
    "    value_vars=gdp_columns,\n",
    "    var_name=\"Year\",\n",
    "    value_name=\"GDP_per_capita\"\n",
    ")\n",
    "\n",
    "# Extract the year from the column names\n",
    "GDP_capita_state[\"Year\"] = GDP_capita_state[\"Year\"].str.extract(r\"(\\d{4})\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "37825122-2896-482b-8d96-5828ee5c9f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>State</th>\n",
       "      <th>Year</th>\n",
       "      <th>GDP_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2001</td>\n",
       "      <td>27650.993121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>2001</td>\n",
       "      <td>45226.812095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>2001</td>\n",
       "      <td>32786.650250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2001</td>\n",
       "      <td>26415.667653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>California</td>\n",
       "      <td>2001</td>\n",
       "      <td>39828.665955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>51</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>2022</td>\n",
       "      <td>76402.574622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>53</td>\n",
       "      <td>Washington</td>\n",
       "      <td>2022</td>\n",
       "      <td>94817.084950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>54</td>\n",
       "      <td>West Virginia</td>\n",
       "      <td>2022</td>\n",
       "      <td>54912.855722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>55</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2022</td>\n",
       "      <td>67261.927975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>56</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2022</td>\n",
       "      <td>84384.686802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      STATE          State  Year  GDP_per_capita\n",
       "0         1        Alabama  2001    27650.993121\n",
       "1         2         Alaska  2001    45226.812095\n",
       "2         4        Arizona  2001    32786.650250\n",
       "3         5       Arkansas  2001    26415.667653\n",
       "4         6     California  2001    39828.665955\n",
       "...     ...            ...   ...             ...\n",
       "1095     51       Virginia  2022    76402.574622\n",
       "1096     53     Washington  2022    94817.084950\n",
       "1097     54  West Virginia  2022    54912.855722\n",
       "1098     55      Wisconsin  2022    67261.927975\n",
       "1099     56        Wyoming  2022    84384.686802\n",
       "\n",
       "[1100 rows x 4 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GDP_capita_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6434242a-9bd7-4ae8-bdb9-50b52f112b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_columns = [col for col in GDP_capita_county.columns if col.startswith(\"GDP_capita\")]\n",
    "\n",
    "# Include 'CTYNAME' and 'GeoFIPS' in id_vars to retain them\n",
    "GDP_capita_county = GDP_capita_county.melt(\n",
    "    id_vars=[\"STATE\", \"County\", \"GEOID\"],  # Include additional columns to retain\n",
    "    value_vars=gdp_columns,\n",
    "    var_name=\"Year\",\n",
    "    value_name=\"GDP_per_capita\"\n",
    ")\n",
    "\n",
    "# Extract the year from the column names\n",
    "GDP_capita_county[\"Year\"] = GDP_capita_county[\"Year\"].str.extract(r\"(\\d{4})\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c629d5a0-d91b-489c-bb28-121329e77d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>County</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>Year</th>\n",
       "      <th>GDP_per_capita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>1001</td>\n",
       "      <td>2002</td>\n",
       "      <td>17426.757281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>1003</td>\n",
       "      <td>2002</td>\n",
       "      <td>22772.548781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>1005</td>\n",
       "      <td>2002</td>\n",
       "      <td>22258.227760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>1007</td>\n",
       "      <td>2002</td>\n",
       "      <td>10604.887023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>1009</td>\n",
       "      <td>2002</td>\n",
       "      <td>12134.954616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64591</th>\n",
       "      <td>56</td>\n",
       "      <td>Sweetwater County</td>\n",
       "      <td>56037</td>\n",
       "      <td>2022</td>\n",
       "      <td>99552.255039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64592</th>\n",
       "      <td>56</td>\n",
       "      <td>Teton County</td>\n",
       "      <td>56039</td>\n",
       "      <td>2022</td>\n",
       "      <td>178405.288235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64593</th>\n",
       "      <td>56</td>\n",
       "      <td>Uinta County</td>\n",
       "      <td>56041</td>\n",
       "      <td>2022</td>\n",
       "      <td>49882.665123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64594</th>\n",
       "      <td>56</td>\n",
       "      <td>Washakie County</td>\n",
       "      <td>56043</td>\n",
       "      <td>2022</td>\n",
       "      <td>54530.036251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64595</th>\n",
       "      <td>56</td>\n",
       "      <td>Weston County</td>\n",
       "      <td>56045</td>\n",
       "      <td>2022</td>\n",
       "      <td>60319.626713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64596 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       STATE             County  GEOID  Year  GDP_per_capita\n",
       "0          1     Autauga County   1001  2002    17426.757281\n",
       "1          1     Baldwin County   1003  2002    22772.548781\n",
       "2          1     Barbour County   1005  2002    22258.227760\n",
       "3          1        Bibb County   1007  2002    10604.887023\n",
       "4          1      Blount County   1009  2002    12134.954616\n",
       "...      ...                ...    ...   ...             ...\n",
       "64591     56  Sweetwater County  56037  2022    99552.255039\n",
       "64592     56       Teton County  56039  2022   178405.288235\n",
       "64593     56       Uinta County  56041  2022    49882.665123\n",
       "64594     56    Washakie County  56043  2022    54530.036251\n",
       "64595     56      Weston County  56045  2022    60319.626713\n",
       "\n",
       "[64596 rows x 5 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GDP_capita_county"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdbd5de-13aa-46e8-83b0-cb0d47cb4171",
   "metadata": {},
   "source": [
    "## State and county size loading and maipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "54327afe-d512-409d-be65-5930e179de06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1GCUYNOeeRwOB0UVIXwvhXa8-BygnQnx_\n",
      "To: C:\\Users\\nicar\\OneDrive\\Desktop\\Business Analytics and Data science apllication\\Capstone\\size_county.txt\n",
      "100%|██████████| 648k/648k [00:00<00:00, 5.78MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we download the dataset with the size of the counties in such a way that we can calculate the density of each county. This needs to be done because thre is no unique big available dataset on the densityy of the counties \n",
    "\n",
    "# Google Drive file link\n",
    "drive_link = \"https://drive.google.com/file/d/1GCUYNOeeRwOB0UVIXwvhXa8-BygnQnx_/view?usp=sharing\"\n",
    "\n",
    "# Extract the file ID from the link\n",
    "file_id = drive_link.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Create the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Output file name\n",
    "output = \"size_county.txt\"\n",
    "\n",
    "# Download the file\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "print(\"Download complete!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e579722b-3c79-45b2-9b33-c30ebb782f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  USPS  GEOID  ANSICODE            NAME       ALAND      AWATER  ALAND_SQMI  \\\n",
      "0   AL   1001    161526  Autauga County  1539631459    25677536     594.455   \n",
      "1   AL   1003    161527  Baldwin County  4117781416  1132830835    1589.884   \n",
      "2   AL   1005    161528  Barbour County  2292160151    50523213     885.008   \n",
      "3   AL   1007    161529     Bibb County  1612188713     9572302     622.470   \n",
      "4   AL   1009    161530   Blount County  1670259099    14860281     644.891   \n",
      "\n",
      "   AWATER_SQMI   INTPTLAT  \\\n",
      "0        9.914  32.532237   \n",
      "1      437.388  30.659218   \n",
      "2       19.507  31.870253   \n",
      "3        3.696  33.015893   \n",
      "4        5.738  33.977358   \n",
      "\n",
      "   INTPTLONG                                                                                                                 \n",
      "0                                         -86.646440                                                                         \n",
      "1                                         -87.746067                                                                         \n",
      "2                                         -85.405104                                                                         \n",
      "3                                         -87.127148                                                                         \n",
      "4                                         -86.566440                                                                         \n"
     ]
    }
   ],
   "source": [
    "# Read the .txt file into a DataFrame\n",
    "size_county = pd.read_csv(\"size_county.txt\", delimiter='\\t') \n",
    "\n",
    "print(size_county.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "48a1edcc-f2e0-40fc-96c6-3b9e56532a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want just to keep the ALAND (squared meters) to calculate the density\n",
    "size_county = size_county[[\"USPS\", \"GEOID\", \"NAME\", \"ALAND\"]]\n",
    "# To be able to merge we also need to rename the column name which indicates the column for the county name\n",
    "size_county.rename(columns={\"NAME\": \"County\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3abcd533-c1c4-44af-a624-0550de779e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USPS</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>County</th>\n",
       "      <th>ALAND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>1001</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>1539631459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>1003</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>4117781416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL</td>\n",
       "      <td>1005</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>2292160151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AL</td>\n",
       "      <td>1007</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>1612188713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL</td>\n",
       "      <td>1009</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>1670259099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  USPS  GEOID          County       ALAND\n",
       "0   AL   1001  Autauga County  1539631459\n",
       "1   AL   1003  Baldwin County  4117781416\n",
       "2   AL   1005  Barbour County  2292160151\n",
       "3   AL   1007     Bibb County  1612188713\n",
       "4   AL   1009   Blount County  1670259099"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_county.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb1ecb-b0fa-4728-ab92-513b8ef20ccd",
   "metadata": {},
   "source": [
    "As done before we want to have the data for the sate and for the county (note, we do not have the data at state level, but we can sum all the counties of the same state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f2965c58-a8e6-4201-ad5a-432cba33a283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Total_ALAND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AK</td>\n",
       "      <td>1479508971743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>131185561946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AR</td>\n",
       "      <td>134658517854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AZ</td>\n",
       "      <td>294366118294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>403673433805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CO</td>\n",
       "      <td>268418973518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CT</td>\n",
       "      <td>12541999507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DC</td>\n",
       "      <td>158316181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DE</td>\n",
       "      <td>5046692239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FL</td>\n",
       "      <td>138965379385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GA</td>\n",
       "      <td>149485762701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HI</td>\n",
       "      <td>16634423918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>IA</td>\n",
       "      <td>144660344616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ID</td>\n",
       "      <td>214050504522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>IL</td>\n",
       "      <td>143778206717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>IN</td>\n",
       "      <td>92786694938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>KS</td>\n",
       "      <td>211753821560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>KY</td>\n",
       "      <td>102266755818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LA</td>\n",
       "      <td>111920866908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MA</td>\n",
       "      <td>20204400635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MD</td>\n",
       "      <td>25151223822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ME</td>\n",
       "      <td>79888396620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>MI</td>\n",
       "      <td>146621709684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>MN</td>\n",
       "      <td>206244791203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>MO</td>\n",
       "      <td>178052403953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>MS</td>\n",
       "      <td>121534116434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>MT</td>\n",
       "      <td>376973404582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NC</td>\n",
       "      <td>125935965771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ND</td>\n",
       "      <td>178694319927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NE</td>\n",
       "      <td>198949953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>NH</td>\n",
       "      <td>23190211616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NJ</td>\n",
       "      <td>19049229866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NM</td>\n",
       "      <td>314198519809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NV</td>\n",
       "      <td>284537045743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NY</td>\n",
       "      <td>122049155860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>OH</td>\n",
       "      <td>105824130554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>OK</td>\n",
       "      <td>177664629521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>OR</td>\n",
       "      <td>248630421366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>PA</td>\n",
       "      <td>115881476238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>PR</td>\n",
       "      <td>8869519701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>RI</td>\n",
       "      <td>2677768885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>SC</td>\n",
       "      <td>77865663223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>SD</td>\n",
       "      <td>196341670967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>TN</td>\n",
       "      <td>106770544103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>TX</td>\n",
       "      <td>676656702022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>UT</td>\n",
       "      <td>213921882299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>VA</td>\n",
       "      <td>102256342555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>VT</td>\n",
       "      <td>23872594714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>WA</td>\n",
       "      <td>172118798160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>WI</td>\n",
       "      <td>140294555214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>WV</td>\n",
       "      <td>62266513826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>WY</td>\n",
       "      <td>251458190512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   State    Total_ALAND\n",
       "0     AK  1479508971743\n",
       "1     AL   131185561946\n",
       "2     AR   134658517854\n",
       "3     AZ   294366118294\n",
       "4     CA   403673433805\n",
       "5     CO   268418973518\n",
       "6     CT    12541999507\n",
       "7     DC      158316181\n",
       "8     DE     5046692239\n",
       "9     FL   138965379385\n",
       "10    GA   149485762701\n",
       "11    HI    16634423918\n",
       "12    IA   144660344616\n",
       "13    ID   214050504522\n",
       "14    IL   143778206717\n",
       "15    IN    92786694938\n",
       "16    KS   211753821560\n",
       "17    KY   102266755818\n",
       "18    LA   111920866908\n",
       "19    MA    20204400635\n",
       "20    MD    25151223822\n",
       "21    ME    79888396620\n",
       "22    MI   146621709684\n",
       "23    MN   206244791203\n",
       "24    MO   178052403953\n",
       "25    MS   121534116434\n",
       "26    MT   376973404582\n",
       "27    NC   125935965771\n",
       "28    ND   178694319927\n",
       "29    NE   198949953125\n",
       "30    NH    23190211616\n",
       "31    NJ    19049229866\n",
       "32    NM   314198519809\n",
       "33    NV   284537045743\n",
       "34    NY   122049155860\n",
       "35    OH   105824130554\n",
       "36    OK   177664629521\n",
       "37    OR   248630421366\n",
       "38    PA   115881476238\n",
       "39    PR     8869519701\n",
       "40    RI     2677768885\n",
       "41    SC    77865663223\n",
       "42    SD   196341670967\n",
       "43    TN   106770544103\n",
       "44    TX   676656702022\n",
       "45    UT   213921882299\n",
       "46    VA   102256342555\n",
       "47    VT    23872594714\n",
       "48    WA   172118798160\n",
       "49    WI   140294555214\n",
       "50    WV    62266513826\n",
       "51    WY   251458190512"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size state\n",
    "# Group by 'USPS' (state) and sum the 'ALAND' values\n",
    "size_state = size_county.groupby('USPS')['ALAND'].sum().reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "size_state.columns = ['State', 'Total_ALAND']\n",
    "size_state # we have 52 states because of puerto rico and dc, we need to rename the states to their \"long names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6649248c-090c-4ea1-9abf-222c692573c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Total_ALAND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>1479508971743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>131185561946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>134658517854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>294366118294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>403673433805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>268418973518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>12541999507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>158316181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>5046692239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Florida</td>\n",
       "      <td>138965379385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>149485762701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Hawaii</td>\n",
       "      <td>16634423918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>144660344616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Idaho</td>\n",
       "      <td>214050504522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Illinois</td>\n",
       "      <td>143778206717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Indiana</td>\n",
       "      <td>92786694938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>211753821560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>102266755818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Louisiana</td>\n",
       "      <td>111920866908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>20204400635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>25151223822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Maine</td>\n",
       "      <td>79888396620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>146621709684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Minnesota</td>\n",
       "      <td>206244791203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>178052403953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Mississippi</td>\n",
       "      <td>121534116434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Montana</td>\n",
       "      <td>376973404582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>125935965771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>178694319927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Nebraska</td>\n",
       "      <td>198949953125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>23190211616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>19049229866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>314198519809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>284537045743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>New York</td>\n",
       "      <td>122049155860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>105824130554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>177664629521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Oregon</td>\n",
       "      <td>248630421366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>115881476238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>8869519701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>2677768885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>South Carolina</td>\n",
       "      <td>77865663223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>196341670967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Tennessee</td>\n",
       "      <td>106770544103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Texas</td>\n",
       "      <td>676656702022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Utah</td>\n",
       "      <td>213921882299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>102256342555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>23872594714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Washington</td>\n",
       "      <td>172118798160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>140294555214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>62266513826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>251458190512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   State    Total_ALAND\n",
       "0                 Alaska  1479508971743\n",
       "1                Alabama   131185561946\n",
       "2               Arkansas   134658517854\n",
       "3                Arizona   294366118294\n",
       "4             California   403673433805\n",
       "5               Colorado   268418973518\n",
       "6            Connecticut    12541999507\n",
       "7   District of Columbia      158316181\n",
       "8               Delaware     5046692239\n",
       "9                Florida   138965379385\n",
       "10               Georgia   149485762701\n",
       "11                Hawaii    16634423918\n",
       "12                  Iowa   144660344616\n",
       "13                 Idaho   214050504522\n",
       "14              Illinois   143778206717\n",
       "15               Indiana    92786694938\n",
       "16                Kansas   211753821560\n",
       "17              Kentucky   102266755818\n",
       "18             Louisiana   111920866908\n",
       "19         Massachusetts    20204400635\n",
       "20              Maryland    25151223822\n",
       "21                 Maine    79888396620\n",
       "22              Michigan   146621709684\n",
       "23             Minnesota   206244791203\n",
       "24              Missouri   178052403953\n",
       "25           Mississippi   121534116434\n",
       "26               Montana   376973404582\n",
       "27        North Carolina   125935965771\n",
       "28          North Dakota   178694319927\n",
       "29              Nebraska   198949953125\n",
       "30         New Hampshire    23190211616\n",
       "31            New Jersey    19049229866\n",
       "32            New Mexico   314198519809\n",
       "33                Nevada   284537045743\n",
       "34              New York   122049155860\n",
       "35                  Ohio   105824130554\n",
       "36              Oklahoma   177664629521\n",
       "37                Oregon   248630421366\n",
       "38          Pennsylvania   115881476238\n",
       "39           Puerto Rico     8869519701\n",
       "40          Rhode Island     2677768885\n",
       "41        South Carolina    77865663223\n",
       "42          South Dakota   196341670967\n",
       "43             Tennessee   106770544103\n",
       "44                 Texas   676656702022\n",
       "45                  Utah   213921882299\n",
       "46              Virginia   102256342555\n",
       "47               Vermont    23872594714\n",
       "48            Washington   172118798160\n",
       "49             Wisconsin   140294555214\n",
       "50         West Virginia    62266513826\n",
       "51               Wyoming   251458190512"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_mapping = {\n",
    "    'AK': 'Alaska', 'AL': 'Alabama', 'AR': 'Arkansas', 'AZ': 'Arizona',\n",
    "    'CA': 'California', 'CO': 'Colorado', 'CT': 'Connecticut', 'DC': 'District of Columbia',\n",
    "    'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia', 'HI': 'Hawaii',\n",
    "    'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'KS': 'Kansas', 'KY': 'Kentucky',\n",
    "    'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland', 'MA': 'Massachusetts',\n",
    "    'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',\n",
    "    'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire',\n",
    "    'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina',\n",
    "    'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma', 'OR': 'Oregon',\n",
    "    'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina', 'SD': 'South Dakota',\n",
    "    'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont', 'VA': 'Virginia',\n",
    "    'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming',\n",
    "    'PR': 'Puerto Rico', \"IA\" : \"Iowa\"\n",
    "}\n",
    "\n",
    "# Map abbreviations to full names in the 'State' column\n",
    "size_state['State'] = size_state['State'].map(state_mapping)\n",
    "size_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100aabd-df92-4758-b4e1-b3dbba866748",
   "metadata": {},
   "source": [
    "Now we can calculate the density for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "48d43e8a-1015-4746-9408-3ddfbf600e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USPS</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>County</th>\n",
       "      <th>ALAND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>1001</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>1539631459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>1003</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>4117781416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL</td>\n",
       "      <td>1005</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>2292160151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AL</td>\n",
       "      <td>1007</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>1612188713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL</td>\n",
       "      <td>1009</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>1670259099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>PR</td>\n",
       "      <td>72145</td>\n",
       "      <td>Vega Baja Municipio</td>\n",
       "      <td>118746311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>PR</td>\n",
       "      <td>72147</td>\n",
       "      <td>Vieques Municipio</td>\n",
       "      <td>131541389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>PR</td>\n",
       "      <td>72149</td>\n",
       "      <td>Villalba Municipio</td>\n",
       "      <td>92200653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3220</th>\n",
       "      <td>PR</td>\n",
       "      <td>72151</td>\n",
       "      <td>Yabucoa Municipio</td>\n",
       "      <td>143005187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3221</th>\n",
       "      <td>PR</td>\n",
       "      <td>72153</td>\n",
       "      <td>Yauco Municipio</td>\n",
       "      <td>175436927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3222 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     USPS  GEOID               County       ALAND\n",
       "0      AL   1001       Autauga County  1539631459\n",
       "1      AL   1003       Baldwin County  4117781416\n",
       "2      AL   1005       Barbour County  2292160151\n",
       "3      AL   1007          Bibb County  1612188713\n",
       "4      AL   1009        Blount County  1670259099\n",
       "...   ...    ...                  ...         ...\n",
       "3217   PR  72145  Vega Baja Municipio   118746311\n",
       "3218   PR  72147    Vieques Municipio   131541389\n",
       "3219   PR  72149   Villalba Municipio    92200653\n",
       "3220   PR  72151    Yabucoa Municipio   143005187\n",
       "3221   PR  72153      Yauco Municipio   175436927\n",
       "\n",
       "[3222 rows x 4 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "34a0091b-fa1a-4d58-8c8d-1ebe8e974c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove Puerto rico from both \n",
    "population_state = Population_state.rename(columns={'CTYNAME': 'State'})  # Rename column to match\n",
    "population_state = population_state[population_state['State'] != \"Puerto Rico\"]\n",
    "size_state = size_state[size_state['State'] != \"Puerto Rico\"]\n",
    "# we merge the 2  datasets \n",
    "density_state = pd.merge(population_state, size_state, on = \"State\", how = \"inner\")\n",
    "# Loop through the years to calculate density\n",
    "for year in range(2001, 2023):  # From 2001 to 2023\n",
    "    density_state[f\"Density{year}\"] = density_state[f\"POPESTIMATE{year}\"] / (density_state[\"Total_ALAND\"] / 1e+6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e09b3e7e-b7a8-47c6-bb8d-c45e0874426e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>State</th>\n",
       "      <th>POPESTIMATE2001</th>\n",
       "      <th>POPESTIMATE2002</th>\n",
       "      <th>POPESTIMATE2003</th>\n",
       "      <th>POPESTIMATE2004</th>\n",
       "      <th>POPESTIMATE2005</th>\n",
       "      <th>POPESTIMATE2006</th>\n",
       "      <th>POPESTIMATE2007</th>\n",
       "      <th>POPESTIMATE2008</th>\n",
       "      <th>POPESTIMATE2009</th>\n",
       "      <th>POPESTIMATE2010</th>\n",
       "      <th>POPESTIMATE2011</th>\n",
       "      <th>POPESTIMATE2012</th>\n",
       "      <th>POPESTIMATE2013</th>\n",
       "      <th>POPESTIMATE2014</th>\n",
       "      <th>POPESTIMATE2015</th>\n",
       "      <th>POPESTIMATE2016</th>\n",
       "      <th>POPESTIMATE2017</th>\n",
       "      <th>POPESTIMATE2018</th>\n",
       "      <th>POPESTIMATE2019</th>\n",
       "      <th>POPESTIMATE2020</th>\n",
       "      <th>POPESTIMATE2021</th>\n",
       "      <th>POPESTIMATE2022</th>\n",
       "      <th>Total_ALAND</th>\n",
       "      <th>Density2001</th>\n",
       "      <th>Density2002</th>\n",
       "      <th>Density2003</th>\n",
       "      <th>Density2004</th>\n",
       "      <th>Density2005</th>\n",
       "      <th>Density2006</th>\n",
       "      <th>Density2007</th>\n",
       "      <th>Density2008</th>\n",
       "      <th>Density2009</th>\n",
       "      <th>Density2010</th>\n",
       "      <th>Density2011</th>\n",
       "      <th>Density2012</th>\n",
       "      <th>Density2013</th>\n",
       "      <th>Density2014</th>\n",
       "      <th>Density2015</th>\n",
       "      <th>Density2016</th>\n",
       "      <th>Density2017</th>\n",
       "      <th>Density2018</th>\n",
       "      <th>Density2019</th>\n",
       "      <th>Density2020</th>\n",
       "      <th>Density2021</th>\n",
       "      <th>Density2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>4467634</td>\n",
       "      <td>4480089</td>\n",
       "      <td>4503491</td>\n",
       "      <td>4530729</td>\n",
       "      <td>4569805</td>\n",
       "      <td>4628981</td>\n",
       "      <td>4672840</td>\n",
       "      <td>4718206</td>\n",
       "      <td>4757938</td>\n",
       "      <td>4779736</td>\n",
       "      <td>4799642</td>\n",
       "      <td>4816632</td>\n",
       "      <td>4831586</td>\n",
       "      <td>4843737</td>\n",
       "      <td>4854803</td>\n",
       "      <td>4866824</td>\n",
       "      <td>4877989</td>\n",
       "      <td>4891628</td>\n",
       "      <td>4907965</td>\n",
       "      <td>4921532</td>\n",
       "      <td>5050380</td>\n",
       "      <td>5073903</td>\n",
       "      <td>131185561946</td>\n",
       "      <td>34.055836</td>\n",
       "      <td>34.150778</td>\n",
       "      <td>34.329167</td>\n",
       "      <td>34.536796</td>\n",
       "      <td>34.834664</td>\n",
       "      <td>35.285750</td>\n",
       "      <td>35.620078</td>\n",
       "      <td>35.965894</td>\n",
       "      <td>36.268763</td>\n",
       "      <td>36.434924</td>\n",
       "      <td>36.586663</td>\n",
       "      <td>36.716175</td>\n",
       "      <td>36.830166</td>\n",
       "      <td>36.922790</td>\n",
       "      <td>37.007144</td>\n",
       "      <td>37.098778</td>\n",
       "      <td>37.183886</td>\n",
       "      <td>37.287853</td>\n",
       "      <td>37.412387</td>\n",
       "      <td>37.515805</td>\n",
       "      <td>38.497987</td>\n",
       "      <td>38.677297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>633714</td>\n",
       "      <td>642337</td>\n",
       "      <td>648414</td>\n",
       "      <td>659286</td>\n",
       "      <td>666946</td>\n",
       "      <td>675302</td>\n",
       "      <td>680300</td>\n",
       "      <td>687455</td>\n",
       "      <td>698895</td>\n",
       "      <td>710231</td>\n",
       "      <td>722349</td>\n",
       "      <td>730810</td>\n",
       "      <td>737626</td>\n",
       "      <td>737075</td>\n",
       "      <td>738430</td>\n",
       "      <td>742575</td>\n",
       "      <td>740983</td>\n",
       "      <td>736624</td>\n",
       "      <td>733603</td>\n",
       "      <td>731158</td>\n",
       "      <td>734923</td>\n",
       "      <td>733276</td>\n",
       "      <td>1479508971743</td>\n",
       "      <td>0.428327</td>\n",
       "      <td>0.434156</td>\n",
       "      <td>0.438263</td>\n",
       "      <td>0.445611</td>\n",
       "      <td>0.450789</td>\n",
       "      <td>0.456437</td>\n",
       "      <td>0.459815</td>\n",
       "      <td>0.464651</td>\n",
       "      <td>0.472383</td>\n",
       "      <td>0.480045</td>\n",
       "      <td>0.488236</td>\n",
       "      <td>0.493954</td>\n",
       "      <td>0.498561</td>\n",
       "      <td>0.498189</td>\n",
       "      <td>0.499105</td>\n",
       "      <td>0.501906</td>\n",
       "      <td>0.500830</td>\n",
       "      <td>0.497884</td>\n",
       "      <td>0.495842</td>\n",
       "      <td>0.494190</td>\n",
       "      <td>0.496734</td>\n",
       "      <td>0.495621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>5273477</td>\n",
       "      <td>5396255</td>\n",
       "      <td>5510364</td>\n",
       "      <td>5652404</td>\n",
       "      <td>5839077</td>\n",
       "      <td>6029141</td>\n",
       "      <td>6167681</td>\n",
       "      <td>6280362</td>\n",
       "      <td>6343154</td>\n",
       "      <td>6392017</td>\n",
       "      <td>6473416</td>\n",
       "      <td>6556344</td>\n",
       "      <td>6634690</td>\n",
       "      <td>6732873</td>\n",
       "      <td>6832810</td>\n",
       "      <td>6944767</td>\n",
       "      <td>7048088</td>\n",
       "      <td>7164228</td>\n",
       "      <td>7291843</td>\n",
       "      <td>7421401</td>\n",
       "      <td>7272487</td>\n",
       "      <td>7365684</td>\n",
       "      <td>294366118294</td>\n",
       "      <td>17.914687</td>\n",
       "      <td>18.331780</td>\n",
       "      <td>18.719423</td>\n",
       "      <td>19.201952</td>\n",
       "      <td>19.836104</td>\n",
       "      <td>20.481776</td>\n",
       "      <td>20.952415</td>\n",
       "      <td>21.335207</td>\n",
       "      <td>21.548519</td>\n",
       "      <td>21.714513</td>\n",
       "      <td>21.991036</td>\n",
       "      <td>22.272754</td>\n",
       "      <td>22.538905</td>\n",
       "      <td>22.872446</td>\n",
       "      <td>23.211944</td>\n",
       "      <td>23.592277</td>\n",
       "      <td>23.943272</td>\n",
       "      <td>24.337815</td>\n",
       "      <td>24.771339</td>\n",
       "      <td>25.211465</td>\n",
       "      <td>24.705584</td>\n",
       "      <td>25.022187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2691571</td>\n",
       "      <td>2705927</td>\n",
       "      <td>2724816</td>\n",
       "      <td>2749686</td>\n",
       "      <td>2781097</td>\n",
       "      <td>2821761</td>\n",
       "      <td>2848650</td>\n",
       "      <td>2874554</td>\n",
       "      <td>2896843</td>\n",
       "      <td>2915918</td>\n",
       "      <td>2941038</td>\n",
       "      <td>2952876</td>\n",
       "      <td>2960459</td>\n",
       "      <td>2968759</td>\n",
       "      <td>2979732</td>\n",
       "      <td>2991815</td>\n",
       "      <td>3003855</td>\n",
       "      <td>3012161</td>\n",
       "      <td>3020985</td>\n",
       "      <td>3030522</td>\n",
       "      <td>3028443</td>\n",
       "      <td>3046404</td>\n",
       "      <td>134658517854</td>\n",
       "      <td>19.988123</td>\n",
       "      <td>20.094733</td>\n",
       "      <td>20.235007</td>\n",
       "      <td>20.419696</td>\n",
       "      <td>20.652960</td>\n",
       "      <td>20.954939</td>\n",
       "      <td>21.154622</td>\n",
       "      <td>21.346990</td>\n",
       "      <td>21.512512</td>\n",
       "      <td>21.654167</td>\n",
       "      <td>21.840713</td>\n",
       "      <td>21.928624</td>\n",
       "      <td>21.984937</td>\n",
       "      <td>22.046574</td>\n",
       "      <td>22.128062</td>\n",
       "      <td>22.217792</td>\n",
       "      <td>22.307204</td>\n",
       "      <td>22.368886</td>\n",
       "      <td>22.434414</td>\n",
       "      <td>22.505238</td>\n",
       "      <td>22.489799</td>\n",
       "      <td>22.623181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>California</td>\n",
       "      <td>34479458</td>\n",
       "      <td>34871843</td>\n",
       "      <td>35253159</td>\n",
       "      <td>35574576</td>\n",
       "      <td>35827943</td>\n",
       "      <td>36021202</td>\n",
       "      <td>36250311</td>\n",
       "      <td>36604337</td>\n",
       "      <td>36961229</td>\n",
       "      <td>37253956</td>\n",
       "      <td>37636311</td>\n",
       "      <td>37944551</td>\n",
       "      <td>38253768</td>\n",
       "      <td>38586706</td>\n",
       "      <td>38904296</td>\n",
       "      <td>39149186</td>\n",
       "      <td>39337785</td>\n",
       "      <td>39437463</td>\n",
       "      <td>39437610</td>\n",
       "      <td>39368078</td>\n",
       "      <td>39145060</td>\n",
       "      <td>39040616</td>\n",
       "      <td>403673433805</td>\n",
       "      <td>85.414236</td>\n",
       "      <td>86.386272</td>\n",
       "      <td>87.330887</td>\n",
       "      <td>88.127117</td>\n",
       "      <td>88.754771</td>\n",
       "      <td>89.233521</td>\n",
       "      <td>89.801082</td>\n",
       "      <td>90.678093</td>\n",
       "      <td>91.562203</td>\n",
       "      <td>92.287361</td>\n",
       "      <td>93.234550</td>\n",
       "      <td>93.998138</td>\n",
       "      <td>94.764145</td>\n",
       "      <td>95.588916</td>\n",
       "      <td>96.375666</td>\n",
       "      <td>96.982320</td>\n",
       "      <td>97.449527</td>\n",
       "      <td>97.696454</td>\n",
       "      <td>97.696818</td>\n",
       "      <td>97.524570</td>\n",
       "      <td>96.972099</td>\n",
       "      <td>96.713365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE  COUNTY       State  POPESTIMATE2001  POPESTIMATE2002  POPESTIMATE2003  POPESTIMATE2004  POPESTIMATE2005  POPESTIMATE2006  POPESTIMATE2007  POPESTIMATE2008  POPESTIMATE2009  POPESTIMATE2010  POPESTIMATE2011  POPESTIMATE2012  POPESTIMATE2013  POPESTIMATE2014  POPESTIMATE2015  POPESTIMATE2016  POPESTIMATE2017  POPESTIMATE2018  POPESTIMATE2019  POPESTIMATE2020  POPESTIMATE2021  POPESTIMATE2022    Total_ALAND  Density2001  Density2002  Density2003  Density2004  Density2005  Density2006  Density2007  Density2008  Density2009  Density2010  Density2011  Density2012  Density2013  Density2014  Density2015  Density2016  Density2017  Density2018  Density2019  Density2020  Density2021  Density2022\n",
       "0      1       0     Alabama          4467634          4480089          4503491          4530729          4569805          4628981          4672840          4718206          4757938          4779736          4799642          4816632          4831586          4843737          4854803          4866824          4877989          4891628          4907965          4921532          5050380          5073903   131185561946    34.055836    34.150778    34.329167    34.536796    34.834664    35.285750    35.620078    35.965894    36.268763    36.434924    36.586663    36.716175    36.830166    36.922790    37.007144    37.098778    37.183886    37.287853    37.412387    37.515805    38.497987    38.677297\n",
       "1      2       0      Alaska           633714           642337           648414           659286           666946           675302           680300           687455           698895           710231           722349           730810           737626           737075           738430           742575           740983           736624           733603           731158           734923           733276  1479508971743     0.428327     0.434156     0.438263     0.445611     0.450789     0.456437     0.459815     0.464651     0.472383     0.480045     0.488236     0.493954     0.498561     0.498189     0.499105     0.501906     0.500830     0.497884     0.495842     0.494190     0.496734     0.495621\n",
       "2      4       0     Arizona          5273477          5396255          5510364          5652404          5839077          6029141          6167681          6280362          6343154          6392017          6473416          6556344          6634690          6732873          6832810          6944767          7048088          7164228          7291843          7421401          7272487          7365684   294366118294    17.914687    18.331780    18.719423    19.201952    19.836104    20.481776    20.952415    21.335207    21.548519    21.714513    21.991036    22.272754    22.538905    22.872446    23.211944    23.592277    23.943272    24.337815    24.771339    25.211465    24.705584    25.022187\n",
       "3      5       0    Arkansas          2691571          2705927          2724816          2749686          2781097          2821761          2848650          2874554          2896843          2915918          2941038          2952876          2960459          2968759          2979732          2991815          3003855          3012161          3020985          3030522          3028443          3046404   134658517854    19.988123    20.094733    20.235007    20.419696    20.652960    20.954939    21.154622    21.346990    21.512512    21.654167    21.840713    21.928624    21.984937    22.046574    22.128062    22.217792    22.307204    22.368886    22.434414    22.505238    22.489799    22.623181\n",
       "4      6       0  California         34479458         34871843         35253159         35574576         35827943         36021202         36250311         36604337         36961229         37253956         37636311         37944551         38253768         38586706         38904296         39149186         39337785         39437463         39437610         39368078         39145060         39040616   403673433805    85.414236    86.386272    87.330887    88.127117    88.754771    89.233521    89.801082    90.678093    91.562203    92.287361    93.234550    93.998138    94.764145    95.588916    96.375666    96.982320    97.449527    97.696454    97.696818    97.524570    96.972099    96.713365"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.width', 1000)       \n",
    "\n",
    "# Now display the first few rows\n",
    "density_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b8cc48e0-bb33-4a8d-8155-76b8a5c4610d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>State</th>\n",
       "      <th>Density2001</th>\n",
       "      <th>Density2002</th>\n",
       "      <th>Density2003</th>\n",
       "      <th>Density2004</th>\n",
       "      <th>Density2005</th>\n",
       "      <th>Density2006</th>\n",
       "      <th>Density2007</th>\n",
       "      <th>Density2008</th>\n",
       "      <th>Density2009</th>\n",
       "      <th>Density2010</th>\n",
       "      <th>Density2011</th>\n",
       "      <th>Density2012</th>\n",
       "      <th>Density2013</th>\n",
       "      <th>Density2014</th>\n",
       "      <th>Density2015</th>\n",
       "      <th>Density2016</th>\n",
       "      <th>Density2017</th>\n",
       "      <th>Density2018</th>\n",
       "      <th>Density2019</th>\n",
       "      <th>Density2020</th>\n",
       "      <th>Density2021</th>\n",
       "      <th>Density2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>34.055836</td>\n",
       "      <td>34.150778</td>\n",
       "      <td>34.329167</td>\n",
       "      <td>34.536796</td>\n",
       "      <td>34.834664</td>\n",
       "      <td>35.285750</td>\n",
       "      <td>35.620078</td>\n",
       "      <td>35.965894</td>\n",
       "      <td>36.268763</td>\n",
       "      <td>36.434924</td>\n",
       "      <td>36.586663</td>\n",
       "      <td>36.716175</td>\n",
       "      <td>36.830166</td>\n",
       "      <td>36.922790</td>\n",
       "      <td>37.007144</td>\n",
       "      <td>37.098778</td>\n",
       "      <td>37.183886</td>\n",
       "      <td>37.287853</td>\n",
       "      <td>37.412387</td>\n",
       "      <td>37.515805</td>\n",
       "      <td>38.497987</td>\n",
       "      <td>38.677297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>0.428327</td>\n",
       "      <td>0.434156</td>\n",
       "      <td>0.438263</td>\n",
       "      <td>0.445611</td>\n",
       "      <td>0.450789</td>\n",
       "      <td>0.456437</td>\n",
       "      <td>0.459815</td>\n",
       "      <td>0.464651</td>\n",
       "      <td>0.472383</td>\n",
       "      <td>0.480045</td>\n",
       "      <td>0.488236</td>\n",
       "      <td>0.493954</td>\n",
       "      <td>0.498561</td>\n",
       "      <td>0.498189</td>\n",
       "      <td>0.499105</td>\n",
       "      <td>0.501906</td>\n",
       "      <td>0.500830</td>\n",
       "      <td>0.497884</td>\n",
       "      <td>0.495842</td>\n",
       "      <td>0.494190</td>\n",
       "      <td>0.496734</td>\n",
       "      <td>0.495621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>17.914687</td>\n",
       "      <td>18.331780</td>\n",
       "      <td>18.719423</td>\n",
       "      <td>19.201952</td>\n",
       "      <td>19.836104</td>\n",
       "      <td>20.481776</td>\n",
       "      <td>20.952415</td>\n",
       "      <td>21.335207</td>\n",
       "      <td>21.548519</td>\n",
       "      <td>21.714513</td>\n",
       "      <td>21.991036</td>\n",
       "      <td>22.272754</td>\n",
       "      <td>22.538905</td>\n",
       "      <td>22.872446</td>\n",
       "      <td>23.211944</td>\n",
       "      <td>23.592277</td>\n",
       "      <td>23.943272</td>\n",
       "      <td>24.337815</td>\n",
       "      <td>24.771339</td>\n",
       "      <td>25.211465</td>\n",
       "      <td>24.705584</td>\n",
       "      <td>25.022187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>19.988123</td>\n",
       "      <td>20.094733</td>\n",
       "      <td>20.235007</td>\n",
       "      <td>20.419696</td>\n",
       "      <td>20.652960</td>\n",
       "      <td>20.954939</td>\n",
       "      <td>21.154622</td>\n",
       "      <td>21.346990</td>\n",
       "      <td>21.512512</td>\n",
       "      <td>21.654167</td>\n",
       "      <td>21.840713</td>\n",
       "      <td>21.928624</td>\n",
       "      <td>21.984937</td>\n",
       "      <td>22.046574</td>\n",
       "      <td>22.128062</td>\n",
       "      <td>22.217792</td>\n",
       "      <td>22.307204</td>\n",
       "      <td>22.368886</td>\n",
       "      <td>22.434414</td>\n",
       "      <td>22.505238</td>\n",
       "      <td>22.489799</td>\n",
       "      <td>22.623181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>California</td>\n",
       "      <td>85.414236</td>\n",
       "      <td>86.386272</td>\n",
       "      <td>87.330887</td>\n",
       "      <td>88.127117</td>\n",
       "      <td>88.754771</td>\n",
       "      <td>89.233521</td>\n",
       "      <td>89.801082</td>\n",
       "      <td>90.678093</td>\n",
       "      <td>91.562203</td>\n",
       "      <td>92.287361</td>\n",
       "      <td>93.234550</td>\n",
       "      <td>93.998138</td>\n",
       "      <td>94.764145</td>\n",
       "      <td>95.588916</td>\n",
       "      <td>96.375666</td>\n",
       "      <td>96.982320</td>\n",
       "      <td>97.449527</td>\n",
       "      <td>97.696454</td>\n",
       "      <td>97.696818</td>\n",
       "      <td>97.524570</td>\n",
       "      <td>96.972099</td>\n",
       "      <td>96.713365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE  COUNTY       State  Density2001  Density2002  Density2003  Density2004  Density2005  Density2006  Density2007  Density2008  Density2009  Density2010  Density2011  Density2012  Density2013  Density2014  Density2015  Density2016  Density2017  Density2018  Density2019  Density2020  Density2021  Density2022\n",
       "0      1       0     Alabama    34.055836    34.150778    34.329167    34.536796    34.834664    35.285750    35.620078    35.965894    36.268763    36.434924    36.586663    36.716175    36.830166    36.922790    37.007144    37.098778    37.183886    37.287853    37.412387    37.515805    38.497987    38.677297\n",
       "1      2       0      Alaska     0.428327     0.434156     0.438263     0.445611     0.450789     0.456437     0.459815     0.464651     0.472383     0.480045     0.488236     0.493954     0.498561     0.498189     0.499105     0.501906     0.500830     0.497884     0.495842     0.494190     0.496734     0.495621\n",
       "2      4       0     Arizona    17.914687    18.331780    18.719423    19.201952    19.836104    20.481776    20.952415    21.335207    21.548519    21.714513    21.991036    22.272754    22.538905    22.872446    23.211944    23.592277    23.943272    24.337815    24.771339    25.211465    24.705584    25.022187\n",
       "3      5       0    Arkansas    19.988123    20.094733    20.235007    20.419696    20.652960    20.954939    21.154622    21.346990    21.512512    21.654167    21.840713    21.928624    21.984937    22.046574    22.128062    22.217792    22.307204    22.368886    22.434414    22.505238    22.489799    22.623181\n",
       "4      6       0  California    85.414236    86.386272    87.330887    88.127117    88.754771    89.233521    89.801082    90.678093    91.562203    92.287361    93.234550    93.998138    94.764145    95.588916    96.375666    96.982320    97.449527    97.696454    97.696818    97.524570    96.972099    96.713365"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we keep only the density \n",
    "density_state = density_state.drop(['POPESTIMATE2001','POPESTIMATE2002','POPESTIMATE2003','POPESTIMATE2004','POPESTIMATE2005','POPESTIMATE2006','POPESTIMATE2007','POPESTIMATE2008','POPESTIMATE2009','POPESTIMATE2010','POPESTIMATE2011','POPESTIMATE2012','POPESTIMATE2013','POPESTIMATE2014','POPESTIMATE2015','POPESTIMATE2016','POPESTIMATE2017','POPESTIMATE2018','POPESTIMATE2019','POPESTIMATE2020','POPESTIMATE2021','POPESTIMATE2022', 'Total_ALAND'], axis=1)\n",
    "density_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "33cb08ca-43aa-498d-81a5-6d58669efaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USPS</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>County</th>\n",
       "      <th>ALAND</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>1001</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>1539631459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>1003</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>4117781416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL</td>\n",
       "      <td>1005</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>2292160151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AL</td>\n",
       "      <td>1007</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>1612188713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL</td>\n",
       "      <td>1009</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>1670259099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>PR</td>\n",
       "      <td>72145</td>\n",
       "      <td>Vega Baja Municipio</td>\n",
       "      <td>118746311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3218</th>\n",
       "      <td>PR</td>\n",
       "      <td>72147</td>\n",
       "      <td>Vieques Municipio</td>\n",
       "      <td>131541389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>PR</td>\n",
       "      <td>72149</td>\n",
       "      <td>Villalba Municipio</td>\n",
       "      <td>92200653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3220</th>\n",
       "      <td>PR</td>\n",
       "      <td>72151</td>\n",
       "      <td>Yabucoa Municipio</td>\n",
       "      <td>143005187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3221</th>\n",
       "      <td>PR</td>\n",
       "      <td>72153</td>\n",
       "      <td>Yauco Municipio</td>\n",
       "      <td>175436927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3222 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     USPS  GEOID               County       ALAND\n",
       "0      AL   1001       Autauga County  1539631459\n",
       "1      AL   1003       Baldwin County  4117781416\n",
       "2      AL   1005       Barbour County  2292160151\n",
       "3      AL   1007          Bibb County  1612188713\n",
       "4      AL   1009        Blount County  1670259099\n",
       "...   ...    ...                  ...         ...\n",
       "3217   PR  72145  Vega Baja Municipio   118746311\n",
       "3218   PR  72147    Vieques Municipio   131541389\n",
       "3219   PR  72149   Villalba Municipio    92200653\n",
       "3220   PR  72151    Yabucoa Municipio   143005187\n",
       "3221   PR  72153      Yauco Municipio   175436927\n",
       "\n",
       "[3222 rows x 4 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can procede with the counties \n",
    "size_county # we see that there are counties from Puerto Rico which we want to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "08f7d2cf-3094-482e-8af3-588a2b4c36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_county = size_county[size_county['USPS'] != 'PR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5d29f339-eee6-44ec-9257-c048532199ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique counties: 1882\n"
     ]
    }
   ],
   "source": [
    "unique_count = size_county['County'].nunique()\n",
    "\n",
    "print(f\"Number of unique counties: {unique_count}\") # we see that we have unique 1882 names of the counties\n",
    "# this is due to some counties that have the same name. Therefore we use the GEOID (FIPS code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "28f6d5c9-2c7b-4023-87b7-edd3fb323181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>State</th>\n",
       "      <th>POPESTIMATE2001</th>\n",
       "      <th>POPESTIMATE2002</th>\n",
       "      <th>POPESTIMATE2003</th>\n",
       "      <th>POPESTIMATE2004</th>\n",
       "      <th>POPESTIMATE2005</th>\n",
       "      <th>POPESTIMATE2006</th>\n",
       "      <th>POPESTIMATE2007</th>\n",
       "      <th>POPESTIMATE2008</th>\n",
       "      <th>POPESTIMATE2009</th>\n",
       "      <th>POPESTIMATE2010</th>\n",
       "      <th>POPESTIMATE2011</th>\n",
       "      <th>POPESTIMATE2012</th>\n",
       "      <th>POPESTIMATE2013</th>\n",
       "      <th>POPESTIMATE2014</th>\n",
       "      <th>POPESTIMATE2015</th>\n",
       "      <th>POPESTIMATE2016</th>\n",
       "      <th>POPESTIMATE2017</th>\n",
       "      <th>POPESTIMATE2018</th>\n",
       "      <th>POPESTIMATE2019</th>\n",
       "      <th>POPESTIMATE2020</th>\n",
       "      <th>POPESTIMATE2021</th>\n",
       "      <th>POPESTIMATE2022</th>\n",
       "      <th>GEOID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>44889</td>\n",
       "      <td>45909</td>\n",
       "      <td>46800</td>\n",
       "      <td>48366</td>\n",
       "      <td>49676</td>\n",
       "      <td>51328</td>\n",
       "      <td>52405</td>\n",
       "      <td>53277</td>\n",
       "      <td>54135</td>\n",
       "      <td>54571</td>\n",
       "      <td>55229</td>\n",
       "      <td>54970</td>\n",
       "      <td>54747</td>\n",
       "      <td>54922</td>\n",
       "      <td>54903</td>\n",
       "      <td>55302</td>\n",
       "      <td>55448</td>\n",
       "      <td>55533</td>\n",
       "      <td>55769</td>\n",
       "      <td>56145</td>\n",
       "      <td>59203</td>\n",
       "      <td>59726</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>144875</td>\n",
       "      <td>147957</td>\n",
       "      <td>151509</td>\n",
       "      <td>156266</td>\n",
       "      <td>162183</td>\n",
       "      <td>168121</td>\n",
       "      <td>172404</td>\n",
       "      <td>175827</td>\n",
       "      <td>179406</td>\n",
       "      <td>182265</td>\n",
       "      <td>186579</td>\n",
       "      <td>190203</td>\n",
       "      <td>194978</td>\n",
       "      <td>199306</td>\n",
       "      <td>203101</td>\n",
       "      <td>207787</td>\n",
       "      <td>212737</td>\n",
       "      <td>218071</td>\n",
       "      <td>223565</td>\n",
       "      <td>229287</td>\n",
       "      <td>239439</td>\n",
       "      <td>246531</td>\n",
       "      <td>1003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>28863</td>\n",
       "      <td>28653</td>\n",
       "      <td>28594</td>\n",
       "      <td>28287</td>\n",
       "      <td>28027</td>\n",
       "      <td>27861</td>\n",
       "      <td>27757</td>\n",
       "      <td>27808</td>\n",
       "      <td>27657</td>\n",
       "      <td>27457</td>\n",
       "      <td>27344</td>\n",
       "      <td>27172</td>\n",
       "      <td>26946</td>\n",
       "      <td>26768</td>\n",
       "      <td>26300</td>\n",
       "      <td>25828</td>\n",
       "      <td>25169</td>\n",
       "      <td>24887</td>\n",
       "      <td>24657</td>\n",
       "      <td>24589</td>\n",
       "      <td>24533</td>\n",
       "      <td>24700</td>\n",
       "      <td>1005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>21028</td>\n",
       "      <td>21199</td>\n",
       "      <td>21399</td>\n",
       "      <td>21721</td>\n",
       "      <td>22042</td>\n",
       "      <td>22099</td>\n",
       "      <td>22438</td>\n",
       "      <td>22705</td>\n",
       "      <td>22941</td>\n",
       "      <td>22915</td>\n",
       "      <td>22736</td>\n",
       "      <td>22657</td>\n",
       "      <td>22510</td>\n",
       "      <td>22541</td>\n",
       "      <td>22553</td>\n",
       "      <td>22590</td>\n",
       "      <td>22532</td>\n",
       "      <td>22300</td>\n",
       "      <td>22313</td>\n",
       "      <td>22136</td>\n",
       "      <td>22359</td>\n",
       "      <td>21986</td>\n",
       "      <td>1007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>51845</td>\n",
       "      <td>52551</td>\n",
       "      <td>53457</td>\n",
       "      <td>54124</td>\n",
       "      <td>54624</td>\n",
       "      <td>55485</td>\n",
       "      <td>56240</td>\n",
       "      <td>57055</td>\n",
       "      <td>57341</td>\n",
       "      <td>57322</td>\n",
       "      <td>57561</td>\n",
       "      <td>57585</td>\n",
       "      <td>57630</td>\n",
       "      <td>57536</td>\n",
       "      <td>57535</td>\n",
       "      <td>57487</td>\n",
       "      <td>57801</td>\n",
       "      <td>57770</td>\n",
       "      <td>57840</td>\n",
       "      <td>57879</td>\n",
       "      <td>59079</td>\n",
       "      <td>59516</td>\n",
       "      <td>1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3175</th>\n",
       "      <td>56</td>\n",
       "      <td>37</td>\n",
       "      <td>Sweetwater County</td>\n",
       "      <td>36899</td>\n",
       "      <td>37428</td>\n",
       "      <td>37450</td>\n",
       "      <td>38026</td>\n",
       "      <td>38739</td>\n",
       "      <td>39749</td>\n",
       "      <td>41470</td>\n",
       "      <td>42358</td>\n",
       "      <td>44133</td>\n",
       "      <td>43806</td>\n",
       "      <td>44000</td>\n",
       "      <td>45032</td>\n",
       "      <td>45189</td>\n",
       "      <td>44996</td>\n",
       "      <td>44780</td>\n",
       "      <td>44319</td>\n",
       "      <td>43663</td>\n",
       "      <td>43188</td>\n",
       "      <td>42917</td>\n",
       "      <td>42673</td>\n",
       "      <td>41626</td>\n",
       "      <td>41374</td>\n",
       "      <td>56037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>56</td>\n",
       "      <td>39</td>\n",
       "      <td>Teton County</td>\n",
       "      <td>18653</td>\n",
       "      <td>18837</td>\n",
       "      <td>19066</td>\n",
       "      <td>19467</td>\n",
       "      <td>19632</td>\n",
       "      <td>20014</td>\n",
       "      <td>20472</td>\n",
       "      <td>20988</td>\n",
       "      <td>21232</td>\n",
       "      <td>21294</td>\n",
       "      <td>21422</td>\n",
       "      <td>21643</td>\n",
       "      <td>22335</td>\n",
       "      <td>22801</td>\n",
       "      <td>23083</td>\n",
       "      <td>23255</td>\n",
       "      <td>23383</td>\n",
       "      <td>23261</td>\n",
       "      <td>23385</td>\n",
       "      <td>23497</td>\n",
       "      <td>23605</td>\n",
       "      <td>23297</td>\n",
       "      <td>56039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3177</th>\n",
       "      <td>56</td>\n",
       "      <td>41</td>\n",
       "      <td>Uinta County</td>\n",
       "      <td>19413</td>\n",
       "      <td>19587</td>\n",
       "      <td>19480</td>\n",
       "      <td>19470</td>\n",
       "      <td>19494</td>\n",
       "      <td>19709</td>\n",
       "      <td>20171</td>\n",
       "      <td>20613</td>\n",
       "      <td>21054</td>\n",
       "      <td>21118</td>\n",
       "      <td>20901</td>\n",
       "      <td>21008</td>\n",
       "      <td>20969</td>\n",
       "      <td>20835</td>\n",
       "      <td>20777</td>\n",
       "      <td>20711</td>\n",
       "      <td>20449</td>\n",
       "      <td>20299</td>\n",
       "      <td>20196</td>\n",
       "      <td>20215</td>\n",
       "      <td>20681</td>\n",
       "      <td>20727</td>\n",
       "      <td>56041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3178</th>\n",
       "      <td>56</td>\n",
       "      <td>43</td>\n",
       "      <td>Washakie County</td>\n",
       "      <td>8068</td>\n",
       "      <td>7988</td>\n",
       "      <td>7976</td>\n",
       "      <td>7960</td>\n",
       "      <td>8022</td>\n",
       "      <td>7979</td>\n",
       "      <td>8169</td>\n",
       "      <td>8229</td>\n",
       "      <td>8423</td>\n",
       "      <td>8533</td>\n",
       "      <td>8451</td>\n",
       "      <td>8410</td>\n",
       "      <td>8417</td>\n",
       "      <td>8277</td>\n",
       "      <td>8282</td>\n",
       "      <td>8180</td>\n",
       "      <td>8013</td>\n",
       "      <td>7886</td>\n",
       "      <td>7824</td>\n",
       "      <td>7760</td>\n",
       "      <td>7719</td>\n",
       "      <td>7724</td>\n",
       "      <td>56043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3179</th>\n",
       "      <td>56</td>\n",
       "      <td>45</td>\n",
       "      <td>Weston County</td>\n",
       "      <td>6487</td>\n",
       "      <td>6578</td>\n",
       "      <td>6610</td>\n",
       "      <td>6646</td>\n",
       "      <td>6594</td>\n",
       "      <td>6717</td>\n",
       "      <td>7033</td>\n",
       "      <td>7133</td>\n",
       "      <td>7266</td>\n",
       "      <td>7208</td>\n",
       "      <td>7142</td>\n",
       "      <td>7075</td>\n",
       "      <td>7132</td>\n",
       "      <td>7134</td>\n",
       "      <td>7202</td>\n",
       "      <td>7228</td>\n",
       "      <td>6962</td>\n",
       "      <td>6895</td>\n",
       "      <td>6880</td>\n",
       "      <td>6743</td>\n",
       "      <td>6746</td>\n",
       "      <td>6858</td>\n",
       "      <td>56045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3129 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      STATE  COUNTY              State  POPESTIMATE2001  POPESTIMATE2002  POPESTIMATE2003  POPESTIMATE2004  POPESTIMATE2005  POPESTIMATE2006  POPESTIMATE2007  POPESTIMATE2008  POPESTIMATE2009  POPESTIMATE2010  POPESTIMATE2011  POPESTIMATE2012  POPESTIMATE2013  POPESTIMATE2014  POPESTIMATE2015  POPESTIMATE2016  POPESTIMATE2017  POPESTIMATE2018  POPESTIMATE2019  POPESTIMATE2020  POPESTIMATE2021  POPESTIMATE2022  GEOID\n",
       "1         1       1     Autauga County            44889            45909            46800            48366            49676            51328            52405            53277            54135            54571            55229            54970            54747            54922            54903            55302            55448            55533            55769            56145            59203            59726   1001\n",
       "2         1       3     Baldwin County           144875           147957           151509           156266           162183           168121           172404           175827           179406           182265           186579           190203           194978           199306           203101           207787           212737           218071           223565           229287           239439           246531   1003\n",
       "3         1       5     Barbour County            28863            28653            28594            28287            28027            27861            27757            27808            27657            27457            27344            27172            26946            26768            26300            25828            25169            24887            24657            24589            24533            24700   1005\n",
       "4         1       7        Bibb County            21028            21199            21399            21721            22042            22099            22438            22705            22941            22915            22736            22657            22510            22541            22553            22590            22532            22300            22313            22136            22359            21986   1007\n",
       "5         1       9      Blount County            51845            52551            53457            54124            54624            55485            56240            57055            57341            57322            57561            57585            57630            57536            57535            57487            57801            57770            57840            57879            59079            59516   1009\n",
       "...     ...     ...                ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...              ...    ...\n",
       "3175     56      37  Sweetwater County            36899            37428            37450            38026            38739            39749            41470            42358            44133            43806            44000            45032            45189            44996            44780            44319            43663            43188            42917            42673            41626            41374  56037\n",
       "3176     56      39       Teton County            18653            18837            19066            19467            19632            20014            20472            20988            21232            21294            21422            21643            22335            22801            23083            23255            23383            23261            23385            23497            23605            23297  56039\n",
       "3177     56      41       Uinta County            19413            19587            19480            19470            19494            19709            20171            20613            21054            21118            20901            21008            20969            20835            20777            20711            20449            20299            20196            20215            20681            20727  56041\n",
       "3178     56      43    Washakie County             8068             7988             7976             7960             8022             7979             8169             8229             8423             8533             8451             8410             8417             8277             8282             8180             8013             7886             7824             7760             7719             7724  56043\n",
       "3179     56      45      Weston County             6487             6578             6610             6646             6594             6717             7033             7133             7266             7208             7142             7075             7132             7134             7202             7228             6962             6895             6880             6743             6746             6858  56045\n",
       "\n",
       "[3129 rows x 26 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Population_county # as we see here this does not have the GEOID, however it is quite easy to compose ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4b5f6737-f89b-47da-9833-e29d22e0f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can merge based on the GEOID and calculate the density\n",
    "density_county = pd.merge(Population_county, size_county, on = \"GEOID\", how = \"inner\")\n",
    "# Loop through the years to calculate density\n",
    "for year in range(2001, 2023):\n",
    "    density_county[f\"Density{year}\"] = density_county[f\"POPESTIMATE{year}\"] / (density_county[\"ALAND\"] / 1e+6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cfbaf15d-7bbc-4fda-80b8-6e6ac2a28b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>State</th>\n",
       "      <th>POPESTIMATE2001</th>\n",
       "      <th>POPESTIMATE2002</th>\n",
       "      <th>POPESTIMATE2003</th>\n",
       "      <th>POPESTIMATE2004</th>\n",
       "      <th>POPESTIMATE2005</th>\n",
       "      <th>POPESTIMATE2006</th>\n",
       "      <th>POPESTIMATE2007</th>\n",
       "      <th>POPESTIMATE2008</th>\n",
       "      <th>POPESTIMATE2009</th>\n",
       "      <th>POPESTIMATE2010</th>\n",
       "      <th>POPESTIMATE2011</th>\n",
       "      <th>POPESTIMATE2012</th>\n",
       "      <th>POPESTIMATE2013</th>\n",
       "      <th>POPESTIMATE2014</th>\n",
       "      <th>POPESTIMATE2015</th>\n",
       "      <th>POPESTIMATE2016</th>\n",
       "      <th>POPESTIMATE2017</th>\n",
       "      <th>POPESTIMATE2018</th>\n",
       "      <th>POPESTIMATE2019</th>\n",
       "      <th>POPESTIMATE2020</th>\n",
       "      <th>POPESTIMATE2021</th>\n",
       "      <th>POPESTIMATE2022</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>USPS</th>\n",
       "      <th>County</th>\n",
       "      <th>ALAND</th>\n",
       "      <th>Density2001</th>\n",
       "      <th>Density2002</th>\n",
       "      <th>Density2003</th>\n",
       "      <th>Density2004</th>\n",
       "      <th>Density2005</th>\n",
       "      <th>Density2006</th>\n",
       "      <th>Density2007</th>\n",
       "      <th>Density2008</th>\n",
       "      <th>Density2009</th>\n",
       "      <th>Density2010</th>\n",
       "      <th>Density2011</th>\n",
       "      <th>Density2012</th>\n",
       "      <th>Density2013</th>\n",
       "      <th>Density2014</th>\n",
       "      <th>Density2015</th>\n",
       "      <th>Density2016</th>\n",
       "      <th>Density2017</th>\n",
       "      <th>Density2018</th>\n",
       "      <th>Density2019</th>\n",
       "      <th>Density2020</th>\n",
       "      <th>Density2021</th>\n",
       "      <th>Density2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>44889</td>\n",
       "      <td>45909</td>\n",
       "      <td>46800</td>\n",
       "      <td>48366</td>\n",
       "      <td>49676</td>\n",
       "      <td>51328</td>\n",
       "      <td>52405</td>\n",
       "      <td>53277</td>\n",
       "      <td>54135</td>\n",
       "      <td>54571</td>\n",
       "      <td>55229</td>\n",
       "      <td>54970</td>\n",
       "      <td>54747</td>\n",
       "      <td>54922</td>\n",
       "      <td>54903</td>\n",
       "      <td>55302</td>\n",
       "      <td>55448</td>\n",
       "      <td>55533</td>\n",
       "      <td>55769</td>\n",
       "      <td>56145</td>\n",
       "      <td>59203</td>\n",
       "      <td>59726</td>\n",
       "      <td>1001</td>\n",
       "      <td>AL</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>1539631459</td>\n",
       "      <td>29.155679</td>\n",
       "      <td>29.818175</td>\n",
       "      <td>30.396885</td>\n",
       "      <td>31.414011</td>\n",
       "      <td>32.264864</td>\n",
       "      <td>33.337848</td>\n",
       "      <td>34.037366</td>\n",
       "      <td>34.603736</td>\n",
       "      <td>35.161012</td>\n",
       "      <td>35.444197</td>\n",
       "      <td>35.871572</td>\n",
       "      <td>35.703349</td>\n",
       "      <td>35.558510</td>\n",
       "      <td>35.672173</td>\n",
       "      <td>35.659833</td>\n",
       "      <td>35.918985</td>\n",
       "      <td>36.013813</td>\n",
       "      <td>36.069021</td>\n",
       "      <td>36.222305</td>\n",
       "      <td>36.466519</td>\n",
       "      <td>38.452709</td>\n",
       "      <td>38.792400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>144875</td>\n",
       "      <td>147957</td>\n",
       "      <td>151509</td>\n",
       "      <td>156266</td>\n",
       "      <td>162183</td>\n",
       "      <td>168121</td>\n",
       "      <td>172404</td>\n",
       "      <td>175827</td>\n",
       "      <td>179406</td>\n",
       "      <td>182265</td>\n",
       "      <td>186579</td>\n",
       "      <td>190203</td>\n",
       "      <td>194978</td>\n",
       "      <td>199306</td>\n",
       "      <td>203101</td>\n",
       "      <td>207787</td>\n",
       "      <td>212737</td>\n",
       "      <td>218071</td>\n",
       "      <td>223565</td>\n",
       "      <td>229287</td>\n",
       "      <td>239439</td>\n",
       "      <td>246531</td>\n",
       "      <td>1003</td>\n",
       "      <td>AL</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>4117781416</td>\n",
       "      <td>35.182781</td>\n",
       "      <td>35.931242</td>\n",
       "      <td>36.793842</td>\n",
       "      <td>37.949076</td>\n",
       "      <td>39.386015</td>\n",
       "      <td>40.828054</td>\n",
       "      <td>41.868177</td>\n",
       "      <td>42.699450</td>\n",
       "      <td>43.568607</td>\n",
       "      <td>44.262913</td>\n",
       "      <td>45.310564</td>\n",
       "      <td>46.190650</td>\n",
       "      <td>47.350255</td>\n",
       "      <td>48.401306</td>\n",
       "      <td>49.322919</td>\n",
       "      <td>50.460911</td>\n",
       "      <td>51.663014</td>\n",
       "      <td>52.958372</td>\n",
       "      <td>54.292586</td>\n",
       "      <td>55.682169</td>\n",
       "      <td>58.147574</td>\n",
       "      <td>59.869861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>28863</td>\n",
       "      <td>28653</td>\n",
       "      <td>28594</td>\n",
       "      <td>28287</td>\n",
       "      <td>28027</td>\n",
       "      <td>27861</td>\n",
       "      <td>27757</td>\n",
       "      <td>27808</td>\n",
       "      <td>27657</td>\n",
       "      <td>27457</td>\n",
       "      <td>27344</td>\n",
       "      <td>27172</td>\n",
       "      <td>26946</td>\n",
       "      <td>26768</td>\n",
       "      <td>26300</td>\n",
       "      <td>25828</td>\n",
       "      <td>25169</td>\n",
       "      <td>24887</td>\n",
       "      <td>24657</td>\n",
       "      <td>24589</td>\n",
       "      <td>24533</td>\n",
       "      <td>24700</td>\n",
       "      <td>1005</td>\n",
       "      <td>AL</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>2292160151</td>\n",
       "      <td>12.592052</td>\n",
       "      <td>12.500435</td>\n",
       "      <td>12.474696</td>\n",
       "      <td>12.340761</td>\n",
       "      <td>12.227331</td>\n",
       "      <td>12.154910</td>\n",
       "      <td>12.109538</td>\n",
       "      <td>12.131788</td>\n",
       "      <td>12.065911</td>\n",
       "      <td>11.978657</td>\n",
       "      <td>11.929358</td>\n",
       "      <td>11.854320</td>\n",
       "      <td>11.755723</td>\n",
       "      <td>11.678067</td>\n",
       "      <td>11.473893</td>\n",
       "      <td>11.267974</td>\n",
       "      <td>10.980472</td>\n",
       "      <td>10.857444</td>\n",
       "      <td>10.757102</td>\n",
       "      <td>10.727435</td>\n",
       "      <td>10.703004</td>\n",
       "      <td>10.775861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>21028</td>\n",
       "      <td>21199</td>\n",
       "      <td>21399</td>\n",
       "      <td>21721</td>\n",
       "      <td>22042</td>\n",
       "      <td>22099</td>\n",
       "      <td>22438</td>\n",
       "      <td>22705</td>\n",
       "      <td>22941</td>\n",
       "      <td>22915</td>\n",
       "      <td>22736</td>\n",
       "      <td>22657</td>\n",
       "      <td>22510</td>\n",
       "      <td>22541</td>\n",
       "      <td>22553</td>\n",
       "      <td>22590</td>\n",
       "      <td>22532</td>\n",
       "      <td>22300</td>\n",
       "      <td>22313</td>\n",
       "      <td>22136</td>\n",
       "      <td>22359</td>\n",
       "      <td>21986</td>\n",
       "      <td>1007</td>\n",
       "      <td>AL</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>1612188713</td>\n",
       "      <td>13.043138</td>\n",
       "      <td>13.149205</td>\n",
       "      <td>13.273260</td>\n",
       "      <td>13.472989</td>\n",
       "      <td>13.672097</td>\n",
       "      <td>13.707452</td>\n",
       "      <td>13.917726</td>\n",
       "      <td>14.083339</td>\n",
       "      <td>14.229724</td>\n",
       "      <td>14.213597</td>\n",
       "      <td>14.102567</td>\n",
       "      <td>14.053566</td>\n",
       "      <td>13.962385</td>\n",
       "      <td>13.981614</td>\n",
       "      <td>13.989057</td>\n",
       "      <td>14.012007</td>\n",
       "      <td>13.976031</td>\n",
       "      <td>13.832128</td>\n",
       "      <td>13.840191</td>\n",
       "      <td>13.730403</td>\n",
       "      <td>13.868724</td>\n",
       "      <td>13.637361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>51845</td>\n",
       "      <td>52551</td>\n",
       "      <td>53457</td>\n",
       "      <td>54124</td>\n",
       "      <td>54624</td>\n",
       "      <td>55485</td>\n",
       "      <td>56240</td>\n",
       "      <td>57055</td>\n",
       "      <td>57341</td>\n",
       "      <td>57322</td>\n",
       "      <td>57561</td>\n",
       "      <td>57585</td>\n",
       "      <td>57630</td>\n",
       "      <td>57536</td>\n",
       "      <td>57535</td>\n",
       "      <td>57487</td>\n",
       "      <td>57801</td>\n",
       "      <td>57770</td>\n",
       "      <td>57840</td>\n",
       "      <td>57879</td>\n",
       "      <td>59079</td>\n",
       "      <td>59516</td>\n",
       "      <td>1009</td>\n",
       "      <td>AL</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>1670259099</td>\n",
       "      <td>31.040094</td>\n",
       "      <td>31.462783</td>\n",
       "      <td>32.005214</td>\n",
       "      <td>32.404553</td>\n",
       "      <td>32.703908</td>\n",
       "      <td>33.219397</td>\n",
       "      <td>33.671423</td>\n",
       "      <td>34.159371</td>\n",
       "      <td>34.330602</td>\n",
       "      <td>34.319226</td>\n",
       "      <td>34.462318</td>\n",
       "      <td>34.476687</td>\n",
       "      <td>34.503629</td>\n",
       "      <td>34.447350</td>\n",
       "      <td>34.446751</td>\n",
       "      <td>34.418013</td>\n",
       "      <td>34.606008</td>\n",
       "      <td>34.587448</td>\n",
       "      <td>34.629358</td>\n",
       "      <td>34.652707</td>\n",
       "      <td>35.371159</td>\n",
       "      <td>35.632795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE  COUNTY           State  POPESTIMATE2001  POPESTIMATE2002  POPESTIMATE2003  POPESTIMATE2004  POPESTIMATE2005  POPESTIMATE2006  POPESTIMATE2007  POPESTIMATE2008  POPESTIMATE2009  POPESTIMATE2010  POPESTIMATE2011  POPESTIMATE2012  POPESTIMATE2013  POPESTIMATE2014  POPESTIMATE2015  POPESTIMATE2016  POPESTIMATE2017  POPESTIMATE2018  POPESTIMATE2019  POPESTIMATE2020  POPESTIMATE2021  POPESTIMATE2022  GEOID USPS          County       ALAND  Density2001  Density2002  Density2003  Density2004  Density2005  Density2006  Density2007  Density2008  Density2009  Density2010  Density2011  Density2012  Density2013  Density2014  Density2015  Density2016  Density2017  Density2018  Density2019  Density2020  Density2021  Density2022\n",
       "0      1       1  Autauga County            44889            45909            46800            48366            49676            51328            52405            53277            54135            54571            55229            54970            54747            54922            54903            55302            55448            55533            55769            56145            59203            59726   1001   AL  Autauga County  1539631459    29.155679    29.818175    30.396885    31.414011    32.264864    33.337848    34.037366    34.603736    35.161012    35.444197    35.871572    35.703349    35.558510    35.672173    35.659833    35.918985    36.013813    36.069021    36.222305    36.466519    38.452709    38.792400\n",
       "1      1       3  Baldwin County           144875           147957           151509           156266           162183           168121           172404           175827           179406           182265           186579           190203           194978           199306           203101           207787           212737           218071           223565           229287           239439           246531   1003   AL  Baldwin County  4117781416    35.182781    35.931242    36.793842    37.949076    39.386015    40.828054    41.868177    42.699450    43.568607    44.262913    45.310564    46.190650    47.350255    48.401306    49.322919    50.460911    51.663014    52.958372    54.292586    55.682169    58.147574    59.869861\n",
       "2      1       5  Barbour County            28863            28653            28594            28287            28027            27861            27757            27808            27657            27457            27344            27172            26946            26768            26300            25828            25169            24887            24657            24589            24533            24700   1005   AL  Barbour County  2292160151    12.592052    12.500435    12.474696    12.340761    12.227331    12.154910    12.109538    12.131788    12.065911    11.978657    11.929358    11.854320    11.755723    11.678067    11.473893    11.267974    10.980472    10.857444    10.757102    10.727435    10.703004    10.775861\n",
       "3      1       7     Bibb County            21028            21199            21399            21721            22042            22099            22438            22705            22941            22915            22736            22657            22510            22541            22553            22590            22532            22300            22313            22136            22359            21986   1007   AL     Bibb County  1612188713    13.043138    13.149205    13.273260    13.472989    13.672097    13.707452    13.917726    14.083339    14.229724    14.213597    14.102567    14.053566    13.962385    13.981614    13.989057    14.012007    13.976031    13.832128    13.840191    13.730403    13.868724    13.637361\n",
       "4      1       9   Blount County            51845            52551            53457            54124            54624            55485            56240            57055            57341            57322            57561            57585            57630            57536            57535            57487            57801            57770            57840            57879            59079            59516   1009   AL   Blount County  1670259099    31.040094    31.462783    32.005214    32.404553    32.703908    33.219397    33.671423    34.159371    34.330602    34.319226    34.462318    34.476687    34.503629    34.447350    34.446751    34.418013    34.606008    34.587448    34.629358    34.652707    35.371159    35.632795"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "density_county.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "538e0aad-d682-427e-a33c-9eb3f236ee74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>County</th>\n",
       "      <th>Density2001</th>\n",
       "      <th>Density2002</th>\n",
       "      <th>Density2003</th>\n",
       "      <th>Density2004</th>\n",
       "      <th>Density2005</th>\n",
       "      <th>Density2006</th>\n",
       "      <th>Density2007</th>\n",
       "      <th>Density2008</th>\n",
       "      <th>Density2009</th>\n",
       "      <th>Density2010</th>\n",
       "      <th>Density2011</th>\n",
       "      <th>Density2012</th>\n",
       "      <th>Density2013</th>\n",
       "      <th>Density2014</th>\n",
       "      <th>Density2015</th>\n",
       "      <th>Density2016</th>\n",
       "      <th>Density2017</th>\n",
       "      <th>Density2018</th>\n",
       "      <th>Density2019</th>\n",
       "      <th>Density2020</th>\n",
       "      <th>Density2021</th>\n",
       "      <th>Density2022</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1001</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>29.155679</td>\n",
       "      <td>29.818175</td>\n",
       "      <td>30.396885</td>\n",
       "      <td>31.414011</td>\n",
       "      <td>32.264864</td>\n",
       "      <td>33.337848</td>\n",
       "      <td>34.037366</td>\n",
       "      <td>34.603736</td>\n",
       "      <td>35.161012</td>\n",
       "      <td>35.444197</td>\n",
       "      <td>35.871572</td>\n",
       "      <td>35.703349</td>\n",
       "      <td>35.558510</td>\n",
       "      <td>35.672173</td>\n",
       "      <td>35.659833</td>\n",
       "      <td>35.918985</td>\n",
       "      <td>36.013813</td>\n",
       "      <td>36.069021</td>\n",
       "      <td>36.222305</td>\n",
       "      <td>36.466519</td>\n",
       "      <td>38.452709</td>\n",
       "      <td>38.792400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1003</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>35.182781</td>\n",
       "      <td>35.931242</td>\n",
       "      <td>36.793842</td>\n",
       "      <td>37.949076</td>\n",
       "      <td>39.386015</td>\n",
       "      <td>40.828054</td>\n",
       "      <td>41.868177</td>\n",
       "      <td>42.699450</td>\n",
       "      <td>43.568607</td>\n",
       "      <td>44.262913</td>\n",
       "      <td>45.310564</td>\n",
       "      <td>46.190650</td>\n",
       "      <td>47.350255</td>\n",
       "      <td>48.401306</td>\n",
       "      <td>49.322919</td>\n",
       "      <td>50.460911</td>\n",
       "      <td>51.663014</td>\n",
       "      <td>52.958372</td>\n",
       "      <td>54.292586</td>\n",
       "      <td>55.682169</td>\n",
       "      <td>58.147574</td>\n",
       "      <td>59.869861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1005</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>12.592052</td>\n",
       "      <td>12.500435</td>\n",
       "      <td>12.474696</td>\n",
       "      <td>12.340761</td>\n",
       "      <td>12.227331</td>\n",
       "      <td>12.154910</td>\n",
       "      <td>12.109538</td>\n",
       "      <td>12.131788</td>\n",
       "      <td>12.065911</td>\n",
       "      <td>11.978657</td>\n",
       "      <td>11.929358</td>\n",
       "      <td>11.854320</td>\n",
       "      <td>11.755723</td>\n",
       "      <td>11.678067</td>\n",
       "      <td>11.473893</td>\n",
       "      <td>11.267974</td>\n",
       "      <td>10.980472</td>\n",
       "      <td>10.857444</td>\n",
       "      <td>10.757102</td>\n",
       "      <td>10.727435</td>\n",
       "      <td>10.703004</td>\n",
       "      <td>10.775861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1007</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>13.043138</td>\n",
       "      <td>13.149205</td>\n",
       "      <td>13.273260</td>\n",
       "      <td>13.472989</td>\n",
       "      <td>13.672097</td>\n",
       "      <td>13.707452</td>\n",
       "      <td>13.917726</td>\n",
       "      <td>14.083339</td>\n",
       "      <td>14.229724</td>\n",
       "      <td>14.213597</td>\n",
       "      <td>14.102567</td>\n",
       "      <td>14.053566</td>\n",
       "      <td>13.962385</td>\n",
       "      <td>13.981614</td>\n",
       "      <td>13.989057</td>\n",
       "      <td>14.012007</td>\n",
       "      <td>13.976031</td>\n",
       "      <td>13.832128</td>\n",
       "      <td>13.840191</td>\n",
       "      <td>13.730403</td>\n",
       "      <td>13.868724</td>\n",
       "      <td>13.637361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1009</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>31.040094</td>\n",
       "      <td>31.462783</td>\n",
       "      <td>32.005214</td>\n",
       "      <td>32.404553</td>\n",
       "      <td>32.703908</td>\n",
       "      <td>33.219397</td>\n",
       "      <td>33.671423</td>\n",
       "      <td>34.159371</td>\n",
       "      <td>34.330602</td>\n",
       "      <td>34.319226</td>\n",
       "      <td>34.462318</td>\n",
       "      <td>34.476687</td>\n",
       "      <td>34.503629</td>\n",
       "      <td>34.447350</td>\n",
       "      <td>34.446751</td>\n",
       "      <td>34.418013</td>\n",
       "      <td>34.606008</td>\n",
       "      <td>34.587448</td>\n",
       "      <td>34.629358</td>\n",
       "      <td>34.652707</td>\n",
       "      <td>35.371159</td>\n",
       "      <td>35.632795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE  GEOID          County  Density2001  Density2002  Density2003  Density2004  Density2005  Density2006  Density2007  Density2008  Density2009  Density2010  Density2011  Density2012  Density2013  Density2014  Density2015  Density2016  Density2017  Density2018  Density2019  Density2020  Density2021  Density2022\n",
       "0      1   1001  Autauga County    29.155679    29.818175    30.396885    31.414011    32.264864    33.337848    34.037366    34.603736    35.161012    35.444197    35.871572    35.703349    35.558510    35.672173    35.659833    35.918985    36.013813    36.069021    36.222305    36.466519    38.452709    38.792400\n",
       "1      1   1003  Baldwin County    35.182781    35.931242    36.793842    37.949076    39.386015    40.828054    41.868177    42.699450    43.568607    44.262913    45.310564    46.190650    47.350255    48.401306    49.322919    50.460911    51.663014    52.958372    54.292586    55.682169    58.147574    59.869861\n",
       "2      1   1005  Barbour County    12.592052    12.500435    12.474696    12.340761    12.227331    12.154910    12.109538    12.131788    12.065911    11.978657    11.929358    11.854320    11.755723    11.678067    11.473893    11.267974    10.980472    10.857444    10.757102    10.727435    10.703004    10.775861\n",
       "3      1   1007     Bibb County    13.043138    13.149205    13.273260    13.472989    13.672097    13.707452    13.917726    14.083339    14.229724    14.213597    14.102567    14.053566    13.962385    13.981614    13.989057    14.012007    13.976031    13.832128    13.840191    13.730403    13.868724    13.637361\n",
       "4      1   1009   Blount County    31.040094    31.462783    32.005214    32.404553    32.703908    33.219397    33.671423    34.159371    34.330602    34.319226    34.462318    34.476687    34.503629    34.447350    34.446751    34.418013    34.606008    34.587448    34.629358    34.652707    35.371159    35.632795"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can take out all the useless columns \n",
    "density_county = density_county.drop(['COUNTY', 'State', 'USPS','POPESTIMATE2001','POPESTIMATE2002','POPESTIMATE2003','POPESTIMATE2004','POPESTIMATE2005','POPESTIMATE2006','POPESTIMATE2007','POPESTIMATE2008','POPESTIMATE2009','POPESTIMATE2010','POPESTIMATE2011','POPESTIMATE2012','POPESTIMATE2013','POPESTIMATE2014','POPESTIMATE2015','POPESTIMATE2016','POPESTIMATE2017','POPESTIMATE2018','POPESTIMATE2019','POPESTIMATE2020','POPESTIMATE2021','POPESTIMATE2022', 'ALAND'], axis=1)\n",
    "density_county.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7b9ad8f8-37d5-40fb-85e6-8ec70a2a1aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot for the state\n",
    "# now we want to pivot the tables\n",
    "density_columns = [col for col in density_state.columns if col.startswith(\"Density\")]\n",
    "\n",
    "# Include 'CTYNAME' and 'GeoFIPS' in id_vars to retain them\n",
    "density_state = density_state.melt(\n",
    "    id_vars=[\"STATE\", \"State\"],  # Include additional columns to retain\n",
    "    value_vars= density_columns,\n",
    "    var_name=\"Year\",\n",
    "    value_name=\"Density\"\n",
    ")\n",
    "\n",
    "# Extract the year from the column names\n",
    "density_state[\"Year\"] = density_state[\"Year\"].str.extract(r\"(\\d{4})\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "adac9726-982a-4683-9923-69e2a9475a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>State</th>\n",
       "      <th>Year</th>\n",
       "      <th>Density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2001</td>\n",
       "      <td>34.055836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.428327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>2001</td>\n",
       "      <td>17.914687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2001</td>\n",
       "      <td>19.988123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>California</td>\n",
       "      <td>2001</td>\n",
       "      <td>85.414236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE       State  Year    Density\n",
       "0      1     Alabama  2001  34.055836\n",
       "1      2      Alaska  2001   0.428327\n",
       "2      4     Arizona  2001  17.914687\n",
       "3      5    Arkansas  2001  19.988123\n",
       "4      6  California  2001  85.414236"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "density_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "260d2a20-e1dc-4c2f-abdd-f117a5acde71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot for the county\n",
    "# now we want to pivot the tables\n",
    "density_columns = [col for col in density_county.columns if col.startswith(\"Density\")]\n",
    "\n",
    "# Include 'CTYNAME' and 'GeoFIPS' in id_vars to retain them\n",
    "density_county = density_county.melt(\n",
    "    id_vars=[\"STATE\", \"County\", \"GEOID\"],  # Include additional columns to retain\n",
    "    value_vars= density_columns,\n",
    "    var_name=\"Year\",\n",
    "    value_name=\"Density\"\n",
    ")\n",
    "\n",
    "# Extract the year from the column names\n",
    "density_county[\"Year\"] = density_county[\"Year\"].str.extract(r\"(\\d{4})\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "33371f67-ad11-43aa-8a51-7f6a1ced4320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>County</th>\n",
       "      <th>GEOID</th>\n",
       "      <th>Year</th>\n",
       "      <th>Density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Autauga County</td>\n",
       "      <td>1001</td>\n",
       "      <td>2001</td>\n",
       "      <td>29.155679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Baldwin County</td>\n",
       "      <td>1003</td>\n",
       "      <td>2001</td>\n",
       "      <td>35.182781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Barbour County</td>\n",
       "      <td>1005</td>\n",
       "      <td>2001</td>\n",
       "      <td>12.592052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Bibb County</td>\n",
       "      <td>1007</td>\n",
       "      <td>2001</td>\n",
       "      <td>13.043138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Blount County</td>\n",
       "      <td>1009</td>\n",
       "      <td>2001</td>\n",
       "      <td>31.040094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE          County  GEOID  Year    Density\n",
       "0      1  Autauga County   1001  2001  29.155679\n",
       "1      1  Baldwin County   1003  2001  35.182781\n",
       "2      1  Barbour County   1005  2001  12.592052\n",
       "3      1     Bibb County   1007  2001  13.043138\n",
       "4      1   Blount County   1009  2001  31.040094"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "density_county.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "96ec9a17-1b91-4bbd-978d-4a0763550b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1122 entries, 0 to 1121\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   STATE    1122 non-null   int64  \n",
      " 1   State    1122 non-null   object \n",
      " 2   Year     1122 non-null   int32  \n",
      " 3   Density  1122 non-null   float64\n",
      "dtypes: float64(1), int32(1), int64(1), object(1)\n",
      "memory usage: 30.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Because of the needed data for the streamlit app we need to create a dataset with the data of GDP per capita and density for each year and county\n",
    "density_state.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a86e2920-f3f5-44c1-a9a6-2fad60271046",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = density_state\n",
    "state = state[state['Year'] == 2008]\n",
    "state = state.drop(['Year', 'Density'], axis=1)\n",
    "state['STATE'] = state['STATE'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bba7d867-7466-4a51-87f5-f43b9fb5e950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 51 entries, 357 to 407\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype   \n",
      "---  ------  --------------  -----   \n",
      " 0   STATE   51 non-null     category\n",
      " 1   State   51 non-null     object  \n",
      "dtypes: category(1), object(1)\n",
      "memory usage: 3.3+ KB\n"
     ]
    }
   ],
   "source": [
    "state.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "450a0428-63f0-4c4c-9bed-b2e8355d38fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge density_county and GDP_capita_county\n",
    "merged_data_county = pd.merge(density_county, GDP_capita_county, on=['GEOID', 'Year'], how='inner')\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "merged_data_county.drop(['STATE_x', 'County_y'], axis=1, inplace=True)\n",
    "\n",
    "# Rename columns for clarity\n",
    "merged_data_county.rename(columns={'County_x': 'County', 'STATE_y': 'STATE'}, inplace=True)\n",
    "\n",
    "# Ensure 'STATE' in merged_data_county and 'STATE' in state DataFrame are of the same type\n",
    "state['STATE'] = state['STATE'].astype('category')\n",
    "merged_data_county['STATE'] = merged_data_county['STATE'].astype('category')\n",
    "\n",
    "# Merge with state DataFrame to add state names\n",
    "# Assuming 'state' DataFrame has columns: 'STATE' (state code) and 'State' (state name)\n",
    "merged_data_county = merged_data_county.merge(state[['STATE', 'State']], on='STATE', how='inner')\n",
    "\n",
    "\n",
    "# Save the merged dataset to a CSV file\n",
    "merged_data_county.to_csv(\"merged_data_county.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8689a8ba-9073-4be5-b30b-b895f1165db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64596 entries, 0 to 64595\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   County          64596 non-null  object \n",
      " 1   GEOID           64596 non-null  Int64  \n",
      " 2   Year            64596 non-null  int32  \n",
      " 3   Density         64596 non-null  float64\n",
      " 4   STATE           64596 non-null  int64  \n",
      " 5   GDP_per_capita  64570 non-null  float64\n",
      " 6   State           64596 non-null  object \n",
      "dtypes: Int64(1), float64(2), int32(1), int64(1), object(2)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "merged_data_county.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0110e67b-828c-4c26-bc4d-76adb977b3c9",
   "metadata": {},
   "source": [
    "# Final manipulations and generate the final datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3336114-0a01-4720-999d-6164484d8e71",
   "metadata": {},
   "source": [
    "We might want to know how long an event took, however we only have the beginning and the end of the event, therefore we have to calculate ourselves how much time it took "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "10e12e1b-e69f-46be-8068-128903e61448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH_NAME</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "      <th>CZ_TYPE</th>\n",
       "      <th>CZ_FIPS</th>\n",
       "      <th>CZ_NAME</th>\n",
       "      <th>WFO</th>\n",
       "      <th>BEGIN_DATE_TIME</th>\n",
       "      <th>CZ_TIMEZONE</th>\n",
       "      <th>END_DATE_TIME</th>\n",
       "      <th>INJURIES_DIRECT</th>\n",
       "      <th>INJURIES_INDIRECT</th>\n",
       "      <th>DEATHS_DIRECT</th>\n",
       "      <th>DEATHS_INDIRECT</th>\n",
       "      <th>DAMAGE_PROPERTY</th>\n",
       "      <th>DAMAGE_CROPS</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>MAGNITUDE</th>\n",
       "      <th>MAGNITUDE_TYPE</th>\n",
       "      <th>FLOOD_CAUSE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>TOR_F_SCALE</th>\n",
       "      <th>TOR_LENGTH</th>\n",
       "      <th>TOR_WIDTH</th>\n",
       "      <th>TOR_OTHER_WFO</th>\n",
       "      <th>TOR_OTHER_CZ_STATE</th>\n",
       "      <th>TOR_OTHER_CZ_FIPS</th>\n",
       "      <th>TOR_OTHER_CZ_NAME</th>\n",
       "      <th>BEGIN_RANGE</th>\n",
       "      <th>BEGIN_AZIMUTH</th>\n",
       "      <th>BEGIN_LOCATION</th>\n",
       "      <th>END_RANGE</th>\n",
       "      <th>END_AZIMUTH</th>\n",
       "      <th>END_LOCATION</th>\n",
       "      <th>BEGIN_LAT</th>\n",
       "      <th>BEGIN_LON</th>\n",
       "      <th>END_LAT</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>EPISODE_NARRATIVE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "      <th>DATA_SOURCE</th>\n",
       "      <th>BEGIN_DATETIME</th>\n",
       "      <th>END_DATETIME</th>\n",
       "      <th>DURATION_HOURS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200201</td>\n",
       "      <td>15</td>\n",
       "      <td>700</td>\n",
       "      <td>200201</td>\n",
       "      <td>15</td>\n",
       "      <td>1900</td>\n",
       "      <td>2151794</td>\n",
       "      <td>5324157</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>January</td>\n",
       "      <td>Heavy Snow</td>\n",
       "      <td>Z</td>\n",
       "      <td>8</td>\n",
       "      <td>WASATCH MOUNTAINS SOUTH OF I80</td>\n",
       "      <td>SLC</td>\n",
       "      <td>15-JAN-02 07:00:00</td>\n",
       "      <td>MST</td>\n",
       "      <td>15-JAN-02 19:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10K</td>\n",
       "      <td>0</td>\n",
       "      <td>GENERAL PUBLIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A cold arctic storm dropped several inches of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDS</td>\n",
       "      <td>2002-01-15 07:00:00</td>\n",
       "      <td>2002-01-15 19:00:00</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200201</td>\n",
       "      <td>15</td>\n",
       "      <td>700</td>\n",
       "      <td>200201</td>\n",
       "      <td>15</td>\n",
       "      <td>1900</td>\n",
       "      <td>2151794</td>\n",
       "      <td>5324158</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>January</td>\n",
       "      <td>Heavy Snow</td>\n",
       "      <td>Z</td>\n",
       "      <td>4</td>\n",
       "      <td>SOUTHERN WASATCH FRONT/LEHI/PROVO/NEPHI</td>\n",
       "      <td>SLC</td>\n",
       "      <td>15-JAN-02 07:00:00</td>\n",
       "      <td>MST</td>\n",
       "      <td>15-JAN-02 19:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50K</td>\n",
       "      <td>0</td>\n",
       "      <td>GENERAL PUBLIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A cold arctic storm dropped several inches of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDS</td>\n",
       "      <td>2002-01-15 07:00:00</td>\n",
       "      <td>2002-01-15 19:00:00</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200201</td>\n",
       "      <td>18</td>\n",
       "      <td>600</td>\n",
       "      <td>200201</td>\n",
       "      <td>18</td>\n",
       "      <td>1700</td>\n",
       "      <td>2151795</td>\n",
       "      <td>5324159</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>January</td>\n",
       "      <td>Heavy Snow</td>\n",
       "      <td>Z</td>\n",
       "      <td>2</td>\n",
       "      <td>NORTHERN WASATCH FRONT/BRIGHAM CITY/OGDEN BOUN...</td>\n",
       "      <td>SLC</td>\n",
       "      <td>18-JAN-02 06:00:00</td>\n",
       "      <td>MST</td>\n",
       "      <td>18-JAN-02 17:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>GENERAL PUBLIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Another storm brought good amounts of snow to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDS</td>\n",
       "      <td>2002-01-18 06:00:00</td>\n",
       "      <td>2002-01-18 17:00:00</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200203</td>\n",
       "      <td>13</td>\n",
       "      <td>700</td>\n",
       "      <td>200203</td>\n",
       "      <td>14</td>\n",
       "      <td>1100</td>\n",
       "      <td>2151814</td>\n",
       "      <td>5324343</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>March</td>\n",
       "      <td>Winter Storm</td>\n",
       "      <td>Z</td>\n",
       "      <td>6</td>\n",
       "      <td>P NC WEBER/P C MORGAN/P W C SUMMIT/P SC WASATCH</td>\n",
       "      <td>SLC</td>\n",
       "      <td>13-MAR-02 07:00:00</td>\n",
       "      <td>MST</td>\n",
       "      <td>14-MAR-02 11:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>GENERAL PUBLIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A blast of snow and wind hit much of Utah, pou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDS</td>\n",
       "      <td>2002-03-13 07:00:00</td>\n",
       "      <td>2002-03-14 11:00:00</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200203</td>\n",
       "      <td>13</td>\n",
       "      <td>700</td>\n",
       "      <td>200203</td>\n",
       "      <td>14</td>\n",
       "      <td>1100</td>\n",
       "      <td>2151814</td>\n",
       "      <td>5324344</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>March</td>\n",
       "      <td>Winter Storm</td>\n",
       "      <td>Z</td>\n",
       "      <td>7</td>\n",
       "      <td>RICH/CACHE T NW/P E C WEBER/MOST OF MORGAN/NW ...</td>\n",
       "      <td>SLC</td>\n",
       "      <td>13-MAR-02 07:00:00</td>\n",
       "      <td>MST</td>\n",
       "      <td>14-MAR-02 11:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>GENERAL PUBLIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A blast of snow and wind hit much of Utah, pou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDS</td>\n",
       "      <td>2002-03-13 07:00:00</td>\n",
       "      <td>2002-03-14 11:00:00</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  EPISODE_ID  EVENT_ID STATE  STATE_FIPS  YEAR MONTH_NAME    EVENT_TYPE CZ_TYPE  CZ_FIPS                                            CZ_NAME  WFO     BEGIN_DATE_TIME CZ_TIMEZONE       END_DATE_TIME  INJURIES_DIRECT  INJURIES_INDIRECT  DEATHS_DIRECT  DEATHS_INDIRECT DAMAGE_PROPERTY DAMAGE_CROPS          SOURCE  MAGNITUDE MAGNITUDE_TYPE FLOOD_CAUSE  CATEGORY TOR_F_SCALE  TOR_LENGTH  TOR_WIDTH TOR_OTHER_WFO TOR_OTHER_CZ_STATE  TOR_OTHER_CZ_FIPS TOR_OTHER_CZ_NAME  BEGIN_RANGE BEGIN_AZIMUTH BEGIN_LOCATION  END_RANGE END_AZIMUTH END_LOCATION  BEGIN_LAT  BEGIN_LON  END_LAT  END_LON                                  EPISODE_NARRATIVE EVENT_NARRATIVE DATA_SOURCE      BEGIN_DATETIME        END_DATETIME  DURATION_HOURS\n",
       "0           200201         15         700         200201       15      1900     2151794   5324157  UTAH          49  2002    January    Heavy Snow       Z        8                     WASATCH MOUNTAINS SOUTH OF I80  SLC  15-JAN-02 07:00:00         MST  15-JAN-02 19:00:00                0                  0              0                0             10K            0  GENERAL PUBLIC        NaN            NaN         NaN       NaN         NaN         NaN        NaN           NaN                NaN                NaN               NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  A cold arctic storm dropped several inches of ...             NaN         PDS 2002-01-15 07:00:00 2002-01-15 19:00:00            12.0\n",
       "1           200201         15         700         200201       15      1900     2151794   5324158  UTAH          49  2002    January    Heavy Snow       Z        4            SOUTHERN WASATCH FRONT/LEHI/PROVO/NEPHI  SLC  15-JAN-02 07:00:00         MST  15-JAN-02 19:00:00                0                  0              0                0             50K            0  GENERAL PUBLIC        NaN            NaN         NaN       NaN         NaN         NaN        NaN           NaN                NaN                NaN               NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  A cold arctic storm dropped several inches of ...             NaN         PDS 2002-01-15 07:00:00 2002-01-15 19:00:00            12.0\n",
       "2           200201         18         600         200201       18      1700     2151795   5324159  UTAH          49  2002    January    Heavy Snow       Z        2  NORTHERN WASATCH FRONT/BRIGHAM CITY/OGDEN BOUN...  SLC  18-JAN-02 06:00:00         MST  18-JAN-02 17:00:00                0                  0              0                0               0            0  GENERAL PUBLIC        NaN            NaN         NaN       NaN         NaN         NaN        NaN           NaN                NaN                NaN               NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  Another storm brought good amounts of snow to ...             NaN         PDS 2002-01-18 06:00:00 2002-01-18 17:00:00            11.0\n",
       "3           200203         13         700         200203       14      1100     2151814   5324343  UTAH          49  2002      March  Winter Storm       Z        6    P NC WEBER/P C MORGAN/P W C SUMMIT/P SC WASATCH  SLC  13-MAR-02 07:00:00         MST  14-MAR-02 11:00:00                0                  0              0                0               0            0  GENERAL PUBLIC        NaN            NaN         NaN       NaN         NaN         NaN        NaN           NaN                NaN                NaN               NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  A blast of snow and wind hit much of Utah, pou...             NaN         PDS 2002-03-13 07:00:00 2002-03-14 11:00:00            28.0\n",
       "4           200203         13         700         200203       14      1100     2151814   5324344  UTAH          49  2002      March  Winter Storm       Z        7  RICH/CACHE T NW/P E C WEBER/MOST OF MORGAN/NW ...  SLC  13-MAR-02 07:00:00         MST  14-MAR-02 11:00:00                0                  0              0                0               0            0  GENERAL PUBLIC        NaN            NaN         NaN       NaN         NaN         NaN        NaN           NaN                NaN                NaN               NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  A blast of snow and wind hit much of Utah, pou...             NaN         PDS 2002-03-13 07:00:00 2002-03-14 11:00:00            28.0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert BEGIN and END date/time into datetime\n",
    "df['BEGIN_DATETIME'] = pd.to_datetime(\n",
    "    df['BEGIN_YEARMONTH'].astype(str) + df['BEGIN_DAY'].astype(str).str.zfill(2) + df['BEGIN_TIME'].astype(str).str.zfill(4),\n",
    "    format='%Y%m%d%H%M'\n",
    ")\n",
    "df['END_DATETIME'] = pd.to_datetime(\n",
    "    df['END_YEARMONTH'].astype(str) + df['END_DAY'].astype(str).str.zfill(2) + df['END_TIME'].astype(str).str.zfill(4),\n",
    "    format='%Y%m%d%H%M'\n",
    ")\n",
    "\n",
    "# Calculate the duration in hours\n",
    "df['DURATION_HOURS'] = (df['END_DATETIME'] - df['BEGIN_DATETIME']).dt.total_seconds() / 3600\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfdc3bf-ff7d-41ab-a581-89dac6269441",
   "metadata": {},
   "source": [
    "The same could be said for the distance (we have beginning lat and long and ending lat and long). Based on this we can calculate the distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a5344684-7c49-477d-af51-d105495c0311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH_NAME</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "      <th>CZ_TYPE</th>\n",
       "      <th>CZ_FIPS</th>\n",
       "      <th>CZ_NAME</th>\n",
       "      <th>WFO</th>\n",
       "      <th>BEGIN_DATE_TIME</th>\n",
       "      <th>CZ_TIMEZONE</th>\n",
       "      <th>END_DATE_TIME</th>\n",
       "      <th>INJURIES_DIRECT</th>\n",
       "      <th>INJURIES_INDIRECT</th>\n",
       "      <th>DEATHS_DIRECT</th>\n",
       "      <th>DEATHS_INDIRECT</th>\n",
       "      <th>DAMAGE_PROPERTY</th>\n",
       "      <th>DAMAGE_CROPS</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>MAGNITUDE</th>\n",
       "      <th>MAGNITUDE_TYPE</th>\n",
       "      <th>FLOOD_CAUSE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>TOR_F_SCALE</th>\n",
       "      <th>TOR_LENGTH</th>\n",
       "      <th>TOR_WIDTH</th>\n",
       "      <th>TOR_OTHER_WFO</th>\n",
       "      <th>TOR_OTHER_CZ_STATE</th>\n",
       "      <th>TOR_OTHER_CZ_FIPS</th>\n",
       "      <th>TOR_OTHER_CZ_NAME</th>\n",
       "      <th>BEGIN_RANGE</th>\n",
       "      <th>BEGIN_AZIMUTH</th>\n",
       "      <th>BEGIN_LOCATION</th>\n",
       "      <th>END_RANGE</th>\n",
       "      <th>END_AZIMUTH</th>\n",
       "      <th>END_LOCATION</th>\n",
       "      <th>BEGIN_LAT</th>\n",
       "      <th>BEGIN_LON</th>\n",
       "      <th>END_LAT</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>EPISODE_NARRATIVE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "      <th>DATA_SOURCE</th>\n",
       "      <th>BEGIN_DATETIME</th>\n",
       "      <th>END_DATETIME</th>\n",
       "      <th>DURATION_HOURS</th>\n",
       "      <th>Distance_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200201</td>\n",
       "      <td>15</td>\n",
       "      <td>700</td>\n",
       "      <td>200201</td>\n",
       "      <td>15</td>\n",
       "      <td>1900</td>\n",
       "      <td>2151794</td>\n",
       "      <td>5324157</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>January</td>\n",
       "      <td>Heavy Snow</td>\n",
       "      <td>Z</td>\n",
       "      <td>8</td>\n",
       "      <td>WASATCH MOUNTAINS SOUTH OF I80</td>\n",
       "      <td>SLC</td>\n",
       "      <td>15-JAN-02 07:00:00</td>\n",
       "      <td>MST</td>\n",
       "      <td>15-JAN-02 19:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10K</td>\n",
       "      <td>0</td>\n",
       "      <td>GENERAL PUBLIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A cold arctic storm dropped several inches of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDS</td>\n",
       "      <td>2002-01-15 07:00:00</td>\n",
       "      <td>2002-01-15 19:00:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200201</td>\n",
       "      <td>15</td>\n",
       "      <td>700</td>\n",
       "      <td>200201</td>\n",
       "      <td>15</td>\n",
       "      <td>1900</td>\n",
       "      <td>2151794</td>\n",
       "      <td>5324158</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>January</td>\n",
       "      <td>Heavy Snow</td>\n",
       "      <td>Z</td>\n",
       "      <td>4</td>\n",
       "      <td>SOUTHERN WASATCH FRONT/LEHI/PROVO/NEPHI</td>\n",
       "      <td>SLC</td>\n",
       "      <td>15-JAN-02 07:00:00</td>\n",
       "      <td>MST</td>\n",
       "      <td>15-JAN-02 19:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50K</td>\n",
       "      <td>0</td>\n",
       "      <td>GENERAL PUBLIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A cold arctic storm dropped several inches of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDS</td>\n",
       "      <td>2002-01-15 07:00:00</td>\n",
       "      <td>2002-01-15 19:00:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200201</td>\n",
       "      <td>18</td>\n",
       "      <td>600</td>\n",
       "      <td>200201</td>\n",
       "      <td>18</td>\n",
       "      <td>1700</td>\n",
       "      <td>2151795</td>\n",
       "      <td>5324159</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>January</td>\n",
       "      <td>Heavy Snow</td>\n",
       "      <td>Z</td>\n",
       "      <td>2</td>\n",
       "      <td>NORTHERN WASATCH FRONT/BRIGHAM CITY/OGDEN BOUN...</td>\n",
       "      <td>SLC</td>\n",
       "      <td>18-JAN-02 06:00:00</td>\n",
       "      <td>MST</td>\n",
       "      <td>18-JAN-02 17:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>GENERAL PUBLIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Another storm brought good amounts of snow to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDS</td>\n",
       "      <td>2002-01-18 06:00:00</td>\n",
       "      <td>2002-01-18 17:00:00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200203</td>\n",
       "      <td>13</td>\n",
       "      <td>700</td>\n",
       "      <td>200203</td>\n",
       "      <td>14</td>\n",
       "      <td>1100</td>\n",
       "      <td>2151814</td>\n",
       "      <td>5324343</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>March</td>\n",
       "      <td>Winter Storm</td>\n",
       "      <td>Z</td>\n",
       "      <td>6</td>\n",
       "      <td>P NC WEBER/P C MORGAN/P W C SUMMIT/P SC WASATCH</td>\n",
       "      <td>SLC</td>\n",
       "      <td>13-MAR-02 07:00:00</td>\n",
       "      <td>MST</td>\n",
       "      <td>14-MAR-02 11:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>GENERAL PUBLIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A blast of snow and wind hit much of Utah, pou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDS</td>\n",
       "      <td>2002-03-13 07:00:00</td>\n",
       "      <td>2002-03-14 11:00:00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200203</td>\n",
       "      <td>13</td>\n",
       "      <td>700</td>\n",
       "      <td>200203</td>\n",
       "      <td>14</td>\n",
       "      <td>1100</td>\n",
       "      <td>2151814</td>\n",
       "      <td>5324344</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>March</td>\n",
       "      <td>Winter Storm</td>\n",
       "      <td>Z</td>\n",
       "      <td>7</td>\n",
       "      <td>RICH/CACHE T NW/P E C WEBER/MOST OF MORGAN/NW ...</td>\n",
       "      <td>SLC</td>\n",
       "      <td>13-MAR-02 07:00:00</td>\n",
       "      <td>MST</td>\n",
       "      <td>14-MAR-02 11:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>GENERAL PUBLIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A blast of snow and wind hit much of Utah, pou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDS</td>\n",
       "      <td>2002-03-13 07:00:00</td>\n",
       "      <td>2002-03-14 11:00:00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  EPISODE_ID  EVENT_ID STATE  STATE_FIPS  YEAR MONTH_NAME    EVENT_TYPE CZ_TYPE  CZ_FIPS                                            CZ_NAME  WFO     BEGIN_DATE_TIME CZ_TIMEZONE       END_DATE_TIME  INJURIES_DIRECT  INJURIES_INDIRECT  DEATHS_DIRECT  DEATHS_INDIRECT DAMAGE_PROPERTY DAMAGE_CROPS          SOURCE  MAGNITUDE MAGNITUDE_TYPE FLOOD_CAUSE  CATEGORY TOR_F_SCALE  TOR_LENGTH  TOR_WIDTH TOR_OTHER_WFO TOR_OTHER_CZ_STATE  TOR_OTHER_CZ_FIPS TOR_OTHER_CZ_NAME  BEGIN_RANGE BEGIN_AZIMUTH BEGIN_LOCATION  END_RANGE END_AZIMUTH END_LOCATION  BEGIN_LAT  BEGIN_LON  END_LAT  END_LON                                  EPISODE_NARRATIVE EVENT_NARRATIVE DATA_SOURCE      BEGIN_DATETIME        END_DATETIME  DURATION_HOURS  Distance_km\n",
       "0           200201         15         700         200201       15      1900     2151794   5324157  UTAH          49  2002    January    Heavy Snow       Z        8                     WASATCH MOUNTAINS SOUTH OF I80  SLC  15-JAN-02 07:00:00         MST  15-JAN-02 19:00:00                0                  0              0                0             10K            0  GENERAL PUBLIC        NaN            NaN         NaN       NaN         NaN         NaN        NaN           NaN                NaN                NaN               NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  A cold arctic storm dropped several inches of ...             NaN         PDS 2002-01-15 07:00:00 2002-01-15 19:00:00            12.0          NaN\n",
       "1           200201         15         700         200201       15      1900     2151794   5324158  UTAH          49  2002    January    Heavy Snow       Z        4            SOUTHERN WASATCH FRONT/LEHI/PROVO/NEPHI  SLC  15-JAN-02 07:00:00         MST  15-JAN-02 19:00:00                0                  0              0                0             50K            0  GENERAL PUBLIC        NaN            NaN         NaN       NaN         NaN         NaN        NaN           NaN                NaN                NaN               NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  A cold arctic storm dropped several inches of ...             NaN         PDS 2002-01-15 07:00:00 2002-01-15 19:00:00            12.0          NaN\n",
       "2           200201         18         600         200201       18      1700     2151795   5324159  UTAH          49  2002    January    Heavy Snow       Z        2  NORTHERN WASATCH FRONT/BRIGHAM CITY/OGDEN BOUN...  SLC  18-JAN-02 06:00:00         MST  18-JAN-02 17:00:00                0                  0              0                0               0            0  GENERAL PUBLIC        NaN            NaN         NaN       NaN         NaN         NaN        NaN           NaN                NaN                NaN               NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  Another storm brought good amounts of snow to ...             NaN         PDS 2002-01-18 06:00:00 2002-01-18 17:00:00            11.0          NaN\n",
       "3           200203         13         700         200203       14      1100     2151814   5324343  UTAH          49  2002      March  Winter Storm       Z        6    P NC WEBER/P C MORGAN/P W C SUMMIT/P SC WASATCH  SLC  13-MAR-02 07:00:00         MST  14-MAR-02 11:00:00                0                  0              0                0               0            0  GENERAL PUBLIC        NaN            NaN         NaN       NaN         NaN         NaN        NaN           NaN                NaN                NaN               NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  A blast of snow and wind hit much of Utah, pou...             NaN         PDS 2002-03-13 07:00:00 2002-03-14 11:00:00            28.0          NaN\n",
       "4           200203         13         700         200203       14      1100     2151814   5324344  UTAH          49  2002      March  Winter Storm       Z        7  RICH/CACHE T NW/P E C WEBER/MOST OF MORGAN/NW ...  SLC  13-MAR-02 07:00:00         MST  14-MAR-02 11:00:00                0                  0              0                0               0            0  GENERAL PUBLIC        NaN            NaN         NaN       NaN         NaN         NaN        NaN           NaN                NaN                NaN               NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  A blast of snow and wind hit much of Utah, pou...             NaN         PDS 2002-03-13 07:00:00 2002-03-14 11:00:00            28.0          NaN"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as we did not know how to calculate the distance, this calculation was taken by using Chat GPT\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points on the Earth's surface.\n",
    "    \"\"\"\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    # Radius of the Earth in kilometers\n",
    "    r = 6371  \n",
    "    return c * r  # Distance in kilometers\n",
    "\n",
    "# Calculate distance and add as a new column\n",
    "df['Distance_km'] = df.apply(lambda row: haversine(row['BEGIN_LAT'], row['BEGIN_LON'], row['END_LAT'], row['END_LON']), axis=1)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a99c74ae-d4fb-4375-9925-7c71e8b70086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH_NAME</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "      <th>CZ_TYPE</th>\n",
       "      <th>CZ_FIPS</th>\n",
       "      <th>CZ_NAME</th>\n",
       "      <th>WFO</th>\n",
       "      <th>BEGIN_DATE_TIME</th>\n",
       "      <th>CZ_TIMEZONE</th>\n",
       "      <th>END_DATE_TIME</th>\n",
       "      <th>INJURIES_DIRECT</th>\n",
       "      <th>INJURIES_INDIRECT</th>\n",
       "      <th>DEATHS_DIRECT</th>\n",
       "      <th>DEATHS_INDIRECT</th>\n",
       "      <th>DAMAGE_PROPERTY</th>\n",
       "      <th>DAMAGE_CROPS</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>MAGNITUDE</th>\n",
       "      <th>MAGNITUDE_TYPE</th>\n",
       "      <th>FLOOD_CAUSE</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>TOR_F_SCALE</th>\n",
       "      <th>TOR_LENGTH</th>\n",
       "      <th>TOR_WIDTH</th>\n",
       "      <th>TOR_OTHER_WFO</th>\n",
       "      <th>TOR_OTHER_CZ_STATE</th>\n",
       "      <th>TOR_OTHER_CZ_FIPS</th>\n",
       "      <th>TOR_OTHER_CZ_NAME</th>\n",
       "      <th>BEGIN_RANGE</th>\n",
       "      <th>BEGIN_AZIMUTH</th>\n",
       "      <th>BEGIN_LOCATION</th>\n",
       "      <th>END_RANGE</th>\n",
       "      <th>END_AZIMUTH</th>\n",
       "      <th>END_LOCATION</th>\n",
       "      <th>BEGIN_LAT</th>\n",
       "      <th>BEGIN_LON</th>\n",
       "      <th>END_LAT</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>EPISODE_NARRATIVE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "      <th>DATA_SOURCE</th>\n",
       "      <th>BEGIN_DATETIME</th>\n",
       "      <th>END_DATETIME</th>\n",
       "      <th>DURATION_HOURS</th>\n",
       "      <th>Distance_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200201</td>\n",
       "      <td>15</td>\n",
       "      <td>700</td>\n",
       "      <td>200201</td>\n",
       "      <td>15</td>\n",
       "      <td>1900</td>\n",
       "      <td>2151794</td>\n",
       "      <td>5324157</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>January</td>\n",
       "      <td>Heavy Snow</td>\n",
       "      <td>Z</td>\n",
       "      <td>8</td>\n",
       "      <td>WASATCH MOUNTAINS SOUTH OF I80</td>\n",
       "      <td>SLC</td>\n",
       "      <td>15-JAN-02 07:00:00</td>\n",
       "      <td>MST</td>\n",
       "      <td>15-JAN-02 19:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10K</td>\n",
       "      <td>0</td>\n",
       "      <td>GENERAL PUBLIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A cold arctic storm dropped several inches of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDS</td>\n",
       "      <td>2002-01-15 07:00:00</td>\n",
       "      <td>2002-01-15 19:00:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200201</td>\n",
       "      <td>15</td>\n",
       "      <td>700</td>\n",
       "      <td>200201</td>\n",
       "      <td>15</td>\n",
       "      <td>1900</td>\n",
       "      <td>2151794</td>\n",
       "      <td>5324158</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>January</td>\n",
       "      <td>Heavy Snow</td>\n",
       "      <td>Z</td>\n",
       "      <td>4</td>\n",
       "      <td>SOUTHERN WASATCH FRONT/LEHI/PROVO/NEPHI</td>\n",
       "      <td>SLC</td>\n",
       "      <td>15-JAN-02 07:00:00</td>\n",
       "      <td>MST</td>\n",
       "      <td>15-JAN-02 19:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50K</td>\n",
       "      <td>0</td>\n",
       "      <td>GENERAL PUBLIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A cold arctic storm dropped several inches of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDS</td>\n",
       "      <td>2002-01-15 07:00:00</td>\n",
       "      <td>2002-01-15 19:00:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200201</td>\n",
       "      <td>18</td>\n",
       "      <td>600</td>\n",
       "      <td>200201</td>\n",
       "      <td>18</td>\n",
       "      <td>1700</td>\n",
       "      <td>2151795</td>\n",
       "      <td>5324159</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>January</td>\n",
       "      <td>Heavy Snow</td>\n",
       "      <td>Z</td>\n",
       "      <td>2</td>\n",
       "      <td>NORTHERN WASATCH FRONT/BRIGHAM CITY/OGDEN BOUN...</td>\n",
       "      <td>SLC</td>\n",
       "      <td>18-JAN-02 06:00:00</td>\n",
       "      <td>MST</td>\n",
       "      <td>18-JAN-02 17:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>GENERAL PUBLIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Another storm brought good amounts of snow to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDS</td>\n",
       "      <td>2002-01-18 06:00:00</td>\n",
       "      <td>2002-01-18 17:00:00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200203</td>\n",
       "      <td>13</td>\n",
       "      <td>700</td>\n",
       "      <td>200203</td>\n",
       "      <td>14</td>\n",
       "      <td>1100</td>\n",
       "      <td>2151814</td>\n",
       "      <td>5324343</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>March</td>\n",
       "      <td>Winter Storm</td>\n",
       "      <td>Z</td>\n",
       "      <td>6</td>\n",
       "      <td>P NC WEBER/P C MORGAN/P W C SUMMIT/P SC WASATCH</td>\n",
       "      <td>SLC</td>\n",
       "      <td>13-MAR-02 07:00:00</td>\n",
       "      <td>MST</td>\n",
       "      <td>14-MAR-02 11:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>GENERAL PUBLIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A blast of snow and wind hit much of Utah, pou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDS</td>\n",
       "      <td>2002-03-13 07:00:00</td>\n",
       "      <td>2002-03-14 11:00:00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200203</td>\n",
       "      <td>13</td>\n",
       "      <td>700</td>\n",
       "      <td>200203</td>\n",
       "      <td>14</td>\n",
       "      <td>1100</td>\n",
       "      <td>2151814</td>\n",
       "      <td>5324344</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>March</td>\n",
       "      <td>Winter Storm</td>\n",
       "      <td>Z</td>\n",
       "      <td>7</td>\n",
       "      <td>RICH/CACHE T NW/P E C WEBER/MOST OF MORGAN/NW ...</td>\n",
       "      <td>SLC</td>\n",
       "      <td>13-MAR-02 07:00:00</td>\n",
       "      <td>MST</td>\n",
       "      <td>14-MAR-02 11:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>GENERAL PUBLIC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A blast of snow and wind hit much of Utah, pou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDS</td>\n",
       "      <td>2002-03-13 07:00:00</td>\n",
       "      <td>2002-03-14 11:00:00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  EPISODE_ID  EVENT_ID STATE  STATE_FIPS  YEAR MONTH_NAME    EVENT_TYPE CZ_TYPE  CZ_FIPS                                            CZ_NAME  WFO     BEGIN_DATE_TIME CZ_TIMEZONE       END_DATE_TIME  INJURIES_DIRECT  INJURIES_INDIRECT  DEATHS_DIRECT  DEATHS_INDIRECT DAMAGE_PROPERTY DAMAGE_CROPS          SOURCE  MAGNITUDE MAGNITUDE_TYPE FLOOD_CAUSE  CATEGORY TOR_F_SCALE  TOR_LENGTH  TOR_WIDTH TOR_OTHER_WFO TOR_OTHER_CZ_STATE  TOR_OTHER_CZ_FIPS TOR_OTHER_CZ_NAME  BEGIN_RANGE BEGIN_AZIMUTH BEGIN_LOCATION  END_RANGE END_AZIMUTH END_LOCATION  BEGIN_LAT  BEGIN_LON  END_LAT  END_LON                                  EPISODE_NARRATIVE EVENT_NARRATIVE DATA_SOURCE      BEGIN_DATETIME        END_DATETIME  DURATION_HOURS  Distance_km\n",
       "0           200201         15         700         200201       15      1900     2151794   5324157  UTAH          49  2002    January    Heavy Snow       Z        8                     WASATCH MOUNTAINS SOUTH OF I80  SLC  15-JAN-02 07:00:00         MST  15-JAN-02 19:00:00                0                  0              0                0             10K            0  GENERAL PUBLIC        NaN            NaN         NaN       NaN         NaN         NaN        NaN           NaN                NaN                NaN               NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  A cold arctic storm dropped several inches of ...             NaN         PDS 2002-01-15 07:00:00 2002-01-15 19:00:00            12.0          NaN\n",
       "1           200201         15         700         200201       15      1900     2151794   5324158  UTAH          49  2002    January    Heavy Snow       Z        4            SOUTHERN WASATCH FRONT/LEHI/PROVO/NEPHI  SLC  15-JAN-02 07:00:00         MST  15-JAN-02 19:00:00                0                  0              0                0             50K            0  GENERAL PUBLIC        NaN            NaN         NaN       NaN         NaN         NaN        NaN           NaN                NaN                NaN               NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  A cold arctic storm dropped several inches of ...             NaN         PDS 2002-01-15 07:00:00 2002-01-15 19:00:00            12.0          NaN\n",
       "2           200201         18         600         200201       18      1700     2151795   5324159  UTAH          49  2002    January    Heavy Snow       Z        2  NORTHERN WASATCH FRONT/BRIGHAM CITY/OGDEN BOUN...  SLC  18-JAN-02 06:00:00         MST  18-JAN-02 17:00:00                0                  0              0                0               0            0  GENERAL PUBLIC        NaN            NaN         NaN       NaN         NaN         NaN        NaN           NaN                NaN                NaN               NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  Another storm brought good amounts of snow to ...             NaN         PDS 2002-01-18 06:00:00 2002-01-18 17:00:00            11.0          NaN\n",
       "3           200203         13         700         200203       14      1100     2151814   5324343  UTAH          49  2002      March  Winter Storm       Z        6    P NC WEBER/P C MORGAN/P W C SUMMIT/P SC WASATCH  SLC  13-MAR-02 07:00:00         MST  14-MAR-02 11:00:00                0                  0              0                0               0            0  GENERAL PUBLIC        NaN            NaN         NaN       NaN         NaN         NaN        NaN           NaN                NaN                NaN               NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  A blast of snow and wind hit much of Utah, pou...             NaN         PDS 2002-03-13 07:00:00 2002-03-14 11:00:00            28.0          NaN\n",
       "4           200203         13         700         200203       14      1100     2151814   5324344  UTAH          49  2002      March  Winter Storm       Z        7  RICH/CACHE T NW/P E C WEBER/MOST OF MORGAN/NW ...  SLC  13-MAR-02 07:00:00         MST  14-MAR-02 11:00:00                0                  0              0                0               0            0  GENERAL PUBLIC        NaN            NaN         NaN       NaN         NaN         NaN        NaN           NaN                NaN                NaN               NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  A blast of snow and wind hit much of Utah, pou...             NaN         PDS 2002-03-13 07:00:00 2002-03-14 11:00:00            28.0          NaN"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4b1684a9-cddf-4de7-bfb1-eaab1e551d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64\n"
     ]
    }
   ],
   "source": [
    "df['GEOID'] = (\n",
    "    df['STATE_FIPS'].astype(str).str.zfill(2) +  \n",
    "    df['CZ_FIPS'].astype(str).str.zfill(3))\n",
    "df['GEOID'] = df['GEOID'].astype('Int64')\n",
    "print(df['GEOID'].dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c07077-f29e-47cc-9c91-f33dc8dd4d7c",
   "metadata": {},
   "source": [
    "Now we can drop all the columns that are of no interest to our prediction. These would be: <br>\n",
    "- EPISODE_ID\n",
    "- EVENT_ID\n",
    "- WFO\n",
    "- CZ_TIMEZONE\n",
    "- SOURCE\n",
    "- CATEGORY\n",
    "- TOR_OTHER_CZ_STATE\n",
    "- TOR_OTHER_CZ_NAME\n",
    "- DATA_SOURCE\n",
    "- TOR_OTHER_CZ_FIPS\n",
    "- DAMAGE_CROPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a1abaa86-4b77-4d77-bc2a-283fb51f6a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "# We drop all the columns that are not useful for our predictiom\n",
    "columns_to_drop = [\n",
    "    \"EPISODE_ID\", # describes just the identification number of the Episode, has no predictive utility\n",
    "    \"EVENT_ID\", # describes just the identification of the Event, has no predictive utility\n",
    "    \"WFO\", # National Weather Service Forecast Office’s area of responsibility, has no predictive utiltiy\n",
    "    \"CZ_TIMEZONE\", # Describes the predictive power of the County or zone, has no predictive utility\n",
    "    \"SOURCE\", # describes the soruce of the information, has no predictive utility\n",
    "    \"CATEGORY\", # Unknown (During the time of downloading this particular file, NCDC has never seen anything provided within this field.)-> copy pasted from the explanation of the variabes by the source, has no explanatory utility\n",
    "    \"TOR_OTHER_CZ_STATE\", # shows if the tornado went from one state to another -> just tells to which state (State-code), has no explanatory utility\n",
    "    \"TOR_OTHER_CZ_NAME\", # shows if the tornado went from one state to another-> just tells to which state (name), has no explanatory utility\n",
    "    \"DATA_SOURCE\", # shows the soruce of the infromation of the data, has no explanatory utility\n",
    "    \"TOR_OTHER_WFO\", # as for the normal WFO, this does not interest us\n",
    "    \"TOR_OTHER_CZ_FIPS\", # FIPS code of the county or zone where the Tornado moved, has no explanatory utily\n",
    "    \"DAMAGE_CROPS\", # We are only interested on the damages on the crops\n",
    "]\n",
    "\n",
    "# Drop the columns\n",
    "df = df.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ebfd4f-01ee-4105-957d-a7fe3f4fd084",
   "metadata": {},
   "source": [
    "From the Master dataset we are only interested in the following Natural Hazards:<br>\n",
    "- Avalanche <br>\n",
    "- Blizzards <br>\n",
    "- Coastal Flood<br>\n",
    "- Debris Flood<br>\n",
    "- Flash Flood<br>\n",
    "- Flood<br>\n",
    "- Hail<br>\n",
    "- High Surf <br>\n",
    "- High Wind<br>\n",
    "- Hurricane<br>\n",
    "- Lakeshore Flood<br>\n",
    "- Lightning<br>\n",
    "- Seiche <br>\n",
    "- Storm Surge/Tide <br>\n",
    "- Strong wind <br>\n",
    "- Thunderstorm Wind <br>\n",
    "- Tornado<br>\n",
    "- Tropical Depression<br>\n",
    "- Tropical storm<br>\n",
    "- Tsunami<br>\n",
    "- Wildfire<br>\n",
    "- Winter storm<br>\n",
    "\n",
    "These are the dataset of natural hazards which are interesting for our purpose of predicting damages. There are other types of Natural hazards, but we only selected the previously shown Natural Hazards. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822f1ee2-b388-46ec-87ef-0542ee95bda3",
   "metadata": {},
   "source": [
    "We know generate sub-datasets based on this which will be used for each inividual prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a63f4de-c748-49cd-a7dc-51dd08f38ada",
   "metadata": {},
   "source": [
    "Now we want to take out all the observations that have no cost assigned to the Natural Hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6e0c01cf-d954-45c1-ab44-f05b62c54055",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['DAMAGE_PROPERTY']) # if there are NA in our \"y\" we cannot use it, thus we dropped all the NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "daa4166c-cfab-4f46-8096-65a3b115af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By looking at the data we noticed that the valeus are not represente in full numeric values but take a k which represents thousands and m if they are millions \n",
    "def convert_cost(value):\n",
    "    try:\n",
    "        value = value.lower()\n",
    "        if 'k' in value:\n",
    "            return float(value.replace('k', '')) * 1000\n",
    "        elif 'm' in value:\n",
    "            return float(value.replace('m', '')) * 1000000\n",
    "        else:\n",
    "            return float(value)\n",
    "    except Exception:\n",
    "        return None  # Or any placeholder for invalid values\n",
    "df['DAMAGE_PROPERTY'] = df['DAMAGE_PROPERTY'].apply(convert_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "18c51bff-4cf0-4066-82f2-8987e5c8cf13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH_NAME</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "      <th>CZ_TYPE</th>\n",
       "      <th>CZ_FIPS</th>\n",
       "      <th>CZ_NAME</th>\n",
       "      <th>BEGIN_DATE_TIME</th>\n",
       "      <th>END_DATE_TIME</th>\n",
       "      <th>INJURIES_DIRECT</th>\n",
       "      <th>INJURIES_INDIRECT</th>\n",
       "      <th>DEATHS_DIRECT</th>\n",
       "      <th>DEATHS_INDIRECT</th>\n",
       "      <th>DAMAGE_PROPERTY</th>\n",
       "      <th>MAGNITUDE</th>\n",
       "      <th>MAGNITUDE_TYPE</th>\n",
       "      <th>FLOOD_CAUSE</th>\n",
       "      <th>TOR_F_SCALE</th>\n",
       "      <th>TOR_LENGTH</th>\n",
       "      <th>TOR_WIDTH</th>\n",
       "      <th>BEGIN_RANGE</th>\n",
       "      <th>BEGIN_AZIMUTH</th>\n",
       "      <th>BEGIN_LOCATION</th>\n",
       "      <th>END_RANGE</th>\n",
       "      <th>END_AZIMUTH</th>\n",
       "      <th>END_LOCATION</th>\n",
       "      <th>BEGIN_LAT</th>\n",
       "      <th>BEGIN_LON</th>\n",
       "      <th>END_LAT</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>EPISODE_NARRATIVE</th>\n",
       "      <th>EVENT_NARRATIVE</th>\n",
       "      <th>BEGIN_DATETIME</th>\n",
       "      <th>END_DATETIME</th>\n",
       "      <th>DURATION_HOURS</th>\n",
       "      <th>Distance_km</th>\n",
       "      <th>GEOID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200201</td>\n",
       "      <td>15</td>\n",
       "      <td>700</td>\n",
       "      <td>200201</td>\n",
       "      <td>15</td>\n",
       "      <td>1900</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>January</td>\n",
       "      <td>Heavy Snow</td>\n",
       "      <td>Z</td>\n",
       "      <td>8</td>\n",
       "      <td>WASATCH MOUNTAINS SOUTH OF I80</td>\n",
       "      <td>15-JAN-02 07:00:00</td>\n",
       "      <td>15-JAN-02 19:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A cold arctic storm dropped several inches of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002-01-15 07:00:00</td>\n",
       "      <td>2002-01-15 19:00:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200201</td>\n",
       "      <td>15</td>\n",
       "      <td>700</td>\n",
       "      <td>200201</td>\n",
       "      <td>15</td>\n",
       "      <td>1900</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>January</td>\n",
       "      <td>Heavy Snow</td>\n",
       "      <td>Z</td>\n",
       "      <td>4</td>\n",
       "      <td>SOUTHERN WASATCH FRONT/LEHI/PROVO/NEPHI</td>\n",
       "      <td>15-JAN-02 07:00:00</td>\n",
       "      <td>15-JAN-02 19:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A cold arctic storm dropped several inches of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002-01-15 07:00:00</td>\n",
       "      <td>2002-01-15 19:00:00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200201</td>\n",
       "      <td>18</td>\n",
       "      <td>600</td>\n",
       "      <td>200201</td>\n",
       "      <td>18</td>\n",
       "      <td>1700</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>January</td>\n",
       "      <td>Heavy Snow</td>\n",
       "      <td>Z</td>\n",
       "      <td>2</td>\n",
       "      <td>NORTHERN WASATCH FRONT/BRIGHAM CITY/OGDEN BOUN...</td>\n",
       "      <td>18-JAN-02 06:00:00</td>\n",
       "      <td>18-JAN-02 17:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Another storm brought good amounts of snow to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002-01-18 06:00:00</td>\n",
       "      <td>2002-01-18 17:00:00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200203</td>\n",
       "      <td>13</td>\n",
       "      <td>700</td>\n",
       "      <td>200203</td>\n",
       "      <td>14</td>\n",
       "      <td>1100</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>March</td>\n",
       "      <td>Winter Storm</td>\n",
       "      <td>Z</td>\n",
       "      <td>6</td>\n",
       "      <td>P NC WEBER/P C MORGAN/P W C SUMMIT/P SC WASATCH</td>\n",
       "      <td>13-MAR-02 07:00:00</td>\n",
       "      <td>14-MAR-02 11:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A blast of snow and wind hit much of Utah, pou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002-03-13 07:00:00</td>\n",
       "      <td>2002-03-14 11:00:00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200203</td>\n",
       "      <td>13</td>\n",
       "      <td>700</td>\n",
       "      <td>200203</td>\n",
       "      <td>14</td>\n",
       "      <td>1100</td>\n",
       "      <td>UTAH</td>\n",
       "      <td>49</td>\n",
       "      <td>2002</td>\n",
       "      <td>March</td>\n",
       "      <td>Winter Storm</td>\n",
       "      <td>Z</td>\n",
       "      <td>7</td>\n",
       "      <td>RICH/CACHE T NW/P E C WEBER/MOST OF MORGAN/NW ...</td>\n",
       "      <td>13-MAR-02 07:00:00</td>\n",
       "      <td>14-MAR-02 11:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A blast of snow and wind hit much of Utah, pou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2002-03-13 07:00:00</td>\n",
       "      <td>2002-03-14 11:00:00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME STATE  STATE_FIPS  YEAR MONTH_NAME    EVENT_TYPE CZ_TYPE  CZ_FIPS                                            CZ_NAME     BEGIN_DATE_TIME       END_DATE_TIME  INJURIES_DIRECT  INJURIES_INDIRECT  DEATHS_DIRECT  DEATHS_INDIRECT  DAMAGE_PROPERTY  MAGNITUDE MAGNITUDE_TYPE FLOOD_CAUSE TOR_F_SCALE  TOR_LENGTH  TOR_WIDTH  BEGIN_RANGE BEGIN_AZIMUTH BEGIN_LOCATION  END_RANGE END_AZIMUTH END_LOCATION  BEGIN_LAT  BEGIN_LON  END_LAT  END_LON                                  EPISODE_NARRATIVE EVENT_NARRATIVE      BEGIN_DATETIME        END_DATETIME  DURATION_HOURS  Distance_km  GEOID\n",
       "0           200201         15         700         200201       15      1900  UTAH          49  2002    January    Heavy Snow       Z        8                     WASATCH MOUNTAINS SOUTH OF I80  15-JAN-02 07:00:00  15-JAN-02 19:00:00                0                  0              0                0          10000.0        NaN            NaN         NaN         NaN         NaN        NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  A cold arctic storm dropped several inches of ...             NaN 2002-01-15 07:00:00 2002-01-15 19:00:00            12.0          NaN  49008\n",
       "1           200201         15         700         200201       15      1900  UTAH          49  2002    January    Heavy Snow       Z        4            SOUTHERN WASATCH FRONT/LEHI/PROVO/NEPHI  15-JAN-02 07:00:00  15-JAN-02 19:00:00                0                  0              0                0          50000.0        NaN            NaN         NaN         NaN         NaN        NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  A cold arctic storm dropped several inches of ...             NaN 2002-01-15 07:00:00 2002-01-15 19:00:00            12.0          NaN  49004\n",
       "2           200201         18         600         200201       18      1700  UTAH          49  2002    January    Heavy Snow       Z        2  NORTHERN WASATCH FRONT/BRIGHAM CITY/OGDEN BOUN...  18-JAN-02 06:00:00  18-JAN-02 17:00:00                0                  0              0                0              0.0        NaN            NaN         NaN         NaN         NaN        NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  Another storm brought good amounts of snow to ...             NaN 2002-01-18 06:00:00 2002-01-18 17:00:00            11.0          NaN  49002\n",
       "3           200203         13         700         200203       14      1100  UTAH          49  2002      March  Winter Storm       Z        6    P NC WEBER/P C MORGAN/P W C SUMMIT/P SC WASATCH  13-MAR-02 07:00:00  14-MAR-02 11:00:00                0                  0              0                0              0.0        NaN            NaN         NaN         NaN         NaN        NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  A blast of snow and wind hit much of Utah, pou...             NaN 2002-03-13 07:00:00 2002-03-14 11:00:00            28.0          NaN  49006\n",
       "4           200203         13         700         200203       14      1100  UTAH          49  2002      March  Winter Storm       Z        7  RICH/CACHE T NW/P E C WEBER/MOST OF MORGAN/NW ...  13-MAR-02 07:00:00  14-MAR-02 11:00:00                0                  0              0                0              0.0        NaN            NaN         NaN         NaN         NaN        NaN          NaN           NaN            NaN        NaN         NaN          NaN        NaN        NaN      NaN      NaN  A blast of snow and wind hit much of Utah, pou...             NaN 2002-03-13 07:00:00 2002-03-14 11:00:00            28.0          NaN  49007"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ec46af38-f049-435d-9af7-c966474b4e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"YEAR\": \"Year\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "712031e9-0503-4818-ada1-613cd25411c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"STATE\": \"State\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "108acb3c-edf2-41f1-969e-a9be4e760440",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['State'] = df['State'].str.title()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "322e7f00-ad14-4b7e-b9de-6c5ec2a5d2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>State</th>\n",
       "      <th>Year</th>\n",
       "      <th>Density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>2001</td>\n",
       "      <td>34.055836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Alaska</td>\n",
       "      <td>2001</td>\n",
       "      <td>0.428327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>2001</td>\n",
       "      <td>17.914687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>2001</td>\n",
       "      <td>19.988123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>California</td>\n",
       "      <td>2001</td>\n",
       "      <td>85.414236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATE       State  Year    Density\n",
       "0      1     Alabama  2001  34.055836\n",
       "1      2      Alaska  2001   0.428327\n",
       "2      4     Arizona  2001  17.914687\n",
       "3      5    Arkansas  2001  19.988123\n",
       "4      6  California  2001  85.414236"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "density_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5beead78-883e-47b8-babd-3dbc1000a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalanche\n",
    "avalanche= df[df['EVENT_TYPE'] == 'Avalanche'] # has zones\n",
    "\n",
    "# Blizzards\n",
    "blizzards = df[df['EVENT_TYPE'] == 'Blizzards'] # has zones\n",
    "\n",
    "# Coastal Flood\n",
    "coastal_flood = df[df['EVENT_TYPE'] == 'Coastal Flood'] # has zones\n",
    "\n",
    "# Debris Flood\n",
    "debris_flood = df[df['EVENT_TYPE'] == 'Debris Flood'] # has counties\n",
    "\n",
    "# Flash Flood\n",
    "flash_flood = df[df['EVENT_TYPE'] == 'Flash Flood'] # has counties\n",
    "\n",
    "# Flood\n",
    "flood = df[df['EVENT_TYPE'] == 'Flood'] # has counties\n",
    "\n",
    "# Hail\n",
    "hail = df[df['EVENT_TYPE'] == 'Hail'] # has counties\n",
    "\n",
    "# High Surf\n",
    "high_surf = df[df['EVENT_TYPE'] == 'High Surf'] # has zones\n",
    "\n",
    "# High Wind\n",
    "high_wind = df[df['EVENT_TYPE'] == 'High Wind'] # has zones\n",
    "\n",
    "# Hurricane\n",
    "hurricane = df[df['EVENT_TYPE'] == 'Hurricane'] # has zones \n",
    "\n",
    "# Lakeshore Flood\n",
    "lakeshore_flood = df[df['EVENT_TYPE'] == 'Lakeshore Flood'] # has zone\n",
    "\n",
    "# Lightning\n",
    "lightning = df[df['EVENT_TYPE'] == 'Lightning'] # has counties\n",
    "\n",
    "# Seiche\n",
    "seiche = df[df['EVENT_TYPE'] == 'Seiche'] # has zones \n",
    "\n",
    "# Storm Surge/Tide\n",
    "storm_surge_tide = df[df['EVENT_TYPE'] == 'Storm Surge/Tide'] # has zones \n",
    "\n",
    "# Strong Wind\n",
    "strong_wind = df[df['EVENT_TYPE'] == 'Strong wind'] # has zones \n",
    "\n",
    "# Thunderstorm Wind\n",
    "thunderstorm_wind = df[df['EVENT_TYPE'] == 'Thunderstorm Wind'] # has counties\n",
    "\n",
    "# Tornado\n",
    "tornado = df[df['EVENT_TYPE'] == 'Tornado'] # has counties\n",
    "\n",
    "# Tropical Depression\n",
    "tropical_depression = df[df['EVENT_TYPE'] == 'Tropical Depression'] # has zones\n",
    "\n",
    "# Tropical Storm\n",
    "tropical_storm = df[df['EVENT_TYPE'] == 'Tropical storm'] # has zones\n",
    "\n",
    "# Tsunami\n",
    "tsunami = df[df['EVENT_TYPE'] == 'Tsunami'] # has zones\n",
    "\n",
    "# Wildfire\n",
    "wildfire = df[df['EVENT_TYPE'] == 'Wildfire'] # has zones \n",
    "\n",
    "# Winter Storm\n",
    "winter_storm = df[df['EVENT_TYPE'] == 'Winter storm'] # has zones \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b692aeed-7113-4c8f-a7ea-11024a705372",
   "metadata": {},
   "source": [
    "We are now interested to see how many observations each Natural Hazard has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6f8b8ab2-5537-4507-bbcb-4875b4d13caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n"
     ]
    }
   ],
   "source": [
    "print(len(avalanche))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "083fdd94-439b-4f31-87b5-2d80efcd1cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(blizzards)) # has 0 obs so we can leave it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5feba9be-561a-4331-9cf2-52f7e783211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2768\n"
     ]
    }
   ],
   "source": [
    "print(len(coastal_flood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c3caa35e-259b-4943-81b6-7ad1dd77dfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(debris_flood)) # 0 obs so we can leave it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2897cf1f-e2e2-4397-8125-f63dc68f010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74681\n"
     ]
    }
   ],
   "source": [
    "print(len(flash_flood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "52928ad5-07b2-4de0-a0e5-c5c4f7d60488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49246\n"
     ]
    }
   ],
   "source": [
    "print(len(flood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f76dc456-d663-42da-bd92-1b4279f9f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156023\n"
     ]
    }
   ],
   "source": [
    "print(len(hail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b788bd57-9474-4898-b8ca-59942899a43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7371\n"
     ]
    }
   ],
   "source": [
    "print(len(high_surf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "624fc88a-30eb-4c53-9613-d18bc97913cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55320\n"
     ]
    }
   ],
   "source": [
    "print(len(high_wind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "abe468a5-185c-4b38-865d-570379cddcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "print(len(hurricane))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b8bcf101-3318-45d9-9eeb-d948bb1099d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335\n"
     ]
    }
   ],
   "source": [
    "print(len(lakeshore_flood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f1b9f0c1-4348-4be7-a435-7f5ec452a6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10838\n"
     ]
    }
   ],
   "source": [
    "print(len(lightning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "af2f2f84-8a0d-48e3-a8ac-7d96988f3b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "print(len(seiche)) # not many observations so might be better to leave out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "bd117c1e-7240-43ea-8eb9-4e364b281a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875\n"
     ]
    }
   ],
   "source": [
    "print(len(storm_surge_tide))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fd09e060-2244-4fa6-bd05-4e4175a9ffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(strong_wind)) # 0 obs so we can leave it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "30e5a0cb-5b55-438d-9559-ef3750fdbfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270027\n"
     ]
    }
   ],
   "source": [
    "print(len(thunderstorm_wind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "094ab316-0274-4d5d-87d9-e82061ca3a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25704\n"
     ]
    }
   ],
   "source": [
    "print(len(tornado))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ad760d5e-8bfc-46e4-8b7b-3d7e17c40a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443\n"
     ]
    }
   ],
   "source": [
    "print(len(tropical_depression))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "184a7580-6d12-4947-a6c0-7c36b1abceb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(tropical_storm)) # 0 obs so we can leave it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "f860da91-d87b-404c-a70e-50a7a2795352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    }
   ],
   "source": [
    "print(len(tsunami)) # not many obs, we can try to predict it just bc it is cool but model will be probably very imprecise :(\n",
    "# P.S. betetr that there are not many obs-> we do not want many tsunamis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "69e13b38-5b16-43a5-b99e-03b34821e4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5504\n"
     ]
    }
   ],
   "source": [
    "print(len(wildfire))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "55a9af58-1b0e-4b19-a833-20bb60f3f6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(winter_storm)) # 0 obs so we can leave it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "00c9e8cb-8e51-4764-8f83-7f3045a01073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now depending if the datasets contain zones or county level data we merge it with the respective data of GDP per capita and population density\n",
    "\n",
    "# Flash FLood\n",
    "flash_flood = pd.merge(flash_flood, GDP_capita_county, on=[\"GEOID\", \"Year\"], how='inner')\n",
    "flash_flood = pd.merge(flash_flood, density_county, on=[\"GEOID\", \"Year\"], how='inner')\n",
    "flash_flood = flash_flood.drop(columns=['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH','END_DAY', 'END_TIME', 'STATE_FIPS', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE', 'CZ_FIPS', 'BEGIN_DATE_TIME', 'END_DATE_TIME',  'BEGIN_RANGE','BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH', 'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'BEGIN_DATETIME', 'END_DATETIME', 'STATE_x', 'STATE_y', 'County_y', 'CZ_NAME', 'MAGNITUDE', 'MAGNITUDE_TYPE', 'TOR_F_SCALE', 'TOR_LENGTH', 'TOR_WIDTH', 'EPISODE_NARRATIVE', 'EVENT_NARRATIVE'])\n",
    "flash_flood.rename(columns={\"County_x\": \"County\"}, inplace=True)\n",
    "\n",
    "\n",
    "# FLood\n",
    "flood = pd.merge(flood, GDP_capita_county, on=[\"GEOID\", \"Year\"], how='inner')\n",
    "flood = pd.merge(flood, density_county, on=[\"GEOID\", \"Year\"], how='inner')\n",
    "flood = flood.drop(columns=['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH','END_DAY', 'END_TIME', 'STATE_FIPS', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE', 'CZ_FIPS', 'BEGIN_DATE_TIME', 'END_DATE_TIME',  'BEGIN_RANGE','BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH', 'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'BEGIN_DATETIME', 'END_DATETIME', 'STATE_x', 'STATE_y', 'County_y', 'CZ_NAME', 'MAGNITUDE', 'MAGNITUDE_TYPE', 'TOR_F_SCALE', 'TOR_LENGTH', 'TOR_WIDTH', 'EPISODE_NARRATIVE', 'EVENT_NARRATIVE'])\n",
    "flood.rename(columns={\"County_x\": \"County\"}, inplace=True)\n",
    "\n",
    "\n",
    "# High Wind\n",
    "high_wind = pd.merge(high_wind, GDP_capita_state, on=[\"State\", \"Year\"], how='inner')\n",
    "high_wind = pd.merge(high_wind, density_state, on=[\"State\", \"Year\"], how='inner')\n",
    "high_wind = high_wind.drop(columns=['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH','END_DAY', 'END_TIME', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE', 'CZ_FIPS', 'CZ_NAME','BEGIN_DATE_TIME', 'END_DATE_TIME', 'BEGIN_RANGE', 'BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH', 'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'BEGIN_DATETIME', 'END_DATETIME', 'STATE_x', 'STATE_y', 'STATE_FIPS', 'FLOOD_CAUSE', 'TOR_F_SCALE', 'TOR_LENGTH', 'TOR_WIDTH', 'EPISODE_NARRATIVE', 'EVENT_NARRATIVE','Distance_km'])\n",
    "\n",
    "\n",
    "# Lightning\n",
    "lightning = pd.merge(lightning, GDP_capita_county, on=[\"GEOID\", \"Year\"], how='inner')\n",
    "lightning = pd.merge(lightning, density_county, on=[\"GEOID\", \"Year\"], how='inner')\n",
    "lightning = lightning.drop(columns=['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH','END_DAY', 'END_TIME', 'STATE_FIPS', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE', 'CZ_FIPS', 'BEGIN_DATE_TIME', 'END_DATE_TIME',  'BEGIN_RANGE','BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH', 'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'BEGIN_DATETIME', 'END_DATETIME', 'STATE_x', 'STATE_y', 'County_y', 'CZ_NAME', 'MAGNITUDE', 'MAGNITUDE_TYPE', 'FLOOD_CAUSE', 'TOR_F_SCALE', 'TOR_LENGTH', 'TOR_WIDTH', 'EPISODE_NARRATIVE', 'EVENT_NARRATIVE'])\n",
    "lightning.rename(columns={\"County_x\": \"County\"}, inplace=True)\n",
    "\n",
    "# Seiche\n",
    "seiche = pd.merge(seiche, GDP_capita_state, on=[\"State\", \"Year\"], how='inner')\n",
    "seiche = pd.merge(seiche, density_state, on=[\"State\", \"Year\"], how='inner')\n",
    "seiche = seiche.drop(columns=['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH','END_DAY', 'END_TIME','STATE_x', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE', 'BEGIN_DATE_TIME', 'END_DATE_TIME', 'BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH', 'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'BEGIN_DATETIME', 'END_DATETIME', 'STATE_y', 'State', 'STATE_FIPS', 'CZ_FIPS', 'CZ_NAME', 'MAGNITUDE', 'MAGNITUDE_TYPE', 'FLOOD_CAUSE', 'TOR_F_SCALE' , 'TOR_LENGTH', 'TOR_WIDTH', 'BEGIN_RANGE', 'EPISODE_NARRATIVE', 'EVENT_NARRATIVE', 'Distance_km'])\n",
    "\n",
    "# Tropical Depression\n",
    "tropical_depression = pd.merge(tropical_depression, GDP_capita_state, on=[\"State\", \"Year\"], how='inner')\n",
    "tropical_depression = pd.merge(tropical_depression, density_state, on=[\"State\", \"Year\"], how='inner')\n",
    "tropical_depression = tropical_depression.drop(columns=['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH','END_DAY', 'END_TIME', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE', 'CZ_FIPS', 'CZ_NAME','BEGIN_DATE_TIME', 'END_DATE_TIME', 'BEGIN_RANGE', 'BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH', 'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'BEGIN_DATETIME', 'END_DATETIME', 'STATE_x', 'STATE_y', 'MAGNITUDE', 'MAGNITUDE_TYPE', 'FLOOD_CAUSE', 'TOR_F_SCALE', 'TOR_LENGTH', 'TOR_WIDTH', 'EPISODE_NARRATIVE', 'EVENT_NARRATIVE', 'Distance_km'])\n",
    "\n",
    "\n",
    "# Thunderstorm Wind\n",
    "thunderstorm_wind = pd.merge(thunderstorm_wind, GDP_capita_county, on=[\"GEOID\", \"Year\"], how='inner')\n",
    "thunderstorm_wind = pd.merge(thunderstorm_wind, density_county, on=[\"GEOID\", \"Year\"], how='inner')\n",
    "thunderstorm_wind = thunderstorm_wind.drop(columns=['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH','END_DAY', 'END_TIME', 'STATE_FIPS', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE', 'CZ_FIPS', 'BEGIN_DATE_TIME', 'END_DATE_TIME',  'BEGIN_RANGE','BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH', 'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'BEGIN_DATETIME', 'END_DATETIME', 'STATE_x', 'STATE_y', 'County_y', 'CZ_NAME', 'FLOOD_CAUSE', 'TOR_F_SCALE', 'TOR_LENGTH', 'TOR_WIDTH', 'EPISODE_NARRATIVE', 'EVENT_NARRATIVE', 'STATE_FIPS'])\n",
    "thunderstorm_wind.rename(columns={\"County_x\": \"County\"}, inplace=True)\n",
    "\n",
    "\n",
    "# Tornando\n",
    "tornado = pd.merge(tornado, GDP_capita_county, on=[\"GEOID\", \"Year\"], how='inner')\n",
    "tornado = pd.merge(tornado, density_county, on=[\"GEOID\", \"Year\"], how='inner')\n",
    "tornado = tornado.drop(columns=['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH','END_DAY', 'END_TIME', 'STATE_FIPS', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE', 'CZ_FIPS', 'BEGIN_DATE_TIME', 'END_DATE_TIME',  'BEGIN_RANGE','BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH', 'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'BEGIN_DATETIME', 'END_DATETIME', 'STATE_x', 'STATE_y', 'County_y', 'CZ_NAME', 'MAGNITUDE', 'MAGNITUDE_TYPE', 'FLOOD_CAUSE', 'EPISODE_NARRATIVE', 'EVENT_NARRATIVE'])\n",
    "tornado.rename(columns={\"County_x\": \"County\"}, inplace=True)\n",
    "\n",
    "# Wildfire\n",
    "wildfire = pd.merge(wildfire, GDP_capita_state, on=[\"State\", \"Year\"], how='inner')\n",
    "wildfire = pd.merge(wildfire, density_state, on=[\"State\", \"Year\"], how='inner')\n",
    "wildfire = wildfire.drop(columns=['BEGIN_YEARMONTH', 'BEGIN_DAY', 'BEGIN_TIME', 'END_YEARMONTH','END_DAY', 'END_TIME', 'MONTH_NAME', 'EVENT_TYPE', 'CZ_TYPE', 'CZ_FIPS', 'CZ_NAME','BEGIN_DATE_TIME', 'END_DATE_TIME', 'BEGIN_RANGE', 'BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH', 'END_LOCATION', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'BEGIN_DATETIME', 'END_DATETIME', 'STATE_x', 'STATE_y', 'STATE_FIPS', 'MAGNITUDE', 'MAGNITUDE_TYPE', 'FLOOD_CAUSE', 'TOR_F_SCALE', 'TOR_LENGTH', 'TOR_WIDTH', 'EPISODE_NARRATIVE', 'EVENT_NARRATIVE', 'Distance_km'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525145b-13a3-4c73-95c8-438e2bf34e7e",
   "metadata": {},
   "source": [
    "# Create the final datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6d0fc052-ab76-4ff4-9ff2-5d5782d211f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset flash_flood saved as flash_flood.csv\n",
      "Dataset flood saved as flood.csv\n",
      "Dataset high_wind saved as high_wind.csv\n",
      "Dataset lightning saved as lightning.csv\n",
      "Dataset seiche saved as seiche.csv\n",
      "Dataset thunderstorm_wind saved as thunderstorm_wind.csv\n",
      "Dataset tornado saved as tornado.csv\n",
      "Dataset wildfire saved as wildfire.csv\n",
      "Dataset tropical_depression saved as tropical_depression.csv\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    \"flash_flood\": flash_flood,\n",
    "    \"flood\": flood,\n",
    "    \"high_wind\": high_wind,\n",
    "    \"lightning\": lightning,\n",
    "    \"seiche\" : seiche,\n",
    "    \"thunderstorm_wind\": thunderstorm_wind,\n",
    "    \"tornado\": tornado,\n",
    "    \"wildfire\": wildfire,\n",
    "    \"tropical_depression\" : tropical_depression\n",
    "}\n",
    "\n",
    "# Loop through each dataset and save it as a CSV\n",
    "for name, df in datasets.items():\n",
    "    file_name = f\"{name}.csv\"\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"Dataset {name} saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "dcabdc92-7d34-4a7a-8a5f-53019e151757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 59351 entries, 6261 to 65620\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              59351 non-null  object \n",
      " 1   Year               59351 non-null  int64  \n",
      " 2   INJURIES_DIRECT    59351 non-null  int64  \n",
      " 3   INJURIES_INDIRECT  59351 non-null  int64  \n",
      " 4   DEATHS_DIRECT      59351 non-null  int64  \n",
      " 5   DEATHS_INDIRECT    59351 non-null  int64  \n",
      " 6   DAMAGE_PROPERTY    59351 non-null  float64\n",
      " 7   FLOOD_CAUSE        59351 non-null  object \n",
      " 8   DURATION_HOURS     59351 non-null  float64\n",
      " 9   Distance_km        59351 non-null  float64\n",
      " 10  GEOID              59351 non-null  int64  \n",
      " 11  County             59351 non-null  object \n",
      " 12  GDP_per_capita     59351 non-null  float64\n",
      " 13  Density            59351 non-null  float64\n",
      "dtypes: float64(5), int64(6), object(3)\n",
      "memory usage: 6.8+ MB\n"
     ]
    }
   ],
   "source": [
    "flash_flood = pd.read_csv(\"flash_flood.csv\")\n",
    "flash_flood = flash_flood.dropna()\n",
    "flash_flood.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "abae99ab-a324-4714-b539-9acbd4c577e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 38955 entries, 1771 to 40731\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              38955 non-null  object \n",
      " 1   Year               38955 non-null  int64  \n",
      " 2   INJURIES_DIRECT    38955 non-null  int64  \n",
      " 3   INJURIES_INDIRECT  38955 non-null  int64  \n",
      " 4   DEATHS_DIRECT      38955 non-null  int64  \n",
      " 5   DEATHS_INDIRECT    38955 non-null  int64  \n",
      " 6   DAMAGE_PROPERTY    38955 non-null  float64\n",
      " 7   FLOOD_CAUSE        38955 non-null  object \n",
      " 8   DURATION_HOURS     38955 non-null  float64\n",
      " 9   Distance_km        38955 non-null  float64\n",
      " 10  GEOID              38955 non-null  int64  \n",
      " 11  County             38955 non-null  object \n",
      " 12  GDP_per_capita     38955 non-null  float64\n",
      " 13  Density            38955 non-null  float64\n",
      "dtypes: float64(5), int64(6), object(3)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "flood = pd.read_csv(\"flood.csv\")\n",
    "flood = flood.dropna()\n",
    "flood.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "37726030-5a47-47cd-b587-7144026d674f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 50102 entries, 1 to 50637\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              50102 non-null  object \n",
      " 1   Year               50102 non-null  int64  \n",
      " 2   INJURIES_DIRECT    50102 non-null  int64  \n",
      " 3   INJURIES_INDIRECT  50102 non-null  int64  \n",
      " 4   DEATHS_DIRECT      50102 non-null  int64  \n",
      " 5   DEATHS_INDIRECT    50102 non-null  int64  \n",
      " 6   DAMAGE_PROPERTY    50102 non-null  float64\n",
      " 7   MAGNITUDE          50102 non-null  float64\n",
      " 8   MAGNITUDE_TYPE     50102 non-null  object \n",
      " 9   DURATION_HOURS     50102 non-null  float64\n",
      " 10  GEOID              50102 non-null  int64  \n",
      " 11  GDP_per_capita     50102 non-null  float64\n",
      " 12  Density            50102 non-null  float64\n",
      "dtypes: float64(5), int64(6), object(2)\n",
      "memory usage: 5.4+ MB\n"
     ]
    }
   ],
   "source": [
    "high_wind = pd.read_csv(\"high_wind.csv\")\n",
    "high_wind = high_wind.dropna()\n",
    "high_wind.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "43a003f9-7a0a-4103-b22f-2adbf4b279b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7889 entries, 588 to 10201\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              7889 non-null   object \n",
      " 1   Year               7889 non-null   int64  \n",
      " 2   INJURIES_DIRECT    7889 non-null   int64  \n",
      " 3   INJURIES_INDIRECT  7889 non-null   int64  \n",
      " 4   DEATHS_DIRECT      7889 non-null   int64  \n",
      " 5   DEATHS_INDIRECT    7889 non-null   int64  \n",
      " 6   DAMAGE_PROPERTY    7889 non-null   float64\n",
      " 7   DURATION_HOURS     7889 non-null   float64\n",
      " 8   Distance_km        7889 non-null   float64\n",
      " 9   GEOID              7889 non-null   int64  \n",
      " 10  County             7889 non-null   object \n",
      " 11  GDP_per_capita     7889 non-null   float64\n",
      " 12  Density            7889 non-null   float64\n",
      "dtypes: float64(5), int64(6), object(2)\n",
      "memory usage: 862.9+ KB\n"
     ]
    }
   ],
   "source": [
    "lightning = pd.read_csv(\"lightning.csv\")\n",
    "lightning = lightning.dropna()\n",
    "lightning.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6fab30aa-947d-4958-917e-587c043b1dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55 entries, 0 to 54\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Year               55 non-null     int64  \n",
      " 1   INJURIES_DIRECT    55 non-null     int64  \n",
      " 2   INJURIES_INDIRECT  55 non-null     int64  \n",
      " 3   DEATHS_DIRECT      55 non-null     int64  \n",
      " 4   DEATHS_INDIRECT    55 non-null     int64  \n",
      " 5   DAMAGE_PROPERTY    55 non-null     float64\n",
      " 6   DURATION_HOURS     55 non-null     float64\n",
      " 7   GEOID              55 non-null     int64  \n",
      " 8   GDP_per_capita     55 non-null     float64\n",
      " 9   Density            55 non-null     float64\n",
      "dtypes: float64(4), int64(6)\n",
      "memory usage: 4.4 KB\n"
     ]
    }
   ],
   "source": [
    "seiche = pd.read_csv(\"seiche.csv\")\n",
    "seiche = seiche.dropna()\n",
    "seiche.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4858aedc-6646-4012-be30-31c3633bef85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 231984 entries, 26 to 235838\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   State              231984 non-null  object \n",
      " 1   Year               231984 non-null  int64  \n",
      " 2   INJURIES_DIRECT    231984 non-null  int64  \n",
      " 3   INJURIES_INDIRECT  231984 non-null  int64  \n",
      " 4   DEATHS_DIRECT      231984 non-null  int64  \n",
      " 5   DEATHS_INDIRECT    231984 non-null  int64  \n",
      " 6   DAMAGE_PROPERTY    231984 non-null  float64\n",
      " 7   MAGNITUDE          231984 non-null  float64\n",
      " 8   MAGNITUDE_TYPE     231984 non-null  object \n",
      " 9   DURATION_HOURS     231984 non-null  float64\n",
      " 10  Distance_km        231984 non-null  float64\n",
      " 11  GEOID              231984 non-null  int64  \n",
      " 12  County             231984 non-null  object \n",
      " 13  GDP_per_capita     231984 non-null  float64\n",
      " 14  Density            231984 non-null  float64\n",
      "dtypes: float64(6), int64(6), object(3)\n",
      "memory usage: 28.3+ MB\n"
     ]
    }
   ],
   "source": [
    "thunderstorm_wind = pd.read_csv(\"thunderstorm_wind.csv\")\n",
    "thunderstorm_wind = thunderstorm_wind.dropna()\n",
    "thunderstorm_wind.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ba70513e-d845-46b1-b509-fc105fb18b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 22896 entries, 4 to 22971\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              22896 non-null  object \n",
      " 1   Year               22896 non-null  int64  \n",
      " 2   INJURIES_DIRECT    22896 non-null  int64  \n",
      " 3   INJURIES_INDIRECT  22896 non-null  int64  \n",
      " 4   DEATHS_DIRECT      22896 non-null  int64  \n",
      " 5   DEATHS_INDIRECT    22896 non-null  int64  \n",
      " 6   DAMAGE_PROPERTY    22896 non-null  float64\n",
      " 7   TOR_F_SCALE        22896 non-null  object \n",
      " 8   TOR_LENGTH         22896 non-null  float64\n",
      " 9   TOR_WIDTH          22896 non-null  float64\n",
      " 10  DURATION_HOURS     22896 non-null  float64\n",
      " 11  Distance_km        22896 non-null  float64\n",
      " 12  GEOID              22896 non-null  int64  \n",
      " 13  County             22896 non-null  object \n",
      " 14  GDP_per_capita     22896 non-null  float64\n",
      " 15  Density            22896 non-null  float64\n",
      "dtypes: float64(7), int64(6), object(3)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "tornado = pd.read_csv(\"tornado.csv\")\n",
    "tornado = tornado.dropna()\n",
    "tornado.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4d504f1e-5062-489f-a6df-4881d6fde892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5027 entries, 0 to 5030\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              5027 non-null   object \n",
      " 1   Year               5027 non-null   int64  \n",
      " 2   INJURIES_DIRECT    5027 non-null   int64  \n",
      " 3   INJURIES_INDIRECT  5027 non-null   int64  \n",
      " 4   DEATHS_DIRECT      5027 non-null   int64  \n",
      " 5   DEATHS_INDIRECT    5027 non-null   int64  \n",
      " 6   DAMAGE_PROPERTY    5027 non-null   float64\n",
      " 7   DURATION_HOURS     5027 non-null   float64\n",
      " 8   GEOID              5027 non-null   int64  \n",
      " 9   GDP_per_capita     5027 non-null   float64\n",
      " 10  Density            5027 non-null   float64\n",
      "dtypes: float64(4), int64(6), object(1)\n",
      "memory usage: 471.3+ KB\n"
     ]
    }
   ],
   "source": [
    "wildfire = pd.read_csv(\"wildfire.csv\")\n",
    "wildfire = wildfire.dropna()\n",
    "wildfire.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7db0caa8-5816-4743-8b19-5dca707485f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 423 entries, 0 to 422\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              423 non-null    object \n",
      " 1   STATE_FIPS         423 non-null    int64  \n",
      " 2   Year               423 non-null    int64  \n",
      " 3   INJURIES_DIRECT    423 non-null    int64  \n",
      " 4   INJURIES_INDIRECT  423 non-null    int64  \n",
      " 5   DEATHS_DIRECT      423 non-null    int64  \n",
      " 6   DEATHS_INDIRECT    423 non-null    int64  \n",
      " 7   DAMAGE_PROPERTY    423 non-null    float64\n",
      " 8   DURATION_HOURS     423 non-null    float64\n",
      " 9   GEOID              423 non-null    int64  \n",
      " 10  GDP_per_capita     423 non-null    float64\n",
      " 11  Density            423 non-null    float64\n",
      "dtypes: float64(4), int64(7), object(1)\n",
      "memory usage: 39.8+ KB\n"
     ]
    }
   ],
   "source": [
    "tropical_depression = pd.read_csv(\"tropical_depression.csv\")\n",
    "tropical_depression = tropical_depression.dropna()\n",
    "tropical_depression.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137358a5-d81a-4fdd-8875-07f11fcca3a7",
   "metadata": {},
   "source": [
    "# Prediction <br>\n",
    "In this part we will build our models which will be used for the prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "33919972-ef7f-4071-b942-328280f02e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn Utilities\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    accuracy_score,\n",
    "    make_scorer\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# Statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Bayesian optimisation\n",
    "from bayes_opt import BayesianOptimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "04933929-e1aa-4de2-99d8-0d88c1889539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def regression_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Generates a detailed regression report with key metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: array-like, true target values\n",
    "    - y_pred: array-like, predicted target values\n",
    "    \n",
    "    Returns:\n",
    "    A string with the formatted regression report.\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mse ** 0.5\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    report = (\n",
    "        f\"Regression Report:\\n\"\n",
    "        f\"-------------------\\n\"\n",
    "        f\"Mean Absolute Error (MAE): {mae:.4f}\\n\"\n",
    "        f\"Mean Squared Error (MSE): {mse:.4f}\\n\"\n",
    "        f\"Root Mean Squared Error (RMSE): {rmse:.4f}\\n\"\n",
    "        f\"R-squared (R2): {r2:.4f}\\n\"\n",
    "    )\n",
    "    \n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b76eb1-ad72-4cd8-ac82-c08e2784adbd",
   "metadata": {},
   "source": [
    "# Flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "bbe5f574-56e3-4a4c-b462-c3fe8b17940b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 38955 entries, 1771 to 40731\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              38955 non-null  object \n",
      " 1   Year               38955 non-null  int64  \n",
      " 2   INJURIES_DIRECT    38955 non-null  int64  \n",
      " 3   INJURIES_INDIRECT  38955 non-null  int64  \n",
      " 4   DEATHS_DIRECT      38955 non-null  int64  \n",
      " 5   DEATHS_INDIRECT    38955 non-null  int64  \n",
      " 6   DAMAGE_PROPERTY    38955 non-null  float64\n",
      " 7   FLOOD_CAUSE        38955 non-null  object \n",
      " 8   DURATION_HOURS     38955 non-null  float64\n",
      " 9   Distance_km        38955 non-null  float64\n",
      " 10  GEOID              38955 non-null  int64  \n",
      " 11  GDP_per_capita     38955 non-null  float64\n",
      " 12  Density            38955 non-null  float64\n",
      "dtypes: float64(5), int64(6), object(2)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "flood = pd.read_csv('flood.csv')\n",
    "flood= flood.dropna()\n",
    "flood.drop('County', axis=1, inplace=True)\n",
    "flood.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6b909cde-359e-4dda-92f4-0fe69831521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "flood = pd.get_dummies(flood, drop_first=True)\n",
    "flood = flood.astype(float)\n",
    "\n",
    "y = flood[\"DAMAGE_PROPERTY\"]\n",
    "X = flood.drop([\"DAMAGE_PROPERTY\", \"GEOID\"], axis = 1)\n",
    "y = np.log1p(y) #Log transformation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Split data\n",
    "scaler = StandardScaler() #Scale data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1baa4253-8751-4702-b4c4-2e5e3cd857c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the column names to a txt file\n",
    "with open(\"flood_features.txt\", 'w') as f:\n",
    "    for column in X.columns:\n",
    "        f.write(f\"{column}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "39b1f380-0439-4170-a620-c7a02ad95f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Report:\n",
      "-------------------\n",
      "Mean Absolute Error (MAE): 2.3749\n",
      "Mean Squared Error (MSE): 11.9682\n",
      "Root Mean Squared Error (RMSE): 3.4595\n",
      "R-squared (R2): 0.4860\n",
      "\n",
      "864175.8491600794 2505145709731101.5 50051430.646197334 -65.32764656521762\n"
     ]
    }
   ],
   "source": [
    "model_flood = XGBRegressor(colsample_bytree=0.65, gamma=1.77, learning_rate=0.06, max_depth=10, n_estimators=404, subsample=0.81)\n",
    "model_flood.fit(X_train,y_train)\n",
    "y_pred = model_flood.predict(X_test)\n",
    "print(regression_report(y_test, y_pred))\n",
    "\n",
    "#Original values\n",
    "y_pred = np.expm1(y_pred)  \n",
    "y_true = np.expm1(y_test)\n",
    "\n",
    "# Evaluate\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(mae, mse, rmse, r2)\n",
    "\n",
    "\n",
    "#No tuning 0.377\n",
    "#Tuning: 0.554"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8799d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the evaluation function\n",
    "# def xgb_bayesian(n_estimators, learning_rate, max_depth, colsample_bytree, subsample, gamma):\n",
    "#     model = XGBRegressor(\n",
    "#         n_estimators=int(n_estimators),\n",
    "#         learning_rate=learning_rate,\n",
    "#         max_depth=int(max_depth),\n",
    "#         colsample_bytree=colsample_bytree,\n",
    "#         subsample=subsample,\n",
    "#         gamma=gamma,\n",
    "#         random_state=42\n",
    "#     )\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     return -mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Define the parameter bounds\n",
    "# param_bounds = {\n",
    "#     'n_estimators': (50, 500),\n",
    "#     'learning_rate': (0.01, 0.3),\n",
    "#     'max_depth': (3, 10),\n",
    "#     'colsample_bytree': (0.3, 1.0),\n",
    "#     'subsample': (0.5, 1.0),\n",
    "#     'gamma': (0, 5)\n",
    "# }\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=xgb_bayesian,\n",
    "#     pbounds=param_bounds,\n",
    "#     random_state=42,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# optimizer.maximize(init_points=10, n_iter=25)\n",
    "\n",
    "# Display best parameters\n",
    "# best_params = optimizer.max['params']\n",
    "# print(\"Best Parameters Found:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa208305-3510-4f56-8aa0-6d2672b4b06c",
   "metadata": {},
   "source": [
    "# High Wind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "84c03673-a9ce-46cf-910d-fff3fd7bb58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 50102 entries, 1 to 50637\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              50102 non-null  object \n",
      " 1   Year               50102 non-null  int64  \n",
      " 2   INJURIES_DIRECT    50102 non-null  int64  \n",
      " 3   INJURIES_INDIRECT  50102 non-null  int64  \n",
      " 4   DEATHS_DIRECT      50102 non-null  int64  \n",
      " 5   DEATHS_INDIRECT    50102 non-null  int64  \n",
      " 6   DAMAGE_PROPERTY    50102 non-null  float64\n",
      " 7   MAGNITUDE          50102 non-null  float64\n",
      " 8   MAGNITUDE_TYPE     50102 non-null  object \n",
      " 9   DURATION_HOURS     50102 non-null  float64\n",
      " 10  GEOID              50102 non-null  int64  \n",
      " 11  GDP_per_capita     50102 non-null  float64\n",
      " 12  Density            50102 non-null  float64\n",
      "dtypes: float64(5), int64(6), object(2)\n",
      "memory usage: 5.4+ MB\n"
     ]
    }
   ],
   "source": [
    "high_wind = pd.read_csv('high_wind.csv')\n",
    "high_wind= high_wind.dropna()\n",
    "high_wind.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a442efeb-1bb0-4b5f-af1d-7566de21d3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in the 'MAGNITUDE_TYPE' column:\n",
      "['E' 'M' 'MG' 'EG' 'MS' 'ES']\n"
     ]
    }
   ],
   "source": [
    "# Assuming the dataset is loaded into a DataFrame named flood\n",
    "\n",
    "# Get the unique values in the 'FLOOD_CAUSE' column\n",
    "unique_high_wind = high_wind['MAGNITUDE_TYPE'].unique()\n",
    "\n",
    "# Display the unique values\n",
    "print(\"Unique values in the 'MAGNITUDE_TYPE' column:\")\n",
    "print(unique_high_wind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "efaacfb6-aa33-4743-8ba5-db8c8edf7e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 50102 entries, 1 to 50637\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              50102 non-null  object \n",
      " 1   Year               50102 non-null  int64  \n",
      " 2   INJURIES_DIRECT    50102 non-null  int64  \n",
      " 3   INJURIES_INDIRECT  50102 non-null  int64  \n",
      " 4   DEATHS_DIRECT      50102 non-null  int64  \n",
      " 5   DEATHS_INDIRECT    50102 non-null  int64  \n",
      " 6   DAMAGE_PROPERTY    50102 non-null  float64\n",
      " 7   MAGNITUDE          50102 non-null  float64\n",
      " 8   MAGNITUDE_TYPE     50102 non-null  object \n",
      " 9   DURATION_HOURS     50102 non-null  float64\n",
      " 10  GEOID              50102 non-null  int64  \n",
      " 11  GDP_per_capita     50102 non-null  float64\n",
      " 12  Density            50102 non-null  float64\n",
      "dtypes: float64(5), int64(6), object(2)\n",
      "memory usage: 5.4+ MB\n"
     ]
    }
   ],
   "source": [
    "high_wind.info()\n",
    "high_wind.drop(\"MAGNITUDE_TYPE\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8f83423c-4c70-4f69-9778-fbc47670c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_wind = pd.get_dummies(high_wind, drop_first=True) \n",
    "high_wind = high_wind.astype(float) \n",
    "y = high_wind[\"DAMAGE_PROPERTY\"]\n",
    "X = high_wind.drop([\"DAMAGE_PROPERTY\", \"GEOID\"], axis=1)\n",
    "y = np.log1p(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "073091d4-5e0b-48ce-af9a-2384d97210a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the column names to a txt file\n",
    "with open(\"high_wind_features.txt\", 'w') as f:\n",
    "    for column in X.columns:\n",
    "        f.write(f\"{column}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f7e5ffd9-f966-4c07-974a-d443c7b94310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Report:\n",
      "-------------------\n",
      "Mean Absolute Error (MAE): 1.2485\n",
      "Mean Squared Error (MSE): 5.9147\n",
      "Root Mean Squared Error (RMSE): 2.4320\n",
      "R-squared (R2): 0.6979\n",
      "\n",
      "234639.57711629852 57610551675365.21 7590161.505222745 0.1692522022976729\n"
     ]
    }
   ],
   "source": [
    "model_high_wind = XGBRegressor( colsample_bytree=0.73, gamma=0.85, learning_rate=0.03, max_depth=10, n_estimators=485, subsample=0.9\n",
    "\n",
    ")\n",
    "model_high_wind.fit(X_train,y_train)\n",
    "y_pred = model_high_wind.predict(X_test)\n",
    "print(regression_report(y_test, y_pred))\n",
    "\n",
    "#Original values\n",
    "y_pred = np.expm1(y_pred)  \n",
    "y_true = np.expm1(y_test)\n",
    "\n",
    "# Evaluate\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(mae, mse, rmse, r2)\n",
    "\n",
    "\n",
    "#No tuning\n",
    "#Mean Absolute Error (MAE): 1.1649\n",
    "#Mean Squared Error (MSE): 5.9891\n",
    "#Root Mean Squared Error (RMSE): 2.4473\n",
    "#R-squared (R2): 0.6941\n",
    "\n",
    "#tuning\n",
    "\n",
    "#Mean Absolute Error (MAE): 2.7160\n",
    "#Mean Squared Error (MSE): 13.5957\n",
    "#Root Mean Squared Error (RMSE): 3.6872\n",
    "#R-squared (R2): 0.5627\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a13f2270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the evaluation function\n",
    "# def xgb_bayesian(n_estimators, learning_rate, max_depth, colsample_bytree, subsample, gamma):\n",
    "#     model = XGBRegressor(\n",
    "#         n_estimators=int(n_estimators),\n",
    "#         learning_rate=learning_rate,\n",
    "#         max_depth=int(max_depth),\n",
    "#         colsample_bytree=colsample_bytree,\n",
    "#         subsample=subsample,\n",
    "#         gamma=gamma,\n",
    "#         random_state=42\n",
    "#     )\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     return -mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Define the parameter bounds\n",
    "# param_bounds = {\n",
    "#     'n_estimators': (50, 500),\n",
    "#     'learning_rate': (0.01, 0.3),\n",
    "#     'max_depth': (3, 10),\n",
    "#     'colsample_bytree': (0.3, 1.0),\n",
    "#     'subsample': (0.5, 1.0),\n",
    "#     'gamma': (0, 5)\n",
    "# }\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=xgb_bayesian,\n",
    "#     pbounds=param_bounds,\n",
    "#     random_state=42,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# optimizer.maximize(init_points=10, n_iter=25)\n",
    "\n",
    "# Display best parameters\n",
    "# best_params = optimizer.max['params']\n",
    "# print(\"Best Parameters Found:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2d310-c1a2-421e-b2fa-5885d401d786",
   "metadata": {},
   "source": [
    "# Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c6473564-500d-4e85-b0b2-9b618a56d133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7889 entries, 588 to 10201\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              7889 non-null   object \n",
      " 1   Year               7889 non-null   int64  \n",
      " 2   INJURIES_DIRECT    7889 non-null   int64  \n",
      " 3   INJURIES_INDIRECT  7889 non-null   int64  \n",
      " 4   DEATHS_DIRECT      7889 non-null   int64  \n",
      " 5   DEATHS_INDIRECT    7889 non-null   int64  \n",
      " 6   DAMAGE_PROPERTY    7889 non-null   float64\n",
      " 7   DURATION_HOURS     7889 non-null   float64\n",
      " 8   Distance_km        7889 non-null   float64\n",
      " 9   GEOID              7889 non-null   int64  \n",
      " 10  GDP_per_capita     7889 non-null   float64\n",
      " 11  Density            7889 non-null   float64\n",
      "dtypes: float64(5), int64(6), object(1)\n",
      "memory usage: 801.2+ KB\n"
     ]
    }
   ],
   "source": [
    "lightning = pd.read_csv('lightning.csv')\n",
    "lightning= lightning.dropna()\n",
    "lightning.drop('County', axis=1, inplace=True)\n",
    "lightning.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "06540b88-eda3-4d5d-aec3-4ed07614a776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "lightning = pd.get_dummies(lightning, drop_first=True)\n",
    "lightning = lightning.astype(float)\n",
    "\n",
    "y = lightning[\"DAMAGE_PROPERTY\"]\n",
    "X = lightning.drop([\"DAMAGE_PROPERTY\", \"GEOID\"], axis = 1)\n",
    "y = np.log1p(y) #Log transformation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Split data\n",
    "scaler = StandardScaler() #Scale data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "fa791a2f-e324-434b-b484-fd93337a9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the column names to a txt file\n",
    "with open(\"lightning_features.txt\", 'w') as f:\n",
    "    for column in X.columns:\n",
    "        f.write(f\"{column}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a76032b1-478e-4c83-a6e4-89588e837844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Report:\n",
      "-------------------\n",
      "Mean Absolute Error (MAE): 1.6628\n",
      "Mean Squared Error (MSE): 5.3060\n",
      "Root Mean Squared Error (RMSE): 2.3035\n",
      "R-squared (R2): 0.6453\n",
      "\n",
      "63062.30492722509 93003452195.03131 304964.6736837421 -0.010435759010326118\n"
     ]
    }
   ],
   "source": [
    "model_lightning = XGBRegressor( colsample_bytree=0.32, gamma=4.55, learning_rate=0.09, max_depth=8, n_estimators=190, subsample=0.76)\n",
    "model_lightning.fit(X_train,y_train)\n",
    "y_pred = model_lightning.predict(X_test)\n",
    "print(regression_report(y_test, y_pred))\n",
    "#Original values\n",
    "y_pred = np.expm1(y_pred)  \n",
    "y_true = np.expm1(y_test)\n",
    "\n",
    "# Evaluate\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(mae, mse, rmse, r2)\n",
    "#No tuning Mean Absolute Error (MAE): 2.8357\n",
    "#Mean Squared Error (MSE): 14.5539\n",
    "#Root Mean Squared Error (RMSE): 3.8150\n",
    "#R-squared (R2): 0.5319\n",
    "#tuning\n",
    "# Mean Absolute Error (MAE): 2.8396\n",
    "#Mean Squared Error (MSE): 14.1110\n",
    "#Root Mean Squared Error (RMSE): 3.7565\n",
    "#R-squared (R2): 0.5461\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ea048847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the evaluation function\n",
    "# def xgb_bayesian(n_estimators, learning_rate, max_depth, colsample_bytree, subsample, gamma):\n",
    "#     model = XGBRegressor(\n",
    "#         n_estimators=int(n_estimators),\n",
    "#         learning_rate=learning_rate,\n",
    "#         max_depth=int(max_depth),\n",
    "#         colsample_bytree=colsample_bytree,\n",
    "#         subsample=subsample,\n",
    "#         gamma=gamma,\n",
    "#         random_state=42\n",
    "#     )\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     return -mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Define the parameter bounds\n",
    "# param_bounds = {\n",
    "#     'n_estimators': (50, 500),\n",
    "#     'learning_rate': (0.01, 0.3),\n",
    "#     'max_depth': (3, 10),\n",
    "#     'colsample_bytree': (0.3, 1.0),\n",
    "#     'subsample': (0.5, 1.0),\n",
    "#     'gamma': (0, 5)\n",
    "# }\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=xgb_bayesian,\n",
    "#     pbounds=param_bounds,\n",
    "#     random_state=42,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# optimizer.maximize(init_points=10, n_iter=25)\n",
    "\n",
    "# Display best parameters\n",
    "# best_params = optimizer.max['params']\n",
    "# print(\"Best Parameters Found:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1801b7d-d3c0-4b24-9918-81693578f7ec",
   "metadata": {},
   "source": [
    "# Tornado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "bb2c2a94-ab9b-4eeb-b382-f678ad223c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22972 entries, 0 to 22971\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              22972 non-null  object \n",
      " 1   Year               22972 non-null  int64  \n",
      " 2   INJURIES_DIRECT    22972 non-null  int64  \n",
      " 3   INJURIES_INDIRECT  22972 non-null  int64  \n",
      " 4   DEATHS_DIRECT      22972 non-null  int64  \n",
      " 5   DEATHS_INDIRECT    22972 non-null  int64  \n",
      " 6   DAMAGE_PROPERTY    22965 non-null  float64\n",
      " 7   TOR_F_SCALE        22972 non-null  object \n",
      " 8   TOR_LENGTH         22972 non-null  float64\n",
      " 9   TOR_WIDTH          22972 non-null  float64\n",
      " 10  DURATION_HOURS     22972 non-null  float64\n",
      " 11  Distance_km        22903 non-null  float64\n",
      " 12  GEOID              22972 non-null  int64  \n",
      " 13  County             22972 non-null  object \n",
      " 14  GDP_per_capita     22972 non-null  float64\n",
      " 15  Density            22972 non-null  float64\n",
      "dtypes: float64(7), int64(6), object(3)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "tornado=pd.read_csv('tornado.csv')\n",
    "tornado.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "9a2b2e0e-884b-4c0b-b46e-50898b59a0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'Category' column: ['F2' 'F0' 'F1' 'F3' 'F4' 'EF0' 'EF1' 'EF2' 'EF3' 'EF4' 'EF5' 'EFU']\n"
     ]
    }
   ],
   "source": [
    "unique_values = tornado['TOR_F_SCALE'].unique()\n",
    "print(\"Unique values in 'Category' column:\", unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "1b9783a5-8ba0-4eb0-b599-ad58b4373418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 22896 entries, 4 to 22971\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              22896 non-null  object \n",
      " 1   Year               22896 non-null  int64  \n",
      " 2   INJURIES_DIRECT    22896 non-null  int64  \n",
      " 3   INJURIES_INDIRECT  22896 non-null  int64  \n",
      " 4   DEATHS_DIRECT      22896 non-null  int64  \n",
      " 5   DEATHS_INDIRECT    22896 non-null  int64  \n",
      " 6   DAMAGE_PROPERTY    22896 non-null  float64\n",
      " 7   TOR_F_SCALE        22896 non-null  object \n",
      " 8   TOR_LENGTH         22896 non-null  float64\n",
      " 9   TOR_WIDTH          22896 non-null  float64\n",
      " 10  DURATION_HOURS     22896 non-null  float64\n",
      " 11  Distance_km        22896 non-null  float64\n",
      " 12  GEOID              22896 non-null  int64  \n",
      " 13  GDP_per_capita     22896 non-null  float64\n",
      " 14  Density            22896 non-null  float64\n",
      "dtypes: float64(7), int64(6), object(2)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "tornado = pd.read_csv('tornado.csv')\n",
    "\n",
    "tornado.drop('County', axis=1, inplace=True)\n",
    "tornado = tornado.dropna()\n",
    "tornado.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "e5b31ae3-7ef9-41be-aa9f-496ee9dba0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "tornado = pd.get_dummies(tornado, drop_first=True)\n",
    "tornado = tornado.astype(float)\n",
    "\n",
    "y = tornado[\"DAMAGE_PROPERTY\"]\n",
    "X = tornado.drop([\"DAMAGE_PROPERTY\", \"GEOID\"], axis = 1)\n",
    "y = np.log1p(y) #Log transformation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Split data\n",
    "scaler = StandardScaler() #Scale data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9d726c66-a4b9-4a5d-82a4-2bc4513d785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the column names to a txt file\n",
    "with open(\"tornado_features.txt\", 'w') as f:\n",
    "    for column in X.columns:\n",
    "        f.write(f\"{column}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "cf7a841d-4254-4dab-bf46-4f804f16af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = XGBRegressor(max_depth = 10, random_state = 14)\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "52033b7f-944a-4cb9-9838-31b1f164a8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Report:\n",
      "-------------------\n",
      "Mean Absolute Error (MAE): 2.7375\n",
      "Mean Squared Error (MSE): 13.5686\n",
      "Root Mean Squared Error (RMSE): 3.6836\n",
      "R-squared (R2): 0.5636\n",
      "\n",
      "806601.2095745867 221753985843831.4 14891406.442772 0.040066298248543464\n"
     ]
    }
   ],
   "source": [
    "model_tornado = XGBRegressor( colsample_bytree=0.49, gamma= 0.41, learning_rate=0.03, max_depth=10, n_estimators=480, subsample=0.77\n",
    ")\n",
    "model_tornado.fit(X_train,y_train)\n",
    "y_pred = model_tornado.predict(X_test)\n",
    "print(regression_report(y_test, y_pred))\n",
    "\n",
    "#Original values\n",
    "y_pred = np.expm1(y_pred)  \n",
    "y_true = np.expm1(y_test)\n",
    "\n",
    "# Evaluate\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(mae, mse, rmse, r2)\n",
    "\n",
    "#No tuning\n",
    "#Mean Absolute Error (MAE): 2.7147\n",
    "#Mean Squared Error (MSE): 14.0880\n",
    "#Root Mean Squared Error (RMSE): 3.7534\n",
    "#R-squared (R2): 0.5469\n",
    "\n",
    "#Tuning\n",
    "#Mean Absolute Error (MAE): 2.6542\n",
    "#Mean Squared Error (MSE): 13.5549\n",
    "#Root Mean Squared Error (RMSE): 3.6817\n",
    "#R-squared (R2): 0.5640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a0190506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the evaluation function\n",
    "# def xgb_bayesian(n_estimators, learning_rate, max_depth, colsample_bytree, subsample, gamma):\n",
    "#     model = XGBRegressor(\n",
    "#         n_estimators=int(n_estimators),\n",
    "#         learning_rate=learning_rate,\n",
    "#         max_depth=int(max_depth),\n",
    "#         colsample_bytree=colsample_bytree,\n",
    "#         subsample=subsample,\n",
    "#         gamma=gamma,\n",
    "#         random_state=42\n",
    "#     )\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     return -mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Define the parameter bounds\n",
    "# param_bounds = {\n",
    "#     'n_estimators': (50, 500),\n",
    "#     'learning_rate': (0.01, 0.3),\n",
    "#     'max_depth': (3, 10),\n",
    "#     'colsample_bytree': (0.3, 1.0),\n",
    "#     'subsample': (0.5, 1.0),\n",
    "#     'gamma': (0, 5)\n",
    "# }\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=xgb_bayesian,\n",
    "#     pbounds=param_bounds,\n",
    "#     random_state=42,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# optimizer.maximize(init_points=10, n_iter=25)\n",
    "\n",
    "# Display best parameters\n",
    "# best_params = optimizer.max['params']\n",
    "# print(\"Best Parameters Found:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498d909-dd07-4139-be31-57c1bc3aabb9",
   "metadata": {},
   "source": [
    "# Wildfire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1acba467-d86c-4b53-bbcf-dd8c91da20c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5027 entries, 0 to 5030\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              5027 non-null   object \n",
      " 1   Year               5027 non-null   int64  \n",
      " 2   INJURIES_DIRECT    5027 non-null   int64  \n",
      " 3   INJURIES_INDIRECT  5027 non-null   int64  \n",
      " 4   DEATHS_DIRECT      5027 non-null   int64  \n",
      " 5   DEATHS_INDIRECT    5027 non-null   int64  \n",
      " 6   DAMAGE_PROPERTY    5027 non-null   float64\n",
      " 7   DURATION_HOURS     5027 non-null   float64\n",
      " 8   GEOID              5027 non-null   int64  \n",
      " 9   GDP_per_capita     5027 non-null   float64\n",
      " 10  Density            5027 non-null   float64\n",
      "dtypes: float64(4), int64(6), object(1)\n",
      "memory usage: 471.3+ KB\n"
     ]
    }
   ],
   "source": [
    "wildfire = pd.read_csv('wildfire.csv')\n",
    "# Assuming the DataFrame is named df and the column is named 'column_name'\n",
    "# tornado = tornado[tornado['DAMAGE_PROPERTY'] != 0]\n",
    "wildfire = wildfire.dropna()\n",
    "wildfire.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4007bbc9-1af6-487d-ab92-ff48f1e51c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "wildfire = pd.get_dummies(wildfire, drop_first=True)\n",
    "wildfire = wildfire.astype(float)\n",
    "\n",
    "y = wildfire[\"DAMAGE_PROPERTY\"]\n",
    "X = wildfire.drop([\"DAMAGE_PROPERTY\", \"GEOID\"], axis = 1)\n",
    "y = np.log1p(y) #Log transformation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Split data\n",
    "scaler = StandardScaler() #Scale data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f066e76b-e517-4b39-84fd-52779eae6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the column names to a txt file\n",
    "with open(\"wildfire_features.txt\", 'w') as f:\n",
    "    for column in X.columns:\n",
    "        f.write(f\"{column}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a14594a3-caae-4fc6-88bb-98a288fe81d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Report:\n",
      "-------------------\n",
      "Mean Absolute Error (MAE): 3.0116\n",
      "Mean Squared Error (MSE): 19.5757\n",
      "Root Mean Squared Error (RMSE): 4.4244\n",
      "R-squared (R2): 0.3603\n",
      "\n",
      "761906.9933365353 36792673673404.9 6065696.4705963405 0.041279520409242654\n"
     ]
    }
   ],
   "source": [
    "model_wildfire = XGBRegressor(colsample_bytree=0.72, gamma=0.85, learning_rate=0.03, max_depth=10, n_estimators=485, subsample=0.9\n",
    ")\n",
    "model_wildfire.fit(X_train,y_train)\n",
    "y_pred = model_wildfire.predict(X_test)\n",
    "print(regression_report(y_test, y_pred))\n",
    "\n",
    "#Original values\n",
    "y_pred = np.expm1(y_pred)  \n",
    "y_true = np.expm1(y_test)\n",
    "\n",
    "# Evaluate\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "print(mae, mse, rmse, r2)\n",
    "\n",
    "#No tuning\n",
    "#Mean Absolute Error (MAE): 2.7147\n",
    "#Mean Squared Error (MSE): 14.0880\n",
    "#Root Mean Squared Error (RMSE): 3.7534\n",
    "#R-squared (R2): 0.5469\n",
    "\n",
    "#Tuning\n",
    "#Mean Absolute Error (MAE): 2.7053\n",
    "#Mean Squared Error (MSE): 13.4229\n",
    "#Root Mean Squared Error (RMSE): 3.6637\n",
    "#R-squared (R2): 0.5683\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "fecb7099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "# from bayes_opt import BayesianOptimization\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the evaluation function\n",
    "# def xgb_bayesian(n_estimators, learning_rate, max_depth, colsample_bytree, subsample, gamma):\n",
    "#     model = XGBRegressor(\n",
    "#         n_estimators=int(n_estimators),\n",
    "#         learning_rate=learning_rate,\n",
    "#         max_depth=int(max_depth),\n",
    "#         colsample_bytree=colsample_bytree,\n",
    "#         subsample=subsample,\n",
    "#         gamma=gamma,\n",
    "#         random_state=42\n",
    "#     )\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     return -mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Define the parameter bounds\n",
    "# param_bounds = {\n",
    "#     'n_estimators': (50, 500),\n",
    "#     'learning_rate': (0.01, 0.3),\n",
    "#     'max_depth': (3, 10),\n",
    "#     'colsample_bytree': (0.3, 1.0),\n",
    "#     'subsample': (0.5, 1.0),\n",
    "#     'gamma': (0, 5)\n",
    "# }\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "# optimizer = BayesianOptimization(\n",
    "#     f=xgb_bayesian,\n",
    "#     pbounds=param_bounds,\n",
    "#     random_state=42,\n",
    "#     verbose=2\n",
    "# )\n",
    "\n",
    "# optimizer.maximize(init_points=10, n_iter=25)\n",
    "\n",
    "# Display best parameters\n",
    "# best_params = optimizer.max['params']\n",
    "# print(\"Best Parameters Found:\", best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a406605-ed66-4eff-a31b-2be0191e8d33",
   "metadata": {},
   "source": [
    "# Tropical depression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f0589bb1-0d18-44fe-819b-0e092469749c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 423 entries, 0 to 422\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              423 non-null    object \n",
      " 1   Year               423 non-null    int64  \n",
      " 2   INJURIES_DIRECT    423 non-null    int64  \n",
      " 3   INJURIES_INDIRECT  423 non-null    int64  \n",
      " 4   DEATHS_DIRECT      423 non-null    int64  \n",
      " 5   DEATHS_INDIRECT    423 non-null    int64  \n",
      " 6   DAMAGE_PROPERTY    423 non-null    float64\n",
      " 7   DURATION_HOURS     423 non-null    float64\n",
      " 8   GEOID              423 non-null    int64  \n",
      " 9   GDP_per_capita     423 non-null    float64\n",
      " 10  Density            423 non-null    float64\n",
      "dtypes: float64(4), int64(6), object(1)\n",
      "memory usage: 36.5+ KB\n"
     ]
    }
   ],
   "source": [
    "tropical_depression = pd.read_csv('tropical_depression.csv')\n",
    "tropical_depression.drop('STATE_FIPS', axis=1, inplace=True)\n",
    "tropical_depression = tropical_depression.dropna()\n",
    "tropical_depression.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "71f47dff-55e3-45e1-8c35-0f6195e64600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "tropical_depression = pd.get_dummies(tropical_depression, drop_first=True)\n",
    "tropical_depression = tropical_depression.astype(float)\n",
    "\n",
    "y = tropical_depression[\"DAMAGE_PROPERTY\"]\n",
    "X = tropical_depression.drop([\"DAMAGE_PROPERTY\", \"GEOID\"], axis = 1)\n",
    "y = np.log1p(y) #Log transformation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Split data\n",
    "scaler = StandardScaler() #Scale data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "56fc57a6-9337-40b1-8fa7-2e67ab6dfa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the column names to a txt file\n",
    "with open(\"tropical_depression_features.txt\", 'w') as f:\n",
    "    for column in X.columns:\n",
    "        f.write(f\"{column}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7802f535-f33c-4071-8e64-738b0f849388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Report:\n",
      "-------------------\n",
      "Mean Absolute Error (MAE): 1.2306\n",
      "Mean Squared Error (MSE): 4.7689\n",
      "Root Mean Squared Error (RMSE): 2.1838\n",
      "R-squared (R2): 0.8383\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_tropical_depression = RandomForestRegressor( max_features = 'sqrt', min_samples_split = 5, n_estimators = 500)\n",
    "model_tropical_depression.fit(X_train,y_train)\n",
    "y_pred = model_tropical_depression.predict(X_test)\n",
    "print(regression_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4efa536-643c-48aa-bba7-3264e31ad159",
   "metadata": {},
   "source": [
    "# Now we can try to predict the GDP per capita and density of the conunties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d3174-c599-4b42-9e58-30a14fc67c07",
   "metadata": {},
   "source": [
    "To predict the GDP per capita and density we will use an ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "84930a37-b369-4dff-993b-fa9ad97100a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in 'County': 3076\n",
      "Counts for each unique value in 'County':\n",
      "GEOID\n",
      "1001     21\n",
      "37095    21\n",
      "39033    21\n",
      "39035    21\n",
      "39037    21\n",
      "         ..\n",
      "21095    21\n",
      "21097    21\n",
      "21099    21\n",
      "21101    21\n",
      "56045    21\n",
      "Name: count, Length: 3076, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('merged_data_county.csv')\n",
    "# Count the number of unique values in the 'County' column\n",
    "unique_count = data['GEOID'].nunique()\n",
    "print(f\"Number of unique values in 'County': {unique_count}\")\n",
    "\n",
    "# Count how many times each unique value appears\n",
    "value_counts = data['GEOID'].value_counts()\n",
    "print(\"Counts for each unique value in 'County':\")\n",
    "print(value_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "93fc0955-494c-41bc-b63f-39e70f3b6f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64596 entries, 0 to 64595\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   County          64596 non-null  object \n",
      " 1   GEOID           64596 non-null  int64  \n",
      " 2   Year            64596 non-null  int64  \n",
      " 3   Density         64596 non-null  float64\n",
      " 4   STATE           64596 non-null  int64  \n",
      " 5   GDP_per_capita  64570 non-null  float64\n",
      " 6   State           64596 non-null  object \n",
      "dtypes: float64(2), int64(3), object(2)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "70e9c0e3-d803-42d9-a601-1c01f0c42101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         17426.757281\n",
      "1         22772.548781\n",
      "2         22258.227760\n",
      "3         10604.887023\n",
      "4         12134.954616\n",
      "             ...      \n",
      "64591     99552.255039\n",
      "64592    178405.288235\n",
      "64593     49882.665123\n",
      "64594     54530.036251\n",
      "64595     60319.626713\n",
      "Name: GDP_per_capita, Length: 64596, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(data['GDP_per_capita'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2c57301e-91c1-4458-8954-91f4a1e3a4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               County  GEOID  Year    Density  STATE  GDP_per_capita    State\n",
      "0      Autauga County   1001  2002  29.818175    1.0    17426.757281  Alabama\n",
      "1      Autauga County   1001  2003  30.396885    1.0    17824.957265  Alabama\n",
      "2      Autauga County   1001  2004  31.414011    1.0    20354.401853  Alabama\n",
      "3      Autauga County   1001  2005  32.264864    1.0    20438.843707  Alabama\n",
      "4      Autauga County   1001  2006  33.337848    1.0    21900.112999  Alabama\n",
      "...               ...    ...   ...        ...    ...             ...      ...\n",
      "89199             NaN  56045  2026   1.115646    NaN    65310.853938      NaN\n",
      "89200             NaN  56045  2027   1.118506    NaN    66558.660744      NaN\n",
      "89201             NaN  56045  2028   1.121367    NaN    67806.467550      NaN\n",
      "89202             NaN  56045  2029   1.124227    NaN    69054.274356      NaN\n",
      "89203             NaN  56045  2030   1.127087    NaN    70302.081162      NaN\n",
      "\n",
      "[89204 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the function to calculate predictions for each GEOID\n",
    "def predict_for_geoid(geoid_data):\n",
    "    # Sort the data by Year\n",
    "    geoid_data = geoid_data.sort_values(by='Year')\n",
    "    \n",
    "    # Initialize an empty dictionary to store future data\n",
    "    future_data_dict = {'Year': [], 'GEOID': [], 'GDP_per_capita': [], 'Density': []}\n",
    "    \n",
    "    for column in ['GDP_per_capita', 'Density']:\n",
    "        # Filter the data for years 2002 to 2019 to calculate drift\n",
    "        pre_covid_data = geoid_data[(geoid_data['Year'] >= 2002) & (geoid_data['Year'] <= 2019)]\n",
    "        \n",
    "        # Calculate the average annual growth rate (drift) for the column\n",
    "        drift = pre_covid_data[column].diff().mean()\n",
    "        \n",
    "        # Predict future values for years beyond 2022\n",
    "        n_steps_future = 8  # Predict for 8 years (2023-2030)\n",
    "        start_value = geoid_data.loc[geoid_data['Year'] == 2022, column].values[0] if not geoid_data.loc[geoid_data['Year'] == 2022, column].empty else 0\n",
    "        \n",
    "        # Calculate future values using the drift\n",
    "        future_values = [start_value + (i + 1) * drift for i in range(n_steps_future)]\n",
    "        \n",
    "        # Add the future values to the dictionary\n",
    "        future_data_dict[column].extend(future_values)\n",
    "    \n",
    "    # Create future years and GEOID information\n",
    "    future_years = list(range(2023, 2023 + n_steps_future))\n",
    "    future_data_dict['Year'] = future_years\n",
    "    future_data_dict['GEOID'] = [geoid_data['GEOID'].iloc[0]] * n_steps_future\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    future_data = pd.DataFrame(future_data_dict)\n",
    "    \n",
    "    # Combine historical data with future predictions\n",
    "    return pd.concat([geoid_data, future_data], ignore_index=True)\n",
    "\n",
    "# Group by GEOID and apply the function\n",
    "grouped = data.groupby('GEOID')\n",
    "data_with_predictions = pd.concat([predict_for_geoid(group) for _, group in grouped], ignore_index=True)\n",
    "\n",
    "# Display the result\n",
    "print(data_with_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f6ce6746-cf97-4581-b502-3fa1b333d94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89204 entries, 0 to 89203\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   County          64596 non-null  object \n",
      " 1   GEOID           89204 non-null  int64  \n",
      " 2   Year            89204 non-null  int64  \n",
      " 3   Density         89204 non-null  float64\n",
      " 4   STATE           64596 non-null  float64\n",
      " 5   GDP_per_capita  89178 non-null  float64\n",
      " 6   State           64596 non-null  object \n",
      "dtypes: float64(3), int64(2), object(2)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data_with_predictions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "1cd66510-e86d-4887-abd0-6617e9da1b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            County  GEOID  Year    Density  STATE  GDP_per_capita    State\n",
      "0   Autauga County   1001  2002  29.818175    1.0    17426.757281  Alabama\n",
      "1   Autauga County   1001  2003  30.396885    1.0    17824.957265  Alabama\n",
      "2   Autauga County   1001  2004  31.414011    1.0    20354.401853  Alabama\n",
      "3   Autauga County   1001  2005  32.264864    1.0    20438.843707  Alabama\n",
      "4   Autauga County   1001  2006  33.337848    1.0    21900.112999  Alabama\n",
      "5   Autauga County   1001  2007  34.037366    1.0    22559.564927  Alabama\n",
      "6   Autauga County   1001  2008  34.603736    1.0    20603.693902  Alabama\n",
      "7   Autauga County   1001  2009  35.161012    1.0    21719.201995  Alabama\n",
      "8   Autauga County   1001  2010  35.444197    1.0    23142.016822  Alabama\n",
      "9   Autauga County   1001  2011  35.871572    1.0    24531.514241  Alabama\n",
      "10  Autauga County   1001  2012  35.703349    1.0    28593.651082  Alabama\n",
      "11  Autauga County   1001  2013  35.558510    1.0    27590.059729  Alabama\n",
      "12  Autauga County   1001  2014  35.672173    1.0    28569.971960  Alabama\n",
      "13  Autauga County   1001  2015  35.659833    1.0    31493.688869  Alabama\n",
      "14  Autauga County   1001  2016  35.918985    1.0    32661.495063  Alabama\n",
      "15  Autauga County   1001  2017  36.013813    1.0    31787.584764  Alabama\n",
      "16  Autauga County   1001  2018  36.069021    1.0    32892.910522  Alabama\n",
      "17  Autauga County   1001  2019  36.222305    1.0    32347.953164  Alabama\n",
      "18  Autauga County   1001  2020  36.466519    1.0    32301.237866  Alabama\n",
      "19  Autauga County   1001  2021  38.452709    1.0    32897.353175  Alabama\n",
      "20  Autauga County   1001  2022  38.792400    1.0    39595.670227  Alabama\n",
      "21             NaN   1001  2023  39.169114    NaN    40473.387632      NaN\n",
      "22             NaN   1001  2024  39.545827    NaN    41351.105037      NaN\n",
      "23             NaN   1001  2025  39.922541    NaN    42228.822442      NaN\n",
      "24             NaN   1001  2026  40.299254    NaN    43106.539847      NaN\n",
      "25             NaN   1001  2027  40.675968    NaN    43984.257252      NaN\n",
      "26             NaN   1001  2028  41.052682    NaN    44861.974657      NaN\n",
      "27             NaN   1001  2029  41.429395    NaN    45739.692062      NaN\n",
      "28             NaN   1001  2030  41.806109    NaN    46617.409467      NaN\n"
     ]
    }
   ],
   "source": [
    "specific_county_data = data_with_predictions[data_with_predictions['GEOID'] == 1001.0 ]\n",
    "\n",
    "print(specific_county_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "022a11b3-b4d0-4221-9292-85e4a176829e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               County  GEOID  Year    Density  STATE  GDP_per_capita    State\n",
      "0      Autauga County   1001  2002  29.818175    1.0    17426.757281  Alabama\n",
      "1      Autauga County   1001  2003  30.396885    1.0    17824.957265  Alabama\n",
      "2      Autauga County   1001  2004  31.414011    1.0    20354.401853  Alabama\n",
      "3      Autauga County   1001  2005  32.264864    1.0    20438.843707  Alabama\n",
      "4      Autauga County   1001  2006  33.337848    1.0    21900.112999  Alabama\n",
      "...               ...    ...   ...        ...    ...             ...      ...\n",
      "89199   Weston County  56045  2026   1.115646   56.0    65310.853938  Wyoming\n",
      "89200   Weston County  56045  2027   1.118506   56.0    66558.660744  Wyoming\n",
      "89201   Weston County  56045  2028   1.121367   56.0    67806.467550  Wyoming\n",
      "89202   Weston County  56045  2029   1.124227   56.0    69054.274356  Wyoming\n",
      "89203   Weston County  56045  2030   1.127087   56.0    70302.081162  Wyoming\n",
      "\n",
      "[89204 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicar\\AppData\\Local\\Temp\\ipykernel_5224\\4292446677.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data_with_predictions = data_with_predictions.fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values in the dataset with the preceding non-missing values\n",
    "data_with_predictions = data_with_predictions.fillna(method='ffill')\n",
    "\n",
    "print(data_with_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "5f1ef2a5-33c9-4e3e-8043-cc9059bf4348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            County  GEOID  Year    Density  STATE  GDP_per_capita    State\n",
      "0   Autauga County   1001  2002  29.818175    1.0    17426.757281  Alabama\n",
      "1   Autauga County   1001  2003  30.396885    1.0    17824.957265  Alabama\n",
      "2   Autauga County   1001  2004  31.414011    1.0    20354.401853  Alabama\n",
      "3   Autauga County   1001  2005  32.264864    1.0    20438.843707  Alabama\n",
      "4   Autauga County   1001  2006  33.337848    1.0    21900.112999  Alabama\n",
      "5   Autauga County   1001  2007  34.037366    1.0    22559.564927  Alabama\n",
      "6   Autauga County   1001  2008  34.603736    1.0    20603.693902  Alabama\n",
      "7   Autauga County   1001  2009  35.161012    1.0    21719.201995  Alabama\n",
      "8   Autauga County   1001  2010  35.444197    1.0    23142.016822  Alabama\n",
      "9   Autauga County   1001  2011  35.871572    1.0    24531.514241  Alabama\n",
      "10  Autauga County   1001  2012  35.703349    1.0    28593.651082  Alabama\n",
      "11  Autauga County   1001  2013  35.558510    1.0    27590.059729  Alabama\n",
      "12  Autauga County   1001  2014  35.672173    1.0    28569.971960  Alabama\n",
      "13  Autauga County   1001  2015  35.659833    1.0    31493.688869  Alabama\n",
      "14  Autauga County   1001  2016  35.918985    1.0    32661.495063  Alabama\n",
      "15  Autauga County   1001  2017  36.013813    1.0    31787.584764  Alabama\n",
      "16  Autauga County   1001  2018  36.069021    1.0    32892.910522  Alabama\n",
      "17  Autauga County   1001  2019  36.222305    1.0    32347.953164  Alabama\n",
      "18  Autauga County   1001  2020  36.466519    1.0    32301.237866  Alabama\n",
      "19  Autauga County   1001  2021  38.452709    1.0    32897.353175  Alabama\n",
      "20  Autauga County   1001  2022  38.792400    1.0    39595.670227  Alabama\n",
      "21  Autauga County   1001  2023  39.169114    1.0    40473.387632  Alabama\n",
      "22  Autauga County   1001  2024  39.545827    1.0    41351.105037  Alabama\n",
      "23  Autauga County   1001  2025  39.922541    1.0    42228.822442  Alabama\n",
      "24  Autauga County   1001  2026  40.299254    1.0    43106.539847  Alabama\n",
      "25  Autauga County   1001  2027  40.675968    1.0    43984.257252  Alabama\n",
      "26  Autauga County   1001  2028  41.052682    1.0    44861.974657  Alabama\n",
      "27  Autauga County   1001  2029  41.429395    1.0    45739.692062  Alabama\n",
      "28  Autauga County   1001  2030  41.806109    1.0    46617.409467  Alabama\n"
     ]
    }
   ],
   "source": [
    "specific_county_data = data_with_predictions[data_with_predictions['GEOID'] == 1001.0 ]\n",
    "\n",
    "print(specific_county_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "7dee3483-5d7e-4dab-8e10-9bd21e9c8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_predictions.to_csv('merged_data_county.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e18743-fe49-498e-a624-44e1e89dea58",
   "metadata": {},
   "source": [
    " ## Save models for Streamlit <br>\n",
    " Now we can save the models so that we can use them on Streamlit for our app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "9a7201bd-2201-4e68-a23e-ba482f210b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wildfire_model.pkl']"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "#Save each model\n",
    "joblib.dump(model_flood, 'flood_model.pkl')\n",
    "joblib.dump(model_high_wind, 'high_wind_model.pkl')\n",
    "joblib.dump(model_lightning, 'lightning_model.pkl')\n",
    "joblib.dump(model_tornado, 'tornado_model.pkl')\n",
    "joblib.dump(model_tropical_depression, 'tropical_depression_model.pkl')\n",
    "joblib.dump(model_wildfire, 'wildfire_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
